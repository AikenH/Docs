# 汤斯亮教授 知识计算会议记录



## 人工智能的困境和思考

人工智能究竟是什么，深度学习背后是什么原理，实际上是一种训练的是一种机器直觉，而直觉事实上是一种反直觉，它不思考。

### 大模型“陷阱”

基于GPT-3 等大规模预训练的 promt-Learning，妄图接近通用人工智能（AGI）的这种情况，在这种究极大规模的模型下出现的神奇效果。

Google BIG-bench（dataset）：200+ tasks，show as QAs，里面的这些QA有一些为了探究机器是否有Self-Aware，自我意识。

这种大模型没有什么技术上的创新，还是一个统计模型，而并没有智能的存在，所以事实上与AGI还有很大的距离，它缺乏常识，缺乏对于基础概念的理解，这就是大规模模型的陷阱。

### Neural Tangent Kernel

越大的模型反而越不容易OverFitting，泛化能力可能更强，NTK等方法从数学上证明了在无线参数条件下，深度神经网络会退化为NTK/NNGP.

无限宽的深度学习模型，会收敛到某个特定的高斯核上面，也就是说深度学习实际上是自带正则的，

### Deep Network from First Principles

目前主流卷积网络的神经网络的设计可以通过Rate Reduction出发导出，可以基于数学原理推出神经网络的架构

### Deep Network are Kernel Machines

论文证明，只要模型使用的优化方式是梯度下降，实际上模型只是在记忆之前的样本，最终都可以推导成一个Kernel 计算的形式。

将样本生维到Kernel Space，然后计算KNN，得到标签。

### 大数据的偏与失

常识在描述上的缺失会导致模型的有缺。还有基于新闻统计出来的对于恶意时间的描述会导致topic的偏差，也就会导致机器对人类的认知偏向于灾难。

还有各种各样的不同偏差


## 知识计算

而为了解决这些问题，智商不够，知识来凑。

知识图谱：用图来存储可以被机器利用的结构化知识，用于描述物理世界的对象，概念及其相互关系。
DBPedia

![](https://gitee.com/Aiken97/markdown-image/raw/master/3070imgs/20220104114222.png)
