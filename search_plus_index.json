{"./":{"url":"./","title":"Introduction","keywords":"","body":"1. Readme1.1. Update1.2. Self-Introduction1.3. Get in touch1. Readme @AikenHong 2021 个人笔记整理 按照个人理解和学习流程进行笔记编写，涉及到的知识点等等可能会有一定程度上的不系统以及不全面， 希望看笔记的大家不吝啬个人的见解和知识，对其中偏差与错误多多指教。 Blog在组织的时候，可能会在一定的地方进行重复，善用搜索功能。 按照如下的类别对笔记进行一个初步的整理，可以以章节进行区分，I'll organize my blog in this order 环境配置：Init and custom your env's in windows(WSL) and linux(Bash). 开发工具：Dev tools like vscode, docker, vim,etc 语言学习：Langs include python(include libs like pytorch),cpp,shell and markable langs etc. 算法文章：Papers, Algorithms 实习准备：repare for the Interview, 1.1. Update 通过Github, Onedrive, Gitbook 维护个人的笔记系统，编写blogs_update.sh在启动serve的时候更新远程repo，注意在编写新的blog的同时要更新SUMMARY。 文档的一些组织和编写规范： Blog按照类型存放在不同的文件夹中，以此来做为基本的分类依据。 图片在笔记编写的时候上传到Github或者Gitee，以在线的形式进行查阅。 特殊的章节式的笔记可以考虑重新创建一本gitbook按照章节划分可能更加清晰 1.2. Self-Introduction I'm Aiken H, &#x1F468;‍&#x1F393;A master candidate of XJTU，&#x1F4BB;Working on &#x1F355;Weakly-Supervised-Learning(CV) &#x1F331; I’m currently learning and working on: Reinforcement Learning， Game Ai，Open world Learning Transformer， New MLP，New CLass Discovery，Few-Shot-Learning，... WSL2，Linux+，CPP+ &#x1F52D; I'm currentlu working on: Code A Universal Framework for (CV)Classification to Finish my graduation project 1.3. Get in touch THIS REPO © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-10 12:57:29 "},"Envs/readme.html":{"url":"Envs/readme.html","title":"Chaper1 Dev Env","keywords":"","body":"1. Part1 Deploy you Dev Env1. Part1 Deploy you Dev Env @aikenhong 2021 “工欲善其事必先利其器” 笔记的第一个章节介绍开发环境的部署，OS层面主要包括Linux和Windows的开发相关的初步设置，相关的内容可粗略的分为一下的几个方面；在基本的配置之外，后续会在对应OS的层级下添加对应操作系统的各种使用技巧等等 Windows：环境下的安装较为简单，就不再多说，其余的一些包括 WSL2的安装和基本配置 Powershell的安装和美化 Linux：配置会相对复杂 各种换源处理提高下载速度 开发工具，监测工具等等的安装和初步配置 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-30 00:11:45 "},"Envs/Windows.html":{"url":"Envs/Windows.html","title":"1 Windows","keywords":"","body":"1. Windows 常用操作1.1. WSL1.1.1. Install1.1.2. Mount And Move1.1.3. SSH1.1.4. SETTING1.2. PowerShell1.2.1. Window Terminal1.2.2. Posh and Oh-My-Posh1.2.3. Findstr/Find1.2.4. Generate GUID1.3. PANBAIDU1.4. Office AutoSave1. Windows 常用操作 @AikenHong 2020 This document is about windows tips and powershell. 1.1. WSL Windows Subsystem Linux 部署Linux环境在Windows中，同时通过/mnt和Windows的文件系统进行交互和链接，在这种情况下需要另外设置包括ssh，conda等等环境，这些环境上与主机并不互通。 在Win11中的WSL2已经支持的相应主机的CUDA使用，所以如果要支持GPU的Pytorch，直接按照官网中的方法进行安装就好了。 1.1.1. Install 首先启用WIndows的虚拟机功能； 具体的安装方法（两种）： 在Windows Store中安装相应的发行版 命令行方法：新版本的方法会默认下载最新的Linux内核（Ubuntu）， wsl --install # 列出对应的发行版 wsl -l -o # 选择相应的发行版进行安装 wsl --install -d # 如果当前的WSL是1版本，更新到2 wsl -l -v wsl --set-default-version 2 WSL安装完后Terminal中应该会自动出现相应的配置，如果没有出现可以按照下面的JSON进行配置 1.1.2. Mount And Move WSL的默认存储路径：%LOCALAPPDATA%/packages/c......./local_state 修改WSL的默认存储路径，将磁盘挂载在别的地方 # 关闭wsl服务 wsl --shutdown # 查看关闭状态 wsl -l -v # 导出wsl2 system； docker-desktop & docker-desktop-data # 导出系统 wsl --export wsl --export Ubuntu-20.04 E:\\WSLWorkspace\\ubuntu.tar # 删除（注销）系统 wsl --unregister Ubuntu-20.04 # 导入系统到指定的新位置(使用新路径导入新系统) wsl --import Ubuntu-20.04 E:\\WSLWorkspace E:\\WSLWorkspace\\ubuntu.tar # 启动WSL2服务（这里在terminal中会出现两个ubuntu的unid，把原本的第一个给注释掉才行） # 设置默认wsl2用户 ubuntu2004.exe config --default-user xxx # 删除多余的所有tar，over 1.1.3. SSH wsl2启用SSH，ssh功能应该默认是启用的，如果ssh没有启用的话 vim /etc/ssh/sshd_config 修改如下的几个配置 Port = 22 去掉这行的#，代表启用端口号22 ListenAddress 0.0.0.0 去掉这行的#，代表监听所有地址 PasswordAuthentication yes，将这行的no改成yes，代表支持账号密码形式的登录 重启服务 sudo service ssh restart 此时还不能支持root用户密码登录 默认情况下，root用户只支持用RSA登录，但是可以修改配置的 切换到root用户打开SSH的配置文件 找到行PermitRootLogin prohibit-password保留这行的#，这意味着：允许root登录，但是禁止root用密码登录，所以这行要注释掉。 需要添加一行: PermitRootLogin yes 剩下的其余配置按照Linux文档进行文件的配置 1.1.4. SETTING WSL在Windows Terminal的启动目录设置 //wsl$/Ubuntu-20.04/home/aikenhong ``` 1.2. PowerShell shift+右键: 在此处打开powershell. PowerShell ，这里的PowerShell和windows的已经不是同一个东西了，可能要更先进一些，通过msi进行安装，安装完后重启terminal就会自动的添加配置，后续的配置在这个new shell中进行会更好一些 1.2.1. Window Terminal Install ：Windows Store 添加Terminal到右键菜单： 参考：Windows Terminal 完美配置 中的右键菜单部分：Install/uninstall scripts for Windows Terminal context menu items 注意，这里涉及到注册表修改的操作，所以我们需要在修改注册表之间建立注册表还原点。 Basic Config： 新版本的Terminal中大部分的配置都已经有了UI了，配置起来还是比较方便的，其实主要的配置直接在设置面板里设置就可以了。这里以早期版本的配置文件设置为例： { \"$schema\": \"https://aka.ms/terminal-profiles-schema\", \"actions\": [ { \"command\": { \"action\": \"copy\", \"singleLine\": false }, \"keys\": \"ctrl+c\" }, { \"command\": \"find\", \"keys\": \"ctrl+shift+f\" }, { \"command\": \"paste\", \"keys\": \"ctrl+v\" }, { \"command\": { \"action\": \"splitPane\", \"split\": \"auto\", \"splitMode\": \"duplicate\" }, \"keys\": \"alt+shift+d\" } ], \"alwaysShowTabs\": true, \"copyFormatting\": \"rtf\", \"copyOnSelect\": false, \"defaultProfile\": \"{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\", \"initialCols\": 130, \"initialRows\": 35, \"launchMode\": \"default\", \"profiles\": { \"defaults\": { \"acrylicOpacity\": 0.69999999999999996, \"closeOnExit\": \"graceful\", \"colorScheme\": \"AdventureTime\", \"font\": { \"face\": \"FiraCode Nerd Font\" }, \"historySize\": 9001, \"padding\": \"5, 5, 20, 25\", \"snapOnInput\": true, \"startingDirectory\": \".\", \"useAcrylic\": true }, \"list\": [ { \"backgroundImage\": \"C:\\\\Users\\\\Aiken\\\\Pictures\\\\Camera Roll\\\\a560083febb425e04ba0a86a7851c51dc2b417a4.png\", \"backgroundImageOpacity\": 0.26000000000000001, \"colorScheme\": \"purplepeter\", \"commandline\": \"powershell.exe\", \"font\": { \"face\": \"FiraCode Nerd Font Mono Retina\" }, \"guid\": \"{61c54bbd-c2c6-5271-96e7-009a87ff44bf}\", \"hidden\": false, \"name\": \"Windows PowerShell\" }, { \"commandline\": \"cmd.exe\", \"guid\": \"{0caa0dad-35be-5f56-a8ff-afceeeaa6101}\", \"hidden\": false, \"name\": \"CMD\" }, { \"guid\": \"{b453ae62-4e3d-5e58-b989-0a998ec441b8}\", \"hidden\": true, \"name\": \"Azure Cloud Shell\", \"source\": \"Windows.Terminal.Azure\" }, { \"colorScheme\": \"Banana Blueberry\", \"commandline\": \"ssh root@202.117.43.196 -p 23076\", \"guid\": \"{44257ed0-90f8-41a1-bad0-2c637012ce40}\", \"hidden\": false, \"icon\": \"ms-appx:///ProfileIcons/{9acb9455-ca41-5af7-950f-6bca1bc9722f}.png\", \"name\": \"202.117.43.196\", \"startingDirectory\": \".\" }, { \"acrylicOpacity\": 0.68999999999999995, \"antialiasingMode\": \"cleartype\", \"backgroundImage\": \"desktopWallpaper\", \"backgroundImageOpacity\": 0.20000000000000001, \"colorScheme\": \"purplepeter\", \"commandline\": \"wsl.exe ~\", \"cursorShape\": \"underscore\", \"experimental.retroTerminalEffect\": false, \"font\": { \"face\": \"FiraCode Nerd Font\" }, \"guid\": \"{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\", \"hidden\": false, \"intenseTextStyle\": \"all\", \"name\": \"Linux20.04\", \"padding\": \"10\", \"source\": \"Windows.Terminal.Wsl\", \"startingDirectory\": null, \"tabTitle\": null } ] }, \"schemes\": [ { \"background\": \"#1F1D45\", \"black\": \"#050404\", \"blue\": \"#0F4AC6\", \"brightBlack\": \"#4E7CBF\", \"brightBlue\": \"#1997C6\", \"brightCyan\": \"#C8FAF4\", \"brightGreen\": \"#9EFF6E\", \"brightPurple\": \"#9B5953\", \"brightRed\": \"#FC5F5A\", \"brightWhite\": \"#F6F5FB\", \"brightYellow\": \"#EFC11A\", \"cursorColor\": \"#FFFFFF\", \"cyan\": \"#70A598\", \"foreground\": \"#F8DCC0\", \"green\": \"#4AB118\", \"name\": \"AdventureTime\", \"purple\": \"#665993\", \"red\": \"#BD0013\", \"selectionBackground\": \"#FFFFFF\", \"white\": \"#F8DCC0\", \"yellow\": \"#E7741E\" }, ], \"showTabsInTitlebar\": true, \"tabWidthMode\": \"titleLength\", \"theme\": \"dark\", \"windowingBehavior\": \"useAnyExisting\" } 1.2.2. Posh and Oh-My-Posh 主要步骤可以分为一下几点，安装字体，安装Posh，安装&设置oh-my-posh 参考链接： 个人的知乎回答 Style your Windows terminal Windows Terminal 完美配置 Upgrading | Oh My Posh 字体安装与下载：按照链接下载安装就行了 安装PowerShell插件： # 后面的这些User的限制倒是不需要 # 安装PSReadLine Install-Module -Name PSReadLine -Scope CurrentUser # 如果上面安装出现问题, 可以尝试下面的 Install-Module -Name PSReadLine -Scope CurrentUser -Force -SkipPublisherCheck # 安装PSR包，让命令行更好用，类似ZSH Install-Module -Name PSReadLine Install-Module -Name PSReadLine -Scope CurrentUser # 安装Posh-git包，让git更好用 Install-Module -Name posh-git Install-Module -Name posh-git -Scope CurrentUser 查看现存主题： Get-PoshThemes # 设置主题 Set-PoshPrompt -Theme half-life 设置Terminal中的启动参数 code $PROFILE 并设置成如下的形式 # 导入包 Import-Module posh-git Import-Module oh-my-posh Import-Module PSReadLine # 设置主题 Set-PoshPrompt -Theme spaceship # ================psreadline setting # 设置预测文本来源为历史记录 Set-PSReadLineOption -PredictionSource History # 每次回溯输入历史，光标定位于输入内容末尾 Set-PSReadLineOption -HistorySearchCursorMovesToEnd # 设置 Tab 为菜单补全和 Intellisense Set-PSReadLineKeyHandler -Key \"Tab\" -Function MenuComplete # 设置 Ctrl+d 为退出 PowerShell Set-PSReadlineKeyHandler -Key \"Ctrl+d\" -Function ViExit # 设置 Ctrl+z 为撤销 Set-PSReadLineKeyHandler -Key \"Ctrl+z\" -Function Undo # 设置向上键为后向搜索历史记录 Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward # 设置向下键为前向搜索历史纪录 Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward 1.2.3. Findstr/Find # this command is like grep in Linux Common-u-want-to-carry-out | findStr \"String\" # for example conda list | Select-String (\"matplot\" , \"pillow\", \"scipy\", \"tensorboard\") 1.2.4. Generate GUID this command can generate the only GUID new-guid 1.3. PANBAIDU 百度网盘的网页版倍速播放的技巧： videojs.getPlayers(\"video-player\").html5player.tech_.setPlaybackRate(2) 1.4. Office AutoSave 首先通过设置（选项）界面找到自动保存的asd文件的地址 在信息-管理文档中选择ASD进行对文档的恢复 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-23 11:17:54 "},"Envs/PicBed.html":{"url":"Envs/PicBed.html","title":"1.1 PicBed Setting","keywords":"","body":"1. PicBed1.1. Github1.1.1. 基本部署1.1.2. 存在问题1.2. Gitee1.2.1. gitee基本部署1.2.2. 配置Gitee Repo1.2.3. 设置配置文件1.3. PicGo With Obsidian etc.路漫漫其修远兮，吾将上下而求索The road ahead will be long ,Our climb will be steep. 1. PicBed @Aiken 2020 first write，2021 modify Mainly using picgo-core(command line) to setting picbed，and we can update the setting method 1.1. Github 使用PicGo-Core（command line）设置github图床，自动转义url 插入自动复制图片，使用git上传github 1.1.1. 基本部署 在偏好设置中的图像，进行如下设置&#x1F447;： 下载或更新PicGo-Cord(command line) 接着去Github中建立一个Repo：UserName/RepoName，用以存放图片（Public），简单的用readme初始建立即可。 在Github的setting - developer setting-personal access tokens中新建token，指定简单的repo权限，并记录个人的token（只显示一次） Attention： 忘记记录的话，在token中也是通过update token（好像是这个名，获取新的值的） 用Typora打开配置文件设置，或者使用命令行进行配置 { \"picBed\": { \"github\": { \"repo\": \"UserName/RepoName\", \"token\": \"your github token here\", \"path\": \"img/\", \"customUrl\": \"https://raw.githubusercontent.com/UserName/RepoName/master\", \"branch\": \"master\" }, \"current\": \"github\", \"uploader\": \"github\" }, \"picgoPlugins\": {} } 点击验证图片上传选项，进行测试，成功即可 1.1.2. 存在问题 用Github做图床的话，上传不是十分的稳定（可能需要依赖科学上网技术。请八仙过海，各显神通）。可以用其他的服务器作图床，大体过程应该也差不多，后续个人有更换的话在进行补充。 在其他的pc上可以使用相同的token进行复用，但是在进行测试的时候要记得将repo中的两张测试图片删除，不然可能会导致验证失败的问题。 1.2. Gitee 因为gitee是国内的github，服务器比较稳定，所以我们也可以使用gitee作为我们更为稳定的图床； 两个链接合起来才是好用的，都有一些冗余： Typora+picgo-core+gitee PicGo-core+Gitee+Typora 1.2.1. gitee基本部署 安装Node，npm； 安装picgo-core的命令行命令： npm install picgo -g 安装gitee的插件： picgo install super-prefix picgo install gitee-uploader 1.2.2. 配置Gitee Repo 初始化一个repo，保存URL中的User/repo，不要轻信标题，因为有昵称机制。 在个人资料中初始化个人的Token，勾选projects选项即可; 1.2.3. 设置配置文件 基于picgo的命令，会自动的更新Json文件，我们不许需要 picgo set uploader # up to the command hint, we input those messages 1.按上下键找到gitee，回车 2.repo：用户名/仓库名 （打开自己的仓库，浏览器里的网址username/reponame） 3.token：刚才生成的token 4.path:路径，写仓库的名字就是reponame 5.custompath:不用填，回车 6.customURL:不用填，回车 # finish setting process picgo use uploader 1.3. PicGo With Obsidian etc. 这种方法可能是最稳健的泛化能力也最强，再很多编辑器类似的地方都能用，但是前提就是我们进行笔记撰写的时候要把picgo的客户端打开。 安装picgo客户端，（在其中安装你对应图床的插件） 和上述描述的一样再gitee或者github中设置相应的图床token等，并在软件中填写对应的token picgo设置中，打开server即可 再Obsidian中安装插件 image auto upload plugin 并在其中设置本地ip:port即可 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-27 17:01:14 "},"Envs/VSCode.html":{"url":"Envs/VSCode.html","title":"1.2 VsCode","keywords":"","body":"1. VsCode Python Debug Launch.json 编写1.1. Debug1.2. Test1.3. TodoTree1.4. Plugins1.4.1. Monokai Pro1. VsCode Python Debug Launch.json 编写 @Aiken 2020 配置Launch.json 能够帮助我们更好的进行debug的操作。 有一些一些比较特别的文件名和相关编码。 1.1. Debug ${workspaceFolder} 指代当前运行目录 ${file}指代当前文件 找到launch文件并打开 自定义JSON：执行工作文件夹下的main.py进行调试 { \"name\": \"experiment\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/main.py\", \"console\": \"integratedTerminal\", \"args\": [\"--data_path\",\"${workspaceFolder}/data\", \"--mode\",\"0\",\"--resume\",\"false\"] }, 默认 JSON：执行当前文件 { \"name\": \"current file\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"${file}\", \"console\": \"integratedTerminal\" } 1.2. Test Vscode的新特性，Test模块，学会如何使用这样的测试模块。 1.3. TodoTree 打开设置-打开json文件（设置右上角） 添加如下内容：（颜色和关键词可自定义） \"todo-tree.tree.showScanModeButton\": true, \"todo-tree.highlights.enabled\": true, \"todo-tree.highlights.defaultHighlight\": { \"type\": \"text and comment\", }, \"todo-tree.highlights.customHighlight\": { \"TODO\": { \"foreground\": \"#2f3542\", \"background\": \"#f6b93b\", \"iconColour\": \"#f39c12\", \"icon\": \"issue-opened\", \"type\": \"line\" }, \"FIXME\": { \"foreground\": \"#2f3542\", \"background\": \"#e55039\", \"iconColour\": \"#e55039\", \"icon\": \"flame\", \"type\": \"line\" }, \"NOTE\": { \"foreground\": \"#2f3542\", \"background\": \"#9980FA\", \"iconColour\": \"#6c5ce7\", \"icon\": \"eye\", \"type\": \"line\" }, \"RECORD\": { \"foreground\": \"#2f3542\", \"background\": \"#7bed9f\", \"iconColour\": \"#2ed573\", \"icon\": \"info\", \"type\": \"line\" } }, \"todo-tree.general.tags\": [ \"TODO\", \"FIXME\", \"NOTE\", \"RECORD\" ], 1.4. Plugins 1.4.1. Monokai Pro 切换到目录user/aiken/.vscode/extensions/monokaipro/js/app.js 类似的文件， 找到key: \"isValidLicense\" 将下方的if和return的判定值1即可 最终代码如下： key: \"isValidLicense\", value: function () { var e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : \"\", t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : \"\"; if (!e || !t) return 1; var o = s()(\"\".concat(i.APP.UUID).concat(e)), r = o.match(/.{1,5}/g), n = r.slice(0, 5).join(\"-\"); return t === 1 } © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-30 00:12:40 "},"Envs/GoogleColab.html":{"url":"Envs/GoogleColab.html","title":"1.3 Colab","keywords":"","body":"1. Google Colab 免费羊毛GPU 使用时候遇到的一些问题总结1. Google Colab 免费羊毛GPU 使用时候遇到的一些问题总结 @Aiken 2020 在使用Google Colab的时候会有一些常见的使用错误，然后我们记录一些常见的错误的解决方案，方便后续使用。 INDEX： 命令行参数的输入问题 tensorboard的执行方法 # 在colab中写的时候要把前面的符号也写上 %load_ext tensorboard %tensorboard --logdir './runs' command命令的使用：包括库安装和卸载之类的。 主要就是在命令前+！ !/opt/bin/nvidia-smi # 下面顺便解决了一下 # ImportError: cannot import name 'PILLOW_VERSION'(版本问题) !pip uninstall pillow !pip install pillow==5.2.0 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-14 20:27:15 "},"Envs/Linux.html":{"url":"Envs/Linux.html","title":"2 Linux","keywords":"","body":"1. Linux 常用操作1.1. Readme1.2. 文件架构1.3. 各种类型的指令1.3.1. 压缩、解压缩文件操作1.3.2. 文件操作相关1.3.3. 进程操作1.3.4. 下载安装相关操作1.3.5. grep 命令行输出查找1.3.6. GPU、CPU信息1.4. 非特定的系统指令1.4.1. WATCH1.4.2. HISTORY1.4.3. ln -s1.4.4. HEAD、TAIL1.5. 远程操作1.5.1. 启动ssh服务1.5.2. 避免远程终端执行的代码中断1.5.3. SCP：linux之间传文件（好像也支持和Windows互传）1.5.4. SZ、RZ命令1.6. 环境配置1.6.1. Install Vim/NVim/SpaceVim1.6.2. Install Monitor1.6.3. Zsh and OhMyZsh 终端设置1.7. Anaconda 安装1.7.1. 参考文档1.7.2. 下载安装1.7.3. 环境激活1.7.4. 关闭Conda的命令行提示1.7.5. Anaconda 查看添加删除1.8. 注意事项1.8.1. python环境无法识别1.8.2. 删除Zsh后远程无法连接1.8.3. apt-get 找不到包1.8.4. apt-get /var/lib/dpkg/lock-frontend1.8.5. chsh: PAM: Authentication failure1.8.6. NVCC command not found1. Linux 常用操作 @aiken 2020 1.1. Readme 这篇文章主要是基于Ubuntu来进行编写的，其他的一些发行版的操作，暂不考虑； 记录一些基本的常用操作，方便后续使用和查找之类的 ⚡https://www.runoob.com/w3cnote/linux-common-command-2.html ⚡LINUX shell百科：https://www.explainshell.com/explain/1/ps Linux资源汇总 收录一些Linux的工具书以及相关的OnLine-Doc，方便后续进行学习和查阅： 先从3开始看吧，其他的慢慢看，我觉得一天看一点点就差不多了，不要为难自己。 鸟哥的Linux私房菜：相对全面一点但是内容有点太多了 Linux就该这么学：从开始到结束的流程挺完善的，但是这个网站做的事纯傻逼 Linux Tools Quick Tutorial：简单入门教程好像是 Linux命令行于Shell脚本编程大全：本地PDF，在当前文件夹下面进行查看 1.2. 文件架构 Linux各文件夹的含义 该部分分析LInux下的文件架构体系，最外层的一些系统文件夹的基础作用以及对应的特殊功能等等， 该部分分析的必要性主要在于我们能够更清楚我们文件的存储体系以及系统文件的处理方面。 /tmp：临时文件夹，系统会定期清理其中的文件，用来存放一些下载和安装的文件 /mnt: mount挂载文件夹，作为挂载目录来使用，比如在WSL中，对应的就是windows系统的文件 /etc:用来存放所有的系统管理所需要的配置文件和子目录，linux正是因为这些文件才能正常运行 /home: 个人文件夹，在home下会有自己的user dir,通常情况下我们的工作区和对应的其余资料都会放在这个部分 /bin: 是binary的缩写,包含了引导系统启动所需的命令和普通用户可以使用的常用命令 /root: 系统管理员的主目录 /var:这个目录中存放着那些不断在扩充着的东西，为了保持/usr的相对稳定那些经常被修改的目录可以放在这个目录下，实际上许多系统管理员都是这样干的顺带说一下系统的日志文件就在/var/log目录中。 /usr: 最庞大的目录，要用到的应用程序和文件几乎都在这个目录 1.3. 各种类型的指令 按照使用场景对各种指令进行分类整理，对各种命令有一个更好的理解。 1.3.1. 压缩、解压缩文件操作 各种压缩指令对应不同的命令和设置，其中压缩文件可以包含一下的几种： tar.gz, zip 7z zip files 后缀 ==== zip and unzip .zip files ========== # we need to install the package for .zip sudo apt-get install zip # r means recurrent（递归的遍历） zip -r newpackage.zip dir1 # unzip files to target dir unzip zipfiles.zip -d dir/* -m: 压缩文件删除源文件 -o: 将压缩文件的最新变动时间设置为压缩的时间 -r: 递归压缩，目录下的所有子级目录一并压缩 -x: “文件列表”，压缩时排除文件列表中的文件 tar.gz files tar压缩解压缩命令详解 # 压缩文件 tar -zcvf files.tar.gz dir tar -zcvf files.tar.gz *.md # .tar文件 tar -xvf file.tar -C 指定目录 tar -xvf file.tar dir/ # .gz文件 gzip -d 批量解压tar到相应的文件夹中 有几种不同的写法，这里看一下bash的相应手册，看看我们到底需要采用哪一种来执行 一下实现相应的bash脚本来执行对应的操作： 这里有需要注意的是单引号和相应的顿号要进行区分不然会发生不对的问题 # version 1 ez2understrand for i in `ls *.tar.gz` do mkdir /dir/${i/.tar.gz//} tar zxvf $i -C /dir/${i/.tar.gz//} done # version 2 try to use assignment method # 可以发现基本的操作是一样的，就是对应的定义的地方 # 可以考虑一下是如何使用echo和cut以及对应的-d 和 -f1是什么意思 for file in `ls *.tar` do todir=`echo $file | cut -d\".\" -f1` mkdir $todir && tar -xvf $file -C $todir done 1.3.2. 文件操作相关 建立文件夹： mkdir foldername 删除文件/文件夹： rm -[option] filename rm -[option] foldername [option]: \"rm -f\" 强行删除，忽略不存在的文件，不提示确认。(f为force的意思) \"rm -i\" 进行交互式删除，即删除时会提示确认。(i为interactive的意思) \"rm -r\" 将参数中列出的全部目录和子目录进行递归删除。(r为recursive的意思) \"rm -v\" 详细显示删除操作进行的步骤。(v为verbose的意思) ​ 删除文件夹中的文件不删除文件夹 rm -rf /test/* 查看某个文件夹下文件或者文件夹的个数： 参考链接：https://blog.csdn.net/niguang09/article/details/6445778 # 查看某个文件夹下文件的个数 # 2.包括了子文件夹下的文件 3.之查看文件夹 4. 包括子文件夹中的文件夹 ls [dirname] -l|grep \"^-\"| wc -l ls [dirname] -lR|grep \"^-\"| wc -l ls [dirname] -l|grep \"^d\"| wc -l ls [dirname]-lR|grep \"^d\"| wc -l # 通过管道查看 ll | wc -l 查看文件夹和磁盘的空间占用 explain_shell du df 命令可以显示目前所有文件系统的可用空间和使用情形 # 参数 -h 表示使用「Human-readable」的输出，也就是在档案系统大小使用 GB、MB 等易读的格式。 df -h :zap: du:查询文件或者当前文件夹的磁盘使用空 # 查询当前文件夹下面各个文件夹的大小： # 将深度改成n应该可以改变递归到子文件夹下的深度 du -h --max-depth=1 * # *代表的是当前文件目录 du -h --max-depth=1 [path] 移动或者重命名文件(mv) # 将文件A重命名为文件B mv nameA nameB # 移动文件到指定目录下 mv files dir/ 拷贝文件(cp) # using cp to copy file cp dir1/filea dir2/filea.bak 列出文件(ls) ls -a # list the file in the sub dir too ls -R ls -a -l -R 1.3.3. 进程操作 一些常用的进程操作命令，罗列如下，由于远程的链接常常中断，我们会需要手动终止一些进程，避免多余的内存、chache和性能占用。 TODO： 如果我们中断了远程链接，如何避免进程被kill，（也可以参考mobaxterm） 同时在下次链接的时候，切换到该bash中。 解决方案1 解决方案2 # 常用命令如下 # 搜先列出包含所有其他使用者的进程 ps -aux htop # 针对进程的PID进行关闭 kill PID # 如何执行批量的进程操作 更多相关ps的可选操作可以参考https://www.explainshell.com/explain/1/ps 1.3.4. 下载安装相关操作 安装没安装完成的包： sudo apt-get install package --fix-missing 更新源： 很多情况下Ubuntu的Source是被屏蔽的，所以我们需要使用国内源进行替代，来提升我们的下载和安装的速度 # backup the source files in cases that sth wrong sudo cp /etc/apt/sources.list /etc/apt/source.list.bak # using sudo to modify or recreate the source.list files sudo nvim /etc/apt/source.list # replace the content of it === TSINGHUA SOURCE === # update the source info sudo apt update # update the files to varify the speed. sudo apt upgrade 实际上换源只要找到对应的ubuntu的发行版，也就是在源链接后面的形式，然后将前面的url改成对应的源即可 All the source list；清华源更新地址;聚合；XJTU；Alibaba ==================== aliyun source =================== deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse wget and curl command 如何使用wget和curl下载和安装服务 1.3.5. grep 命令行输出查找 通过grep在命令行中筛选输出显示，只显示grep指定的部分。 # 只显示其中包括str的部分 command | grep 'str' 1.3.6. GPU、CPU信息 Linux查看显卡信息： lspci | grep -i vga # 如果是nvidia还可以 lspci | grep -i nvidia # 最常用：或者使用nvidia的自带命令 nvidia-smi 监视GPU使用情况 watch nvidia-smi # or gpustat --watch 显示CUDA版本 cat /usr/local/cuda/version.txt 查看CPU：相关信息 显示CPU个数： cat /proc/cpuinfo |grep \"processor\"|wc -l 1.4. 非特定的系统指令 一些特殊的通用命令 1.4.1. WATCH 将watch加在前面可以监控一些信息的实时变化 watch ps -aux watch nvidia-smi 1.4.2. HISTORY HISTORY主要针对如何找到历史指令，如何重复执行某一行指令； # show the history command idx history 1262 btm # resume this command idx, it'll get command by the idx !1262 btm 如果我们希望通过关键词搜索指令，然后进行重新调用： + 选择命令 1.4.3. ln -s Linux 软连接，类似windows系统中做快捷方式 # 在target地址建立一个名为linkname的软连接，链接到source_dir ln -s source_dir/ target_dir/linkname # 删除软连接(注意后面千万不能有/)，不然会将文件全部删除 rm -rf linkname 1.4.4. HEAD、TAIL 只显示命令行输出的前几条或者后面几条 history | head -i histo | tail -i 1.5. 远程操作 1.5.1. 启动ssh服务 service ssh start 1.5.2. 避免远程终端执行的代码中断 我们希望再退出窗口的时候没有退出终端执行中的代码的话，我们实际上可以使用一种叫做session的设置来进行操作，如果要支持session的功能，这里主要可以有两种工具： tmux (zhihu.com) screen 我们这里主要选择使用tmux，功能相对于screen来说更全面一些，分屏功能也更好用 配置文件设置 新版本的时候出现了问题，等待更新，official example，tutor 在~目录下生成配置文件，然后写入我们希望的配置 vim ~/.tmux.conf :x:主要包含相关的按键映射以及页面主题等等操作 # use alt+arrpw to switch panes bind -n M-Left select-pane -L bind -n M-Right select-pane -R bind -n M-Up select-pane -U bind -n M-Down select-pane -D # mouse mode set -g mouse on # set easier window split keys bind-key v split-window -h bind-key h split-window -v #Enable oh my zsh in tmux set -g default-command /usr/bin/zsh #################################### config color ###################################### set -g default-terminal \"screen-256color\" ## COLORSCHEME: gruvbox dark set-option -g status \"on\" # default statusbar color set-option -g status-style bg=colour237,fg=colour223 # bg=bg1, fg=fg1 # default window title colors set-window-option -g window-status-style bg=colour214,fg=colour237 # bg=yellow, fg=bg1 # default window with an activity alert set-window-option -g window-status-activity-style bg=colour237,fg=colour248 # bg=bg1, fg=fg3 # active window title colors set-window-option -g window-status-current-style bg=red,fg=colour237 # fg=bg1 # pane border set-option -g pane-active-border-style fg=colour214 #fg2 set-option -g pane-border-style fg=colour237 #bg1 # message infos set-option -g message-style bg=colour239,fg=colour223 # bg=bg2, fg=fg1 # writing commands inactive set-option -g message-command-style bg=colour239,fg=colour223 # bg=fg3, fg=bg1 # pane number display set-option -g display-panes-active-colour colour214 #fg2 set-option -g display-panes-colour colour237 #bg1 # clock #set-window-option -g clock-mode-colour colour109 #blue set-window-option -g clock-mode-colour colour239 #blue # bell set-window-option -g window-status-bell-style bg=colour167,fg=colour235 # bg=red, fg=bg ## Theme settings mixed with colors (unfortunately, but there is no cleaner way) set-option -g status-justify \"left\" set-option -g status-left-style none set-option -g status-left-length \"80\" set-option -g status-right-style none set-option -g status-right-length \"80\" set-window-option -g window-status-separator \"\" #################################### config status ###################################### set-option -g status-left \"#[fg=colour248, bg=colour241] #S #[fg=colour241, bg=colour237, nobold, noitalics, nounderscore]\" set-option -g status-right \"#{prefix_highlight}#[fg=colour239, bg=colour237, nobold, nounderscore, noitalics]#[fg=colour246,bg=colour239] %Y-%m-%d %H:%M #[fg=colour248, bg=colour239, nobold, noitalics, nounderscore]#[fg=colour237, bg=colour248] #h\" set-window-option -g window-status-current-format \"#[fg=colour237, bg=colour214, nobold, noitalics, nounderscore] #[fg=colour239, bg=colour214] #I #[fg=colour239, bg=colour214, bold] #W #[fg=colour214, bg=colour237, nobold, noitalics, nounderscore]\" set-window-option -g window-status-format \"#[fg=colour237,bg=colour239,noitalics]#[fg=colour223,bg=colour239] #I#[fg=colour223, bg=colour239] #W #[fg=colour239, bg=colour237, noitalics]\" 基本的常用操作 tmux 前缀按键：ctrl+b # tmux 新建，离开，重连，关闭，列表，重命名 tmux new -s tmux detach # 或者prefixKey + d tmux attach -t tmux kill-session -t # 或者直接exit tmux kill-windows -t tmux ls tmux rename-session -t # prefixkey + b 1.5.3. SCP：linux之间传文件（好像也支持和Windows互传） 使用指令scp -P localfile username@ip remotepath 出现问题：permission denied：使用chmod 修改远程文件夹权限，774 or 777 具体指令Google it 1.5.4. SZ、RZ命令 sz、rz命令是Linux、Unix与Windows进行ZModem文件传输的命令； sz： sent zmodern 从服务器传输文件到的本地 rz：reveice 从windows传递文件到Linux服务器 if we want use this we need to inster 'lrzsz' first： 1.6. 环境配置 查看当前的ubuntu版本： # method 1 cat /proc/version # method 2 cat /etc/issue 基础命令：修改密码passwd 1.6.1. Install Vim/NVim/SpaceVim vim和nvim的传统配置在vim的教程中另外编写，不在这里进行赘述 # install vim sudo apt-get install vim # install nvim # sudo apt-add-repository ppa:neovim-ppa/stable # sudo apt-get update sudo apt-get install neovim # install the spacevim jump to the spaceVim doc 1.6.2. Install Monitor Install Resource Monitor for Linux Sys. Including: Htop, Bottom, Zenith then we will describe the usage and install method of it. reference: Top Terminal Based Monitoring Tools for Linux | ComputingForGeeks Zenith (reposhub.com) ClementTsang/bottom (github.com) bvaisvil/zenith (github.com) Htop this is the easiest one, we can download it by default command. sudo apt-get install htop usage htop Bottom we need to use curl download the packages, then install it like below: ubuntu/debian: curl -s https://api.github.com/repos/ClementTsang/bottom/releases/latest | grep browser_download_url | grep amd64.deb | cut -d '\"' -f 4 | wget -qi - sudo apt install ./bottom*.deb usage: btm Zenith we need to install cargo/rust first, then we can download Zenith like Bottom, but acturally we only need one of it. ubuntu/debian: This way is not supposed nvidia-GPUs. # install the rust first sudo apt-get install rustc # download the package of Zenith curl -s https://api.github.com/repos/bvaisvil/zenith/releases/latest | grep browser_download_url | grep linux | cut -d '\"' -f 4 | wget -qi - # unzip it and install tar xvf zenith.linux.tgz # change the mode of shell chmod +x zenith sudo mv zenith /usr/local/bin If we want zenith show GPU infomation, we need to build it from the source code. And we will discuss this later.(learn about build/make first) placeholder usage: zenith 1.6.3. Zsh and OhMyZsh 终端设置 0xFFFF zsh & oh-my-zsh 的配置与使用 zsh、oh-my-zsh、tmux、vim 基于zsh和oh my zsh对命令行主题进行美化 # 安装zsh sudo apt-get install zsh # 修改默认shell为zsh chsh -s /bin/zsh # 安装oh-my-zsh sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" # 或者使用wget sh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" # 或者使用git clone 结合镜像站使用下载较为稳定 git clone https://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 如果我们需要将默认终端恢复bash： chsh -s /bin/bash root # h或者尝试直接输入bash 切换主题部分 除了第一个命令其他的都在文档中修改/添加 https://github.com/ohmyzsh/ohmyzsh/wiki/Themes 主题预览 # 切换主题等 code ~/.zshrc ZSH_THEME=\"tonotdo\" # （笔者比较喜欢这套主题，可自行选择，或者随机使用） setopt no_nomatch # （在文档末尾添加，解决zsh语法与bash语法不太兼容的问题，更多请参考[1]） export TERM=xterm-256color #（设置终端色彩模式，vim使用airline增强状态栏，需要此模式） zsh插件安装 oh my zsh插件安装；zsh插件配置 安装语法高亮：zsh-syntax-highlighting 通过命令的颜色来判断输入的指令是否是正确的 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 安装自动补全：zsh-autosuggestions 主要是基于之前的命令来进行不全 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 找到对应的安装位置，在对应的.zshrc中进行配置然后激活配置文件即可。 出现的问题 没有conda指令 # 换用别的终端 conda init zsh conda activate消失 解决办法： # 保守方法，已测试 workflow如下 # 先将主题切换为bash # 在vscode的默认启动项中将启动的terminal切换回bash chsh -s /bin/bash # 卸载oh_my_zsh, and zsh sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/uninstall.sh)\" apt-get remove zsh # 进入portainer，在portainer的bash中执行 conda init # 之后在所有环境中都不需要再执行这个activate的操作了 但是好像在mobaxterm中还是需要手动激活一下，但是至少activate回来了 1.7. Anaconda 安装 1.7.1. 参考文档 如何在 Ubuntu 20.04 上安装 Anaconda；oh-my-zsh主题支持conda虚拟环境；oh-my-zsh主题显示conda环境名称 1.7.2. 下载安装 找到最新的安装包版本：Index of anaconda 后选择对应的版本安装 wget -P /tmp https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh 可选）验证下载版本的正确性： sha256sum /tmp/Anaconda3-2021.05-Linux-x86_64.sh # 查看输出的哈希值和对应的archive上的值是一致的 安装anaconda sh /tmp/Anaconda3-2021.05-Linux-x86_64.sh # 按照intro应该会执行conda init 等等操作，如果没有我们可以自行执行 1.7.3. 环境激活 # 假如当前的shell是zsh bash source ~/.bashrc conda init zsh # 退出bash回到zsh source ~/.zshrc 1.7.4. 关闭Conda的命令行提示 conda config --set changeps1 false 1.7.5. Anaconda 查看添加删除 conda config --show-sources 1.8. 注意事项 @Aiken 2020 本文档主要记录一些Linux下面遇到的问题解决方法： 1.8.1. python环境无法识别 安装完linux以后在bash没法执行conda命令，以及识别不出conda中安装的环境，而从portainer中可以直接启动python的非对等偏差问题。 source /opt/conda/bin/activate conda activate base 应该是由于没有将conda的自启动加入docker中的自动运作中，所以需要自行对conda 命令进行启动。 solve update： 直接在portainer的terminal中执行如下命令即可一劳永逸 conda init 1.8.2. 删除Zsh后远程无法连接 由于设置zsh作为基本bash，导致问题的原因可能是接受ssh启动的bash 换成zsh了，而我们zsh已经卸载了，就无法修改。 解决方法，尝试从potainer控制台的console ，root登录基本的bash，在基本bash中将login shell 改成zsh。 参考链接：LINK vi /etc/passwd # passwd文件中把shell 改回bin/bash 即可 1.8.3. apt-get 找不到包 # 首先执行apt-get的更新 sudo apt-get update 1.8.4. apt-get /var/lib/dpkg/lock-frontend sudo rm /var/lib/dpkg/lock-frontend sudo rm /var/lib/dpkg/lock sudo rm /var/cache/apt/archives/lock 不知道哪个真正起了作用，都试试把 1.8.5. chsh: PAM: Authentication failure code /etc/passwd # 里面可能有一些配置出现了问题，包括 bin/bash 漏了前面的斜杠这种 1.8.6. NVCC command not found 在安装CUDA后还是找不到命令的话，可以去以下地址找一下对应的cuda文件是否存在 ls -l /usr/local/cuda # 如果存在的话，将cuda的路径导入到bashrc中 export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH export PATH=/usr/local/cuda/bin:$PATH © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-09 15:17:12 "},"Envs/Spacevim.html":{"url":"Envs/Spacevim.html","title":"2.1 Deploy SpaceVim","keywords":"","body":"1. Spacevim Configuration1.1. INSTALL SPACEVIM AND CONFIG IT1.2. INSTALL LANGs'1.2.1. COCNVIM1.3. KEYSHORT and special USAGE1.3.1. SPLIT WINDOWS and CHECKOUT1.4. Plugins1. Spacevim Configuration @Aiken 2021 this file is use to record how to config out vim' by spacevim.I'll write this doc with three Parts: Install and envs, Plugins(including the LSP), KeyShort Attention: we have much to do if we want to install some other plugins. maybe it not a good way to set the vim. INSTALL SPACEVIM AND CONFIG IT INSTALL LANGs' COCNVIM KEYSHORT and special USAGE SPLIT WINDOWS and CHECKOUT Plugins 1.1. INSTALL SPACEVIM AND CONFIG IT Install: SpaceVim via the offical websize: spacevim layers colorscheme The COMMAND is like: curl -sLf https://spacevim.org/cn/install.sh | bash After that, the spacevim will install for the vim and neovim. Basic Configuration: modify the spacevim configuration in the file below ~/.SpaceVim.d/init.toml And enable some layers we need: which can select from spc + h + l after enable those layer, DEIN will install those plugins we need use GLOBAL VPN to download plugins. something like set: wrap will be add in ~/.SpaceVim/vimrc (end of it) 1.2. INSTALL LANGs' This is the most important part for coding: lint,autocomplete,warning..At the same time, this part is hardest to install, because the coc.nvim which is not design for spacevim. FIRST OF ALL: enable those langs' layer: python(first), markdown, c++;We can install those module according to the Docs, then install sth like pynvim(pip), node js, yarn, neovim, make...THEN: run :CheckHealth after install coc to check the env status.NEXT: try install debug, c++, c, for the future dev. 1.2.1. COCNVIM Using Python As a example to show how to install this.Hardest Part here: CPP with Coc, Coc Offical Coc_issues Install nodejs and yarn: # add & update apt source before install nodejs. curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get update # install nodejs after that. sudo apt-get install -y nodejs # install yarn in shell refer to the hint like: curl --compressed -o- -L https://yarnpkg.com/install.sh | bash Install Coc in SpaceVim by dein in init.toml: [options] autocomplete_method = 'coc' [[layers]] name = 'lsp' filetype = [ 'c', 'cpp', 'dart'] [layers.override_cmd] c = ['ccls'] cpp = ['ccls'] python = ['pyls'] [[cusiom_plugins]] repo = \"neoclide/coc.nvim\" merge = 0 rev = 'release' after install using :checkhealth 'CocInfo' to comfirm. Install some basic part(jedi): conda install jedi :CocInstall coc-jedi coc-python coc-snippets :CocInstall coc-python :CocInstall coc-clangd coc_keyword: some basic coc command we may use often CocInstall [PackageName] CocUninstall [PackageName] 1.3. KEYSHORT and special USAGE reinstall some plugins can use: SPReinstall coc.nvim. running/debug info will record in SPDebugInfo to_tree will show in spc a o after we save the modify of file 1.3.1. SPLIT WINDOWS and CHECKOUT split windows to show more info and make it easily to code. sp [filename] to splite windows with new files. u-d vsp [filename] to splite windows with new files. l-r spc [num] checkout cursor in diff windows g t checkout from tag to tag 1.4. Plugins This part is depending the DEIN, so we can reference this plugins. Many useful plugins had been add in those layers, learn it from offical website. recommand1 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-10-09 20:15:35 "},"Envs/VimConfig.html":{"url":"Envs/VimConfig.html","title":"2.2 Deploy Nvim","keywords":"","body":"1. Chapter1 Aiken's Vim1.1. Based on neovim1.1.1. Install1.1.2. Where is Config File1.2. Basic Config1.2.1. Basic Setting1.2.2. Folding Setting1.2.3. Color And Theme(basic)1.2.4. Mapping Shortcut1.3. Plugs1.3.1. VIM-PLUG1.3.2. Langs1.4. Appendix1.4.1. Add Header for Langs1.4.2. Dependency1.4.3. Reference1.4.4. Configuration File1.4.5. COC安装注意事项1.4.6. Airline1.4.7. Tagbar1.4.8. Startify 起始页设置1.4.9. Improve NPM download1.4.10. Vim中执行程序1.4.11. reference1.5. Split Windwos 窗口切分1.6. Vim Folding 折叠1.7. Vim Grammer 特殊用法1.7.1. Voice 语态1. Chapter1 Aiken's Vim @Aikenhong 2021 Vim is a important consistant for editing file in shell. It's Hightly Customized for Everyone, In this part I'll Show my personal Vim comfigurations And I'll Discuss about the diff between Spacevim & Neovim. Give a conclusion in advance: Recommand Config the Vim for Yourself You only need to config once, then you should save it in the cloud. You will Know all the Keyshot you setting up, and you can customize it as you want. 1.1. Based on neovim 基于NeoVim进行配置，不采用SpaveVim的配置文件，这里需要建议采用最新的测试版的NeoVim(>= 0.5)，Stable的NVim已经很久没有更新，对一些新的插件缺乏支持。 1.1.1. Install Installing Neovim Download NeoVim Package and Install from source or Install from neovim-ppa Like Following: sudo add-apt-repository ppa:neovim-ppa/unstable sudo apt-get update sudo apt-get install neovim 在安装了Python之后安装对NVim的适配 pip install neovim pip install pynvim 同时在配置文件中设置 let g:python3_host_prog='/home/aikenhong/anaconda3/bin/python' \" the path to your python\" 检查python配置情况： ” check lang suppose for neovim “ focus on the python part :CheckHealth 1.1.2. Where is Config File 可以在vim中使用:version然后在其中调用:echo $VIM查看对应的vimrc存放的地址 vim的配置文件地址： # this time is in ls -l /etc/vim/vimrc # or we can touch one in ls -l ~/.vim/vimrc nvim的配置文件的地址： ls -l ~/.config/nvim/init.vim # if not, touch one 有了配置文件以后就可以开始对Nvim进行配置 1.2. Basic Config this part I'll explain those basic configurations in Vim(NVim). Besides I will simplely introduce the syntax of .ini for writing config. 1.2.1. Basic Setting nvim初始情况就是完全没有配置的记事本，但是相应的定制化程度高，下面这些是一些固定的配置 \"===========================基本配置 set nowrap \" 不自动换行 set nu \"显示行号 set clipboard=unnamed \"共享剪切板 set nocompatible \" 不适配vi避免不兼容 set backup \"生成临时文件（maybe we should make it no） set noswapfile \" i dont like swap files set history=1000 \" 文件在外部被修改过，就重新读入 set sessionoptions+=globals \" 延迟绘制提升性能 set lazyredraw \" 显示确认 set confirm \"set paste autocmd InsertLeave * set nopaste \"结束插入模式的时候关闭paste 配置搜索视图： \" 高亮搜索结果，逐词高亮 set hlsearch set incsearch \" 搜索忽视大小写 set ignorecase set smartcase \" 显示匹配的括号 set showmatch 配置Tab和Indent： ” tab 和indent设置 set tabstop=4 \" Tab键的宽度 set expandtab set smarttab set shiftwidth=4 set autoindent set cindent set si 设置修改配置文件直接应用 autocmd BufWritePost $MYVIMRC source $MYVIMRC 配置调试python,cpp,sh：使用F5执行输出，默认删除编译的c++ set showcmd \"show the cmd before carry out on vim map :call CompileRunGcc() func! CompileRunGcc() exec \"w\" if &filetype == 'cpp' exec '!g++ % -o % \"running python in nvim \"nnoremap :echo system('python3 \"' . expand('%') . '\"') \" running cpp in nvim \" map :w :!g++ % -o % \" nnoremap :!clear;g++ % -o % && ./% 支持中文编码： set encoding=utf-8 set termencoding=utf-8 set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936 支持鼠标操作： set mouse=a set selection=exclusive set selectmode=mouse,key 1.2.2. Folding Setting THE RESULT AFTER SETTING WILL BE LIKE THIS And the Folding can be ：mark,indent,syntax This is not the final version of folding setting, we will contiune complete it. \" https://www.cnblogs.com/zlcxbb/p/6442092.html \" set foldmethod=indent set foldlevel=1 \"预设开始时不收起 autocmd FileType vim set foldmarker={{{,}}} autocmd FileType vim set foldmethod=marker autocmd FileType vim set foldlevel=0 autocmd FileType python,cpp set foldmethod=indent autocmd FileType python,cpp set foldlevel=1 \"autocmd FileType cpp set foldmethod=marker \"autocmd FileType cpp set foldmarker={,} \"autocmd FileType cpp set foldlevel=1 \" let php_folding=1 set foldnestmax=3 1.2.3. Color And Theme(basic) Trending vim color schemes | vimcolorschemes mkdir ~/.config/nvim/colors # them we dowmload theme in this dir 在Plug同级目录下创建colors文件夹，将对应的配色文件放到colors，autoload中， set wildmenu set background=dark colorscheme NeoSolarized highlight Visual cterm=NONE ctermbg=236 ctermfg=NONE guibg=Grey40 highlight LineNr cterm=none ctermfg=240 guifg=#2b506e guibg=#000000 \" 背景透明 \"hi Normal ctermfg=252 ctermbg=none \"背景透明 autocmd vimenter * hi Normal guibg=NONE ctermbg=NONE \" transparent bg “ 语法高亮，高亮当前行，当前列 syntax on set cul \"highlight cursorline set cuc \" set termguicolors set t_Co=256 \" 设置状态栏 set laststatus=2 set ruler \" set themes \" gruvbox \" colorscheme gruvbox ” =====================指定的主题设置 \" NeoSolarized colorscheme NeoSolarized let g:neosolarized_termtrans=1 runtime ./colors/NeoSolarized.vim \" Onedark \" https://github.com/joshdick/onedark.vim \" we should change this in the airline setting \" colorscheme onedark \" let g:airline_theme = 'onedark' \" space-vim-dark \" https://github.com/liuchengxu/space-vim-dark \" colorscheme space-vim-dark \" hi Comment cterm=italic \" hi LineNr ctermbg=NONE guibg=NONE \" one-half \" https://github.com/sonph/onehalf/tree/master/vim \" colorscheme onehalfdark 1.2.4. Mapping Shortcut 快捷键映射是配置自定义的核心内容，这一块会分享一些比较特别的映射。 Shortcut key mapping is the core content of configuration customization. This section will share some special mappings. 快捷键配置 修改位置和方法 插件的默认配置 .config/nvim/plugins/NAME/PLUG.vim 1. 修改对应的快捷键2. 主配置文件中duplicate 对应的命令 默认的配置文件 在不同的命令下分别对应1. 普通模式下的映射2. 可视模式下的映射 配置使用进行windows的切换 \" https://vim.fandom.com/wiki/Switch_between_Vim_window_splits_easily nmap :wincmd k nmap :wincmd j nmap :wincmd h nmap :wincmd l 配置tab和shift tab来实现vscode中的行缩进配置 nmap V> nmap V Vg> vmap Vg 特殊命令： \" del the end space of line nnoremap de :%s/\\s\\+$//:let @/='' \" edit the nvim config file nnoremap ev :vsp $MYVIMRC 复制粘贴全选： map ggVGY map! ggVGY map gg=G ” 选中状态下ctrl+c复制 vmap \"+y 1.3. Plugs 插件是个性化配置的另一个核心点；Plugs is another core of personalized configuration 使用插件来支持语法，外观，补全，文件管理 ； Use plugins to support syntax, appearance, completion, and file management ； 1.3.1. VIM-PLUG junegunn/vim-plug: Minimalist Vim Plugin Manager (github.com) 你只需打开终端并运行以下命令： $ curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim Neovim 用户可以使用以下命令安装 Vim-plug： $ curl -fLo ~/.config/nvim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 安装完插件管理器，我们可以在配置文件中通过一下的操作来安装插件： # 设置存放插件的地址 call plug#begin('~/.config/nvim/plugged') # 添加我们需要的插件 call plug#end() 常用的一些命令： Command Desc PlugInstall Install Package PlugUpdate Update those packages PlugStatus Show The Status of all the package PlugClean Clean the error Plug or del those Plug not Define any more PlugUpgrade Update Vim-Plug itself 常用的一些命令优化下载速度更换源： 加速vim-plug安装，cnpm，github # 在调用call plug之前设置下载的源地址 # 这里实际上可以参考各种镜像站去写 let g:plug_url_format='https://git::@hub.fastgit.org/%s.git' 1.3.2. Langs 1.4. Appendix 1.4.1. Add Header for Langs “ head can write like this \" 新建.c,.h,.sh,.java文件，自动插入文件头 autocmd BufNewFile *.cpp,*.[ch],*.sh,*.py exec \":call SetTitle()\" \"\"定义函数SetTitle，自动插入文件头 func SetTitle() \"如果文件类型为.sh文件 if &filetype == 'sh' call setline(1, \"##########################################################################\") call append(line(\".\"), \"# File Name: \".expand(\"%\")) call append(line(\".\")+1, \"# Author: AikenHong \") call append(line(\".\")+2, \"# mail: h.aiken.970@gmail.com\") call append(line(\".\")+3, \"# Created Time: \".strftime(\"%c\")) call append(line(\".\")+4, \"\") endif if &filetype == 'cpp' call setline(1, \"/*\") call append(line(\".\"), \"# File Name: \".expand(\"%\")) call append(line(\".\")+1, \"# Author: AikenHong \") call append(line(\".\")+2, \"# mail: h.aiken.970@gmail.com\") call append(line(\".\")+3, \"# Created Time: \".strftime(\"%c\")) call append(line(\".\")+4, \" */\") call append(line(\".\")+5, \" \") call append(line(\".\")+6, \"#include \") call append(line(\".\")+7, \"#include \") call append(line(\".\")+8, \"#include \") call append(line(\".\")+9, \"#include \") call append(line(\".\")+10, \"#include \") call append(line(\".\")+11, \"#include \") call append(line(\".\")+12, \"#include \") call append(line(\".\")+13, \"#include \") call append(line(\".\")+14, \"#include \") call append(line(\".\")+15, \"\") call append(line(\".\")+16, \"using namespace std;\") call append(line(\".\")+17, \"\") call append(line(\".\")+18, \"int main()\") call append(line(\".\")+19, \"{\") call append(line(\".\")+20, \" \") call append(line(\".\")+21, \" \") call append(line(\".\")+22, \" return 0;\") call append(line(\".\")+23, \"}\") endif if &filetype == 'python' call setline(1, \"\\\"\\\"\\\"\") call append(line(\".\"), \"# File Name: \".expand(\"%\")) call append(line(\".\")+1, \"# Author: AikenHong \") call append(line(\".\")+2, \"# mail: h.aiken.970@gmail.com\") call append(line(\".\")+3, \"# Created Time: \".strftime(\"%c\")) call append(line(\".\")+4, \"\\\"\\\"\\\"\") call append(line(\".\")+5, \"\") endif \"新建文件后，自动定位到文件末尾 autocmd BufNewFile * normal G endfunction 1.4.2. Dependency This Session I’ll intrduce some dependency for those PLUGs and the env 1.4.3. Reference Yggdroot/LeaderF : basic config and keyshort setting liuchengxu/vim-which-key Config your which key vim-airline-themes Vim Awesome plasticboy/vim-markdown Neovim+Coc.nvim配置 目前个人最舒服终端编辑环境(Python&C++) - zprhhs - 博客园 (cnblogs.com) init.vim · SpringHan/nvim - Gitee.com Python； C：Using g++ instead of gcc will support C++ 1.4.4. Configuration File 1.4.5. COC安装注意事项 安装npm，node npm install -g neovim build/index.js not found, please install dependencies and compile coc.nvim by: yarn insta 切换到coc目录，yarn install， cd ~/.config/nvim/plugged/coc.nvim yarn install yarn build 安装clang： Getting started (llvm.org) 1.4.6. Airline Install Powerline Font powerline/fonts: Patched fonts for Powerline users. (github.com) 1.4.7. Tagbar 该安装依赖于ctags为了支持Markdown情况下的Tagbar,这里推荐安装Universal Ctags 使用镜像站clone cd .install/ sudo git clone https://github.com.cnpmjs.org/universal-ctags/ctags.git cd ctags 安装前置依赖 sudo apt-get install make \\ autoconf \\ ppkg-config sudo apt install \\ gcc make \\ pkg-config autoconf automake \\ python3-docutils \\ libseccomp-dev \\ libjansson-dev \\ libyaml-dev \\ libxml2-dev 在ctags目录下安装 ./autogen.sh ./configure --prefix=/where/you/want # defaults to /usr/local make sudo make install 验证安装成功与否 ctags # sueecess will output # ctags: No files specified. Try \"ctags --help\" support markdown Add those into vimrc let g:tagbar_type_markdown = { \\ 'ctagstype' : 'markdown', \\ 'kinds' : [ \\ 'c:chapter:0:1', \\ 's:section:0:1', \\ 'S:subsection:0:1', \\ 't:subsubsection:0:1', \\ 'T:l4subsection:0:1', \\ 'u:l5subsection:0:1', \\ ], \\ 'sro' : '\"\"', \\ 'kind2scope' : { \\ 'c' : 'chapter', \\ 's' : 'section', \\ 'S' : 'subsection', \\ 't' : 'subsubsection', \\ 'T' : 'l4subsection', \\ }, \\ 'scope2kind' : { \\ 'chapter' : 'c', \\ 'section' : 's', \\ 'subsection' : 'S', \\ 'subsubsection' : 't', \\ 'l4subsection' : 'T', \\ }, \\ } 1.4.8. Startify 起始页设置 Code Yarns – How to create ASCII art of text using FIGlet Vim project switcher using Startify (ricostacruz.com) 1.4.9. Improve NPM download Download Npm Improve 临时设置： npm --registry https://registry.npm.taobao.org install express 全局使用： npm config set registry https://registry.npm.taobao.org 验证配置是否成功: npm config get registry 通过cnpm使用： npm install -g cnpm --registry=https://registry.npm.taobao.org 1.4.10. Vim中执行程序 - 1.4.11. reference - - 1.5. Split Windwos 窗口切分 this part is about windows split which is like tmux. vim的窗口切分命令,在命令行的模式下执行 :vsp filepath/filename # 垂直切分屏幕 :sp filepath/filename # 横向切分屏幕 窗口,缓冲区切换快捷键 # cursor change in diff windows ctrl + ctrl + w + direction # change buffer in diff tabs alt + 1.6. Vim Folding 折叠 vim折叠快捷键 1.7. Vim Grammer 特殊用法 Using Vim KEYSHORT like write an article with special grammar. In the way, there'll be some interesting usage. 这里有一些有趣的用法，通过vim的语法可以列出来 1.7.1. Voice 语态 动词：r replace, d delete, y yank, f find, v visual 介词: i in, a around, t to, f forward 名词: w word, p paragraph, t tag, s sentence © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-17 15:33:09 "},"Envs/Vimtutor.html":{"url":"Envs/Vimtutor.html","title":"2.3 Vim Baisc Usage","keywords":"","body":"1. Vim Tutor Notebook1.1. delete command1.2. skip words and lines1.3. undo and resume1.4. replace and change1.5. location and file status1.5.1. search command1.5.2. find the matched parentheses 找到对应的括号1.5.3. substitute command 替换命令1.6. EXECUTE AN ECTERNAL COMMAND1.7. THE OPEN COMMAND1.8. COPY AND PASTE1.9. SET OPTION1.10. KEYSHORT1. Vim Tutor Notebook this is the Note record the vimtutor (the basic usage of vim.) @Aiken 2021 write some word and we can use shift+a to insert in the end. the doc with Chinese delete command skip words and lines undo and resume replace and change location and file status search command find the matched parentheses 找到对应的括号 substitute command 替换命令 EXECUTE AN ECTERNAL COMMAND THE OPEN COMMAND COPY AND PASTE SET OPTION KEYSHORT 1.1. delete command Most of the command can use NUM to repeat it. d num command means delete num times with args below: c means del and change mode to insert: w delete next num words e delete cur word d delete this line which is not support 'c' $ delete to the end of the line x means delete this cur 1.2. skip words and lines e means jump the end of the word 3e: means skip 3word distance 2w: means skip 2word the num can be decide by ourself.(in the most commands) 1.3. undo and resume u means undo. U undo the line. ctrl+r means resume which is contrast undo. 1.4. replace and change r replace char with new input R become replace mode, replace word by input util we press esc 1.5. location and file status ctrl+g will show the location in file and the file status. gg: move to the head of the file. G: move to the end of the file. idx + G: jump 2 the line. shift+6: jump 2 the head of the line. 1.5.1. search command typing: / to search it. if we want to search same word, just type n, in another order N using ctrl+o to go back the cursor location, ctrl+i to go next using ? instand of / if we want search in the inverse order. 1.5.2. find the matched parentheses 找到对应的括号 typing % near the ( { [, it'll jump to another. this is very useful in debugging a program 1.5.3. substitute command 替换命令 :idx0,idx1s/old/new/g replace old with new in the line between [idx0,idx1] :%s/old/new/g replace all the old with new in whold file :%s/old/new/gc find out all old and we willdecide change it or not manually. 1.6. EXECUTE AN ECTERNAL COMMAND how to execute an command like shell command? using :![command] -[args] :w [filename] can save this file in this position or save change or it. :!rm [filename] :v choose those text or code we want to save(not all this file, just what we selected) and :w [filename] to save it :r [filename] will resume the txt of the file in this cursor. :r !dir will read the command output and puts it below the cursor 1.7. THE OPEN COMMAND o means will insert a line UNDER the cursor O will insert ABOVE the cursor 1.8. COPY AND PASTE y copy command. yw copy a word p paste(put) command v visual mode, select those char we want. $ jump to the end of the line 1.9. SET OPTION set an option so a search or subsititute ignore case after type in ':/ignore' then type in ::set ic will ignore case :set is 部分显示匹配的搜索短语 :set hls 高亮显示所有匹配的短语 :set no+ 前置no可以关闭选项 1.10. KEYSHORT Ctrl + f/b : 往下/上翻页 Ctrl + e/y : 往下/上滚动 V: 列选择模式 U/u: 选中的单词变成大/小写 Ctrl + w: 光标窗口切换 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-10-02 18:06:23 "},"Envs/SSH.html":{"url":"Envs/SSH.html","title":"3 init SSH","keywords":"","body":"1. SSH 系列相关操作1.1. ssh-key1.2. Linux 开放远程权限1.3. Git&Github1.4. ssh 免密认证1. SSH 系列相关操作 @Aiken 2021 主要介绍ssh服务在以下的几个方面（windows，linux）的使用情况：远程服务器连接（22），git&github（gitee），vscode免密登录。 1.1. ssh-key GITHUB关于SSH的教程 &#x1F448;可以直接切换成中文模式的 查看是否已存在 $ ls -al ~/.ssh 初始化 / 生成 ssh key // github 推荐，优先度从上到下递减 $ ssh-keygen -t ed25519 -C \"your_email@example.com\" // if not support $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" // tradition $ ssh-keygen -t rsa -C \"chenlangl@outlook.com\" 将ssh添加到github的个人权限界面中 免密登录 在github的教程中也有另一种方式来实现免密登录，好像是ssh-agent的方式安全的保存密码。 1.2. Linux 开放远程权限 设置sshd配置文件 | 允许passwd登录root 1.3. Git&Github 官方文档介绍的一些权限错误的地址：https://docs.github.com/en/github/authenticating-to-github/error-permission-denied-publickey 初始化git的用户配置，可以按照电脑id来进行命名其实区分起来还是好弄一些。 $ git config --global user.name \"YOURNAME\" $ git config --global user.email YOUEMAILADRESS // 查看相关的配置信息 $ git config --list 将本机的ssh公钥(public)放到GITHUB账户下的ssh管理地址，执行测试 $ ssh -T git@github.com 没有问题的话就可以直接进行clone，之类的git操作了 // 小trick，不拉取历史的commit $ git clone --depth=1 REPO_ADRESS 1.4. ssh 免密认证 windows(Local) - Linux(Services) :Link1 Pro 实际上不光是VsCode，可以在本机上通过ssh服务免密登录服务器了，这一块好像可以通过公钥和私钥两种方式来做，在这里我们首先使用公钥来测试成功。 具体的操作如下： $ cd /root/.ssh // 创建authorized_kes $ touch authorized_kes // 在其中填入我们需要远程登录的服务器的ssh pub key，在这里就是windows本机的。 // 修改权限 $ sudo) chmod 600 authorized_kes $ sudo) chmod 700 ~/.ssh/ 然后检查密钥登录的功能是否开启 // 修改相应的ssh配置文件 $ code ./etc/ssh/sshd_config 查看其中的这两项配置是否打开： RSAAuthentication yes PubkeyAuthentication yes 可以禁用密码登录，但是这样的方式可能会导致后面挂了以后直接GG，所以慎重。 重启服务 $ service ssh restart © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-17 11:11:36 "},"Envs/GitGithub.html":{"url":"Envs/GitGithub.html","title":"3.1 init Git","keywords":"","body":"1. Git Workflow1.1. Init Git1.1.1. Git&Github1.1.2. Gitignore1.2. Git 常用命令1.2.1. 远程操作1.2.2. 暂存区处理1.2.3. commit 处理1.3. Commit Standard1.3.1. Git Rebase1.4. Bug Fixed1.4.1. 大文件1.4.2. openssl error 100541.4.3. time out port4431.4.4. server certificate verification failed. CAfile1. Git Workflow @Aiken 2020(2021) 1.1. Init Git 包含ssh的详细指令在ssh的文档中，这边只介绍设置完这一系列操作之后的git初始化，主要是初始化ssh，并将私钥放到github或者gitee的账户中。 建议用Pc的名字来做标识 git config --global user.name \"YOURNAME\" git config --global user.email YOUEMAILADRESS # 查看相关的配置信息 git config --list 1.1.1. Git&Github 官方文档介绍的一些权限错误的地址：https://docs.github.com/en/github/authenticating-to-github/error-permission-denied-publickey 将本机的ssh公钥(public)放到GITHUB账户下的ssh管理地址，执行测试 ssh -T git@github.com 没有问题的话就可以直接进行clone，之类的git操作了 # 小trick，不拉取历史的commit git clone --depth=1 REPO_ADRESS 1.1.2. Gitignore 对文档创建相应的忽略文件，然后在里面编写要忽略的文件，文件夹就可以了，特殊的通配符多了解。 通过VsCode的插件或GitHub的Doc对不同语言进行初始化配置 touch.gitignore Git忽略提交规则 - .gitignore配置运维总结 1.2. Git 常用命令 普通 command 分支 command 创建本地repo git init 创建/显示分支 git branch 工作区状态 git status 切换分支 git checkout 添加到暂存区 git add /. 创建&切换 git checkout -b 暂存区到本地 git commit -m ‘mesg’ 合并分支 git merge 日志 git log (–oneline) 删除分支 git branch -d 拉取远程库 git pull 推送本地分支 git push origin 克隆远程库 git clone 撤销 标签 撤销工作区修改 git checkout – 创建标签 git tag 撤销暂存区修改 git reset HEAD 显示所有标签 git tag 撤销本地库修改 git reset –hard 删除标签 git tag -d 远程 储藏 · 同步本地库和 git remote add origin xx@y 保存现场 git stash 远程库 git push -u origin master 恢复现场 git stash pop 1.2.1. 远程操作 推送本地的分支到远程的指定分支 git push origin local:remote # 冒号前本地，冒号后远程 git pull origin remote:local git fetch: 实际上pull = fetch + merge git fetch git log -p FETCH_HEAD git merge FETCH_HEAD 1.2.2. 暂存区处理 清除暂存区某个文件的指令（通常是为了修改.gitignore）的时候执行 git rm -r --cache filename 看暂存区有什么文件 git ls-files 1.2.3. commit 处理 撤销commit git reset --soft HEAD^ # 撤销当前commit git commit --amend # 重写当前commit 合并commit rebase： # 查看多个commit的hash值 git log # 找到需要合并的最早commit的上一个的ID git rebase -i # 修改pick squre状态合并commit 1.3. Commit Standard commit message 格式： Type(scope):subject Type：说明git commit的类别，限定标记类型 Scope：用于说明影响的范围，非必须 Subject：简短描述，建议使用中文可以更清楚 Type类型限定表格 TYPE 描述 TYPE 描述 feat 新功能 Fix/to 修复bug（to只产生diff） docs 文档 style 样式（不影响代码运行） refactor 重构 perf 优化（性能和体验） test 增加测试模块 chore 工具变动 revert 返回到上个版本 merge 代码合并 sync 同步主线或分支的bug 1.3.1. Git Rebase 通过rebase命令来进行merge，保持remote和local的commit整洁有效 1.4. Bug Fixed 1.4.1. 大文件 这一部分写的有点小瑕疵，到时候就看超链接 郑宇；主要是要将大文件排除追踪，在commit之前都还是比较好解决的，但是如果已经提交上去了就稍微比较麻烦，尝试将其中的大文件删掉。 # 1. 运行gc，生成pack文件`–prune = now` 表示对所有的文件都做修剪 git gc --prune=now # 2. 找出最大的k个文件，以3为例 git verify -pack -v .git/objects/pack/*.idx |sort -k -3 -n |tail -3 # bug: cannot open ///bad .. # 可能是由于地址出错了，修改地址，如下是查看地址的代码 find .git/objects/ -type -f # 3. 查看那些大文件究竟是谁，按照上一步输出的hash value 进行搜索，（不用全长） git rev-list --objects --all |grep # 4. 移除对该文件的追踪引用 git filter-branch --force --index-filter \"git rm --cache --ignore-unmatch ''\" --prune-empty --tag-name-filter cat -- --all # 5. 进行repack git for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin git reflog expire --expire=now --all git gc --prune=now # 6. 查看pack的空间使用情况 git count-objects -v # 7. 强制推送重构大文件 git push origin local-b:remote-b --force 1.4.2. openssl error 10054 git config --global http.postBuffer 524288000 1.4.3. time out port443 just wait for some time，应该是代理的问题，不行就使用国行版github把 1.4.4. server certificate verification failed. CAfile 使用github.com.cnpmjs.org国内镜像站的时候，可能会出现权限的问题，这种情况下就要对git的证书验证命令做调整，有两种策略，执行其中一种： git config --global http.sshverify false # carry out in the export GIT_SSL_NO_VERIFY=1 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-25 18:05:00 "},"Envs/Gitbook.html":{"url":"Envs/Gitbook.html","title":"4 Deploy Gitbook","keywords":"","body":"1. Gitbook Notebook1.1. Chapter1 Install1.1.1. 安装Gitbook插件1.2. Chapter2 Configure1.3. Chapter3 Deploy1.3.1. 初始化1.3.2. 构建1.3.3. Debugging1.3.4. 启动服务1.4. Chapter4 Publish1.4.1. 托管到Github Pages1.5. Reference1. Gitbook Notebook @aikenhong 2021 Gitbook命令行工具，基于Markdown编写文档，后续基于Github发布该Blog 笔记的构建流程： 1.1. Chapter1 Install 安装Gitbook之前我们需要安装node.js和npm的依赖，使用npm安装gitbook 首先安装Install Nodejs，Npm Windows：Node.js (nodejs.org) Linux: # add & update apt source before install nodejs. curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get update # install nodejs after that. sudo apt-get install -y nodejs 然后安装gitbook npm install gitbook-cli -g gitbook fetch beta # 安装历史版本 gitbook ls-remote # 列举可以下载的版本 检查Gitbook版本 gitbook -V 1.1.1. 安装Gitbook插件 安装插件主要有两种方式：一种是直接通过book和gitbook的安装来实现，另一种是基于Npm预先安装 npm install gitbook-plugin-PACKAGE 基于book的安装方式 插件和相关的配置在book.json中指定，关键词plugins & pluginsConfig为对应的插件的配置信息 添加插件通过修改book.json如下： { \"plugins\": [ \"-summary\",\"-katex\",\"mathjax-pro\",\"anchor-navigation-ex-toc\", \"search-plus\",\"-lunr\",\"-search\",\"splitter\",\"copy-code-button\",\"github\", \"theme-comscore\",\"-latex-codecogs\",\"tbfed-pagefooter\",\"-expandable-chapters-small\", \"-edit-link-plus\",\"expandable-chapters\",\"pageview-count\",\"-editlink\", \"lightbox\",\"-highlight\",\"prism\",\"disqus\" ], } 添加完新的插件配置之后，运行gitbook install ./来安装新的插件 gitbook-plugin-mathjax-pro - npm (npmjs.com)；PrismJS/prism-themes: A wider selection of Prism themes (github.com) # gitbook-plugin-mathjax-pro 安装方式 npm install mathjax@2.7.7 npm install gitbook-plugin-mathjax-pro # and editor the book.json as below { \"author\": \"AikenHong\", \"title\": \"Aiken's Blogs\", \"language\": \"zh-hans\", \"plugins\": [ \"-summary\",\"-katex\",\"mathjax-pro\",\"anchor-navigation-ex-toc\", \"search-plus\",\"-lunr\",\"-search\",\"splitter\",\"copy-code-button\",\"github\", \"theme-comscore\",\"-latex-codecogs\",\"tbfed-pagefooter\",\"-expandable-chapters-small\", \"-edit-link-plus\",\"expandable-chapters\",\"pageview-count\",\"-editlink\", \"lightbox\",\"-highlight\",\"prism\",\"disqus\" ], \"pluginsConfig\": { \"insert-logo\":{ \"url\": \"https://gitee.com/Aiken97/markdown-image/raw/master/img/20210927180958.png\", \"style\": \"background: none; max-height: 120px; min-height: 120px\" }, \"github\":{ \"url\": \"https://aikenh.github.io/about/\" }, \"tbfed-pagefooter\": { \"copyright\":\"&copy AikenHong\", \"modify_label\": \"该文件修订时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"mathjax-pro\":{ \"version\": \"2.7.7\" }, \"prism\":{ \"css\": [ \"prismjs/themes/prism-tomorrow.css\" ], \"lang\":{ \"flow\": \"typescript\" } }, \"disqus\":{ \"shortName\": \"aikensdoc\" } } } 安装Disqus的时候，要到Disqus官网进行账号注册，并安装个人评论的网站，记录唯一的个人标识符，然后再配置中引入即可； 使用prism代替 1.2. Chapter2 Configure gitbook init会初始化文件目录，文件夹会包含如下的结构：目录中的文件对应有如下的作用 . ├── _book # 自动生成的html ├── book.json # 配置文件 ├── README.md # 电子书的前言或者简介 ├── SUMMARY.md # 电子书的目录 ├── chapter-1/ | ├── README.md # 章节的描述 | └── something.md └── chapter-2/ ├── README.md # 章节的描述 └── something.md 编辑对应的SUMMARY同时可以按照文件夹结构进行组织，基本的组织结构可以按照下面的来进行部署 # 概要 * [卷 I](part1/README.md) * [写作很棒](part1/writing.md) * [GitBook很酷](part1/gitbook.md) * [卷 II](part2/README.md) * [期待反馈](part2/feedback_please.md) * [更好的写作工具](part2/better_tools.md) 1.3. Chapter3 Deploy 在本地部署和运行一个样本书，设置gitbook的配置文件 1.3.1. 初始化 将书籍创建到当前的目录或者指定的目录中 gitbook init gitbook init ./directory # 在指定的目录中生成 1.3.2. 构建 使用下面的命令会在当前目录下或者指定目录里生成_book目录，里面的内容是静态站点的资源文件： gitbook build 1.3.3. Debugging 您可以使用选项 --log=debug 和 --debug 来获取更好的错误消息（使用堆栈跟踪）。例如： gitbook build ./ --log=debug --debug 1.3.4. 启动服务 使用下列服务在LocalHost可以预览我们的的本地书籍 gitbook serve 1.4. Chapter4 Publish 希望可以不借助gitbook服务来可视化界面，全靠git & cmd & github来进行一系列操作，这样就能通过我的onedrive来进行比较好的统一管理 1.4.1. 托管到Github Pages optional: 创建username.github.io的个人repo,可以通过jekyll来init该githubpage 创建note's repo, 用来存储自己的所有Liture 调用gitbook serve之后将_book的文件推送到repo的gh-pages分支 就可以在下列的url中看到自己的文档：aikenh.github.io/REPONAME/ 修改io文件夹中的_layout中的Default.html如下 Blog # enter the right dir # we need to excute gitbook serve int terminal first cd _book # using condtional rules to control the update actions if [ -d \".git\" ];then echo \"exist git files\" else git init fi # update those blogs using git git add . git commit -m \"update those blogs\" echo \"update the local git log\" # push the update to the remote repo git remote add origin git@github.com:AikenH/Docs.git git push -f origin master:gh-pages echo \"finish pushing files\" 1.5. Reference 集成GitHub | GitBook文档（中文版） (gitbooks.io) GitBook插件整理 - book.json配置 gitbook-notes (gitbooks.io) 内含github部署资料 Katex 测试验证 Gitbook-plugin-summary 实用配置及插件介绍 gitbook 入门教程之主题插件 Gitbook插件和主题 GitBook相关配置及优化 打造完美写作系统：Gitbook+Github Pages+Github Actions - phyger - 博客园 (cnblogs.com) 插件 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-26 12:37:09 "},"Envs/Docker.html":{"url":"Envs/Docker.html","title":"5 Docker & K8S","keywords":"","body":"1. Docker笔记1.1. 进行本机离线阅读1.2. 笔记目录（TODO）1.3. WSL2以及Docker（win）的存储位置迁移1.3.1. 默认的存储位置（用来确认迁移状态）1.3.2. 存储位置迁移1.4. 镜像获取和运行1.5. Portainer 的安装设置1.6. 常用端口映射1.6.1. 需要被映射的端口1.6.2. 可用端口1.7. 基本镜像定制1.8. Portainer 相关操作（需要更新）1.8.1. 物理机资源介绍1.8.2. 使用Portainer管理1.9. Docker BASH 美化1.9.1. 切换主题部分1.9.2. 出现的问题1.10. Connect refused1. Docker笔记 @ Aiken 2020 同时本文章将包含在windows的wsl2环境下搭建docker以及将wsl2和docker的存储位置迁移到数据盘上的问题。（后续可以将一些数据挂载到其他的盘中，比如一些实验的空间之类的，等不用了在传入原本的盘中） 参考资料：docker从入门到实践;docker volume讲解 1.1. 进行本机离线阅读 gitbook形式阅读为例： 启动：拉取镜像和运行容器. 启动服务之后再浏览器输入localhost:4000 / 127.0.0.1:4000，进行阅读即可。 停止：再CMD ctrl+c或者再Docker终端关闭容器把。 # 为了保持内容最新,建议在每次阅读之前pull镜像 # 安装完镜像以后就可以在hub中进行pull等操作了 $ docker pull ccr.ccs.tencentyun.com/dockerpracticesig/docker_practice # 国内仓库 $ docker run -it --rm -p 4000:80 ccr.ccs.tencentyun.com/dockerpracticesig/docker_practice # docker hub # $ docker run -it --rm -p 4000:80 dockerpracticesig/docker_practice 基于镜像阅读服务下载PDF 安装 gitbook：npm install -g gitbook-cli 查看是否安装成功： gitbook -V 上一步中可能会出现BUG： cb is not a function 解决方案 打开gitbook的目录并在GITHUB项目的目录中执行 gitbook serve . 之后同样可以在localhost4000 访问 gitbook pdf . 在文件目录保存pdf BUG：not such file 暂未解决，先在网上看吧 1.2. 笔记目录（TODO） 从目录开始正式做一些整理，也就是一些重要的信息，操作，以及理解。普通的一些就没必要再记录了。 定制镜像的操作，添加ssh 账号密码passwd命令之类的操作 相关参考资料，初步尝试是失败了 以及使用portainer去构建一个简单的容器的操作，让我们试一试； SSH链接的原理和实现； wsl2下docker中cuda 装pytorch的操作 https://zhuanlan.zhihu.com/p/149517344 https://zhuanlan.zhihu.com/p/337758917 https://blog.csdn.net/fleaxin/article/details/108911522 https://blog.csdn.net/weixin_42882838/article/details/106976430 https://www.cnblogs.com/dadream/p/13640143.html https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started 摘抄师兄的笔记中有用的部分； 1.3. WSL2以及Docker（win）的存储位置迁移 docker；WSL2； 首先建议先在docker-desktop中设置好绑定的WSL version，然后在一次shut down 过程中完成两者的迁移操作，避免不必要的重复操作； 1.3.1. 默认的存储位置（用来确认迁移状态） docker：%LOCALAPPDATA%/Docker/wsl wsl2: %LOCALAPPDATA%/packages/c......./local_state 1.3.2. 存储位置迁移 # 关闭wsl服务 wsl --shutdown # 查看关闭状态 wsl -l -v # 导出wsl2 system； docker-desktop & docker-desktop-data # 导出系统 wsl --export wsl --export Ubuntu-20.04 E:\\WSLWorkspace\\ubuntu.tar wsl --export docker-desktop E:\\WSLWorkspace\\docker-desktop\\docker-desktop.tar wsl --export docker-desktop-data E:\\WSLWorkspace\\docker-desktop-data\\docker-desktop-data.tar # 删除（注销）系统 wsl --unregister Ubuntu-20.04 wsl --unregister docker-desktop wsl --unregister docker-desktop-data # 导入系统到指定的新位置(使用新路径导入新系统) wsl --import Ubuntu-20.04 E:\\WSLWorkspace E:\\WSLWorkspace\\ubuntu.tar wsl --import docker-desktop E:\\WSLWorkspace\\docker-desktop E:\\WSLWorkspace\\docker-desktop\\docker-desktop.tar wsl --import docker-desktop-data E:\\WSLWorkspace\\docker-desktop-data E:\\WSLWorkspace\\docker-desktop-data\\docker-desktop-data.tar # 启动docker-desktop服务 # 启动WSL2服务（这里在terminal中会出现两个ubuntu的unid，把原本的第一个给注释掉才行） # 设置默认wsl2用户 ubuntu2004.exe config --default-user xxx # 删除多余的所有tar，over 1.4. 镜像获取和运行 Docker Hub 上有很多高质量的镜像，这是主要的镜像获取途径 找到需要的镜像以后执行相关操作拉取镜像 $ docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] #查看选项&#x1F447;，是一些比较特殊的一般情况下用不到的操作 $ docker pull -help #如果是从dockerhub中获取镜像的话一般是不需要地址和端口号的 在拥有镜像以后就可以基于镜像运行容器（本质是作为进程存在的） 如果我们希望启动bash并进行交互式操作： $ docker run -it --rm ubuntu:18.04 bash # 参考上面运行reading的步骤也可以设置local端口吧 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是 命令，这里我们希望有个交互式 Shell，因此用的是 bash。 -d : 后台模式 --name: 指定 容器的名字 --restart=always # docker重启后容器也一起重启 -p: #端口映射指令 -v: -v 本地目录:容器目录 或 -v 容器目录 其他的一些常用指令 # 查看所有镜像 （过滤器操作参见文档） docker image ls [可选参数：1仓库名：2标签] # ID和...的前面要加. docker image ls --format \"{{ID}}: {{Repository}} {{Tag}}\" # 查看镜像的体积 docker system df # 虚悬镜像的查看和删除 docker image ls -f dangling=true docker image prune # 删除镜像（不需要完整的ID只需要能区分就行） 基于ID：$ docker image rm [选项] [ ...] 基于镜像名： $ docker image rm ： 比如，我们需要删除所有仓库名为 redis 的镜像： $ docker image rm $(docker image ls -q redis) 1.5. Portainer 的安装设置 首先列出portainer的一些相关参考资料： 基本上是基于这些参考资料进行的学习 docker管理工具portainer介绍安装和使用 | Docker管理面板 | Portainer简明使用教程 Docker镜像部署与运维指南 Portainer for Win10 数据卷 | PART 1 Download And Install Image # 查询镜像 $ docker search portainer # 下载镜像（官方那个） $ docker pull portainer/portainer PART 2 单机版运行 如果我们使用的是单docker宿主主机，就直接单机运行portainer服务器，不需要进行联网操作，Just Do it： 需要注意的是，启动了WSL的windows单机实际上也要执行linux的命令才能正确的启动。 # 这样已经可以了但是还可以考虑指定数据挂载路径，下面这个是Linux的情况 $ docker volume create portainer_data $ docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce # Windows $ docker run -d -p 9000:9000 --name portainer --restart=always -v //./pipe/docker_engine://./pipe/docker_engine portainer/portainer 通过LocalHost或者指定ip以及Port(9000)登录Portainer 1.6. 常用端口映射 Container中有一些指令或者文件是需要一个端口的，所以我们需要将虚拟的端口映射到物理端口上，方便后续的远程链接，比如说ssh，所以我们需要，在启动镜像之前，进行物理端口的映射。 1.6.1. 需要被映射的端口 ssh：默认扫描 22 端口 tensorboard: 默认 6006端口，常用8008 端口 # 启动的时候可以指定端口 $tensorboard --logdir=/tmp --port=8008 jupyterlab：默认8888端口 备用和常用 9999 之类的 1.6.2. 可用端口 windows端口号相关操作：(0-65535) 建议使用5000以外的端口， # 查看所有被打开的端口（不知道是不是只能使用这些。） $netstat -an # 查看电脑端口的占用 $netstat -ano # 在相关指令下查找某个端口 for example $netstat -an0 | findstr \"80\" 1.7. 基本镜像定制 该部分主要介绍最基本的ssh开启和passwd建立的操作，设定ssh和建议指定的root账户，从而对container进行更好的管理，这样的操作可以方便环境的配置。但是实际上也可以在安装完基本docker之后在进行一个个的安装吧，我觉得这应该是可以的。 :star:至关重要师兄的文档 Pytorch Based 我们基于 pytorch官方镜像，进行Pytorch工程环境的构建，其中默认的python环境是通过conda控制的，已经集成在其中了，在下载镜像的时候我们需要明确cuda和nvdia环境. pull 镜像之后，我们采取portainer进行镜像的启动管理， 端口映射 Volume 设置物理存储卷设置 启动模式 dockerfile的构建 Dockerfile 下面是一些需要写在dockerfiles中的操作，对于命令的一些解读，我们之后还是需要进一步的了解。指令的写法和详细的作用 open ssh、passwd、git之类的安装 设置bash、root、软连接、 暴露ssh端口 设置一些依赖包 清楚copy的安装文件 基本的dockerfiles模板如下所示。如果是tensorflow的话我觉得差别也不会太大，之后补充。 # BASE IMAGE FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 # LABEL MAINTAINER LABEL maintainer=\"ltobenull@gmail.com\" SHELL [\"/bin/bash\",\"-c\"] WORKDIR /tmp # copy安装文件 COPY Python-3.6.9.tar.xz /tmp # 设置 root 密码 RUN echo 'root:password' | chpasswd \\ # 安装openssh-server 并配置 && apt-get update && apt-get -y install openssh-server \\ && sed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_config \\ && sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config \\ && mkdir /var/run/sshd \\ # 安装python依赖包 && apt-get -y install build-essential python-dev python-setuptools python-pip python-smbus \\ && apt-get -y install build-essential libncursesw5-dev libgdbm-dev libc6-dev \\ && apt-get -y install zlib1g-dev libsqlite3-dev tk-dev \\ && apt-get -y install libssl-dev openssl \\ && apt-get -y install libffi-dev \\ # 安装python 3.6.9 && mkdir -p /usr/local/python3.6 \\ && tar xvf Python-3.6.9.tar.xz \\ && cd Python-3.6.9 \\ && ./configure --prefix=/usr/local/python3.6 \\ && make altinstall \\ # 建立软链接 && ln -snf /usr/local/python3.6/bin/python3.6 /usr/bin/python3 \\ && ln -snf /usr/local/python3.6/bin/pip3.6 /usr/bin/pip3\\ # 安装pytorch && mkdir ~/.pip && echo -e '[global] \\nindex-url = https://mirrors.aliyun.com/pypi/simple/' >> ~/.pip/pip.conf \\ && pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html \\ # 清理copy的安装文件 && apt-get clean \\ && rm -rf /tmp/* /var/tmp/* EXPOSE 22 # 每次启动docker的时候都自启动ssh CMD [\"/usr/sbin/sshd\", \"-D\"] 1.8. Portainer 相关操作（需要更新） @ 基于196的实验室docker管理作为instance，然后基于自己的安装操作来写这一部分的介绍。 :star:重中之重基于3090的portainer管理 1.8.1. 物理机资源介绍 物理机的账户用于在物理机上创建自己的目录，进行管理和维护，除此之外请尽量避免直接使用物理机进行操作，避免误操作风险。 196的FS（file system）结构如下： 在物理机上进行操作的时候： 在/opt/data3/developers下创建自己名字命名的文件夹作为自己的workspace（这也是后面物理机挂载的时候的重要指标） NOTE：创建新容器或者需要较大存储空间的操作的时候要检查硬盘的余量$df-h是否足够 1.8.2. 使用Portainer管理 在这种高度可视化的界面下，已有的Image的使用，从dockerhub pull等的操作就不多说了（优先使用本地的）， 通常登录的端口都是9000：202.117.43.196:9000 My Account（密码同）：hongzx 新容器的创建注意事项 记得勾选auto remove 端口设置（也就是旁边的端口转发） 具体需要设置的参见上面的常见端口的安排 196的端口占用表（我的部分）： user Occupy hongzhenxin 23600-23699 数据卷挂载&GPU设置 包括将存储数据挂载到物理机上的指定盘点，以及开放CUDA的使用 绑定路径设置（主要的文件要传到挂载的路径下面） GPU设置： 1.9. Docker BASH 美化 基于zsh和oh my zsh对命令行主题进行美化 # 安装zsh $ sudo apt-get install zsh # 修改默认shell为zsh $ chsh -s /bin/zsh # 安装oh-my-zsh $ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" # 或者使用wget $ sh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" 如果我们需要将默认终端恢复bash： $ chsh -s /bin/bash root # h或者尝试直接输入bash https://traceme.space/blog/page/6/zshoh-my-zshtmuxvimdocker/ 配置教程 1.9.1. 切换主题部分 除了第一个命令其他的都在文档中修改/添加 https://github.com/ohmyzsh/ohmyzsh/wiki/Themes 主题预览 # 切换主题等 $ code ~/.zshrc $ZSH_THEME=\"tonotdo\" # （笔者比较喜欢这套主题，可自行选择，或者随机使用） $ setopt no_nomatch # （在文档末尾添加，解决zsh语法与bash语法不太兼容的问题，更多请参考[1]） $ export TERM=xterm-256color #（设置终端色彩模式，vim使用airline增强状态栏，需要此模式） 1.9.2. 出现的问题 conda activate消失 解决办法： # 保守方法，已测试 workflow如下 # 先将主题切换为bash # 在vscode的默认启动项中将启动的terminal切换回bash $ chsh -s /bin/bash # 卸载oh_my_zsh, and zsh $ sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/uninstall.sh)\" $ apt-get remove zsh # 进入portainer，在portainer的bash中执行 $ conda init # 之后在所有环境中都不需要再执行这个activate的操作了 但是好像在mobaxterm中还是需要手动激活一下，但是至少activate回来了 1.10. Connect refused 首先检查Linux Host 的SSH 是否启动，如果没有启动的话，手动启动，特别注意再重新run container的时候，要注意执行以下命令启动ssh service： $ service ssh start # 通过ps 查看sshd的服务是否运行 $ ps -aux | grep ssh # finished © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-28 18:39:44 "},"Langs/readme.html":{"url":"Langs/readme.html","title":"Chapter2 Langs ","keywords":"","body":"1. Chapter2 Langs1. Chapter2 Langs @AikenHong 2021 水之積也不厚，則其負大舟也無力 该部分主要整理各种编程语言的学习笔记，主要包括CPP，Python，Shell等等； 每部分主要的参考文献内容也都会在对应的笔记中体现，在这里就不再进行重复整理； 在该部分的内容中，一些特别重要和庞大的库，像PyTorch也会独立的拥有章节来进行学习和描述； 对应的一些像是数据结构，计算机网络等等重要的编程知识，后续也会在Langs的部分进行描述和补充。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 12:54:25 "},"Langs/Python.html":{"url":"Langs/Python.html","title":"1 Pyhon","keywords":"","body":"1. Python Notebook2. 数据模型（Python结构的通用范式）3. 变量赋值传递时的引用和拷贝4. 类与函数4.1. 匿名函数4.2. 单下划线4.3. Bool and or not4.4. Argparse4.5. Random5. Logging System5.1. 基础使用5.2. 进阶使用5.3. Logger与装饰器的组合使用6. FIles System6.1. Import manage6.1.1. init.py 文件的作用6.2. Path Manage6.2.1. 路径切分6.3. 文件遍历6.3.1. os.walk()6.3.2. Glob.glob()6.4. 文件读写7. Data Structural7.1. 位运算7.2. 二进制操作7.3. 序列构成的数组7.3.1. 列表推导式的使用7.3.2. 使用列表推导式生成笛卡尔积7.3.3. 生成器表达式7.3.4. 元组不仅是不可变的列表7.4. 列表的基本操作7.4.1. 列表的条件加和7.4.2. 列表的数乘7.4.3. range函数常用操作7.5. Universal Method7.5.1. Sort（）对列表进行排序7.6. 队列queue & deque7.7. SET集合7.7.1. 利用set进行去重7.8. Dict，Hashmap7.8.1. 判断字典中的key是否存在的方法7.8.2. 字典中的常用方法7.8.3. collections.defaultdict 指定dict中未定义key的value7.9. Vars（）7.10. Python中的数字日期时间计算7.10.1. 获取本机时间的几种方法7.10.2. 精确数字运算7.10.3. 数字的格式化输出8. 迭代器9. 元编程9.1. Some Rules9.2. 装饰器模块9.2.1. Basic Type9.2.2. 接受参数传递9.2.3. 修改装饰器参数9.2.4. 带可选参数的修饰器9.2.5. @property的用法10. Exception10.1. Python3 错误和异常10.2. 异常捕捉try except10.2.1. exception的多种写法和多异常分支10.3. 抛出异常 Raise Exception11. Numpy Tips11.1. reshape11.2. tolist11.3. 用array给list中的元素赋值11.4. flatten & flat operation11.5. Numpy.pad11.6. :: Numpy 索引中双冒号的实际用途11.7. Argpartition()11.8. Allclose()11.9. Clip()11.10. extract()11.11. where()12. DEBUG12.1. 避免重复/冲突的import12.2. 内存调用与Method的定义12.3. TypeError：1. Python Notebook Created by: Aiken H Detail: lang Finished?: No Tags: Code @Aiken 2021 you know what 主要参考文献：《Python Cookbook》 && 《Fluent Python》 2. 数据模型（Python结构的通用范式） （Magic method）dunder method：Python特有的双下划线方法，这些方法能够支持Python进行特殊的调用，实现通用方法在新定义的数据结构上的使用，比如最典型的: __len__()后可以支持len()，获得结构的长度 __getitem__()后可以支持data[index]来获取相应的元素，切片，等等数组的操作； # 也可以支持类似如下的一些操作 # 从数据结构中随机选出一个items from random import choice choice(datas) # 也可以支持迭代方法和反迭代方法 for data in datas: ... for data in reversed(datas): ... # 也可以支持sort函数 到这里也就说明了，只要我们在数据结构（class）中定义了相应的dunder method，该class就能支持相应的一系列操作，getitems就可以类比为列表，相应的操作都能够在python解释器下自动的赋予支持。 还有一些好用但不常用的方法： __contain__实现的是in ，当没有实现contain的方法的时候会按照顺序在list中进行搜索 __abs__ __repr__实现的是输出的format设置，也就是print的时候的输出形式 __eq__ 实现的是 == 命令，同时in调用的是eq 下面附上一张特殊方法表： 基本命名规范 相关的文件和函数等命名规则。 命名样例表 3. 变量赋值传递时的引用和拷贝 Python 变量的传递类型：（赋值过程） https://www.runoob.com/w3cnote/python-variable-references-and-copies.html Python 赋值过程中不明确区分拷贝和引用，一般对静态变量的传递为拷贝，对动态变量的传递为引用。（注，对静态变量首次传递时也是引用，当需要修改静态变量时，因为静态变量不能改变，所以需要生成一个新的空间存储数据）。 • 字符串，数值，元组 均为静态变量 • 列表，字典为动态变量。 可以用id（）查看指向的地址 在修改列表值之类的时候要注意这一点，不然可能会影响到源列表，可能要使用深拷贝的方法， copy.deepcopy() python 定义方法时候指定参数，返回值和变量的类型： def test(a:int, b:str) -> str: print(a, b) return 1000 if __name__ == '__main__': test('test', 'abc') 4. 类与函数 args，kwargs的用法和解包，主要将字典作为参数整体传入的这种方法值得学习 可以用*，**定义和解包 id()可以获取变量的地址，type（）查看数据类型，isinstance判断类型 locals().keys() 获得当前已经声明的变量列表 sys.argv[0] 可获取当前工作区的地址 4.1. 匿名函数 4.2. 单下划线 定义的函数，属性，或者方法 这表明这个member是受保护的： 是用来指定私有变量和方法的方式（只是一种约定习惯）,不希望被下游的程序员直接访问的函数。 如果使用from a_module import导入时，这部分变量和函数不会被导入 但是如果使用 import a_module这样导入模块，仍然可以用a_module._pythonPoint这样的形式访问到这样的对象。 4.3. Bool and or not 基本的就不用说了，主要是一些特殊的用法举例 # not 会先于 and 执行 if not flag1 and flag2 == True 用逻辑运算符做参数选择 judge = index == 0 and num1 or num2 4.4. Argparse 基本的用法：参考universal framework即可，主要是bool类型无法通过命令行传入 # 使用store_true属性，就可以执行默认的True or False parser.add_argument(\"--bool_chose\",default=False ,action='store_true',help='a switch of bool variable') # &#x1F447;选择上与原本完全是相反的 parser.add_argument(\"--bool_chose\",default=True ,action='store_true',help='a switch of bool variable') 4.5. Random 使用sample不重复的选取字典或者列表中的指定项 list = [1,2,3,4,5] choose = random.sample(list,2) 使用choice进行可重复的选取 c_r = np.arange(20) for i in range(10): c_i = random.choice(c_r) print(c_i) 打乱列表排序 A = [1,2,3,4,5,6] # 得到index的列表 B = np.arange(len(A)) # 对该列表进行打乱，通过打乱的列表进行索引 random.shuffle(B) print(B) 5. Logging System 日志 HOWTO — Python 3.9.4 文档；日志操作手册 — Python 3.9.4 文档 Python logging模块；logging模块的简单使用 5.1. 基础使用 从一个非常简单的例子开始，默认的命令行输出等级是warning import logging logging.debug('this message should only show up in log file') logging.info('so do this one ') logging.warning('this one will also show up in the console') logging.error('And non-ASCII stuff, too, like resund and Malm') 假如我们设置log文件的存储以及输出的格式（包括算法运行的时间） 但是注意这个config是一次性设置，只有第一次设置是有效的 logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s ', datefmt='%Y-%m-%d %I:%M:%S %p', filename=\"exampleFile.log\",level=logging.DEBUG) # 这里设置了文件的输出名称和输出的格式，以及相应的记录到文件中的等级 也可以从命令行设置日志等级,可以获取当前的等级 --log = INFO # getattr 这个方法目前好像还有点问题， 5.2. 进阶使用 通过4个module的组合来实现record log的功能，通过Logger载入多个Handler，可以通过不同的标准和方式在多个File以及控制台输出不同Level的东西，这就是主要的功能。 Untitled 具体的实现样例如下： import logging # create logger to record log messages logger = logging.getLogger('textlogger') # 避免等级c logger.propagate = 0 logger.setLevel(logging.DEBUG) # create file handler which logs even debug messages fh = logging.FileHandler('exampleFile.log') fh.setLevel(logging.WARNING) # creatr console handler... ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # create output format for all the handler formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s ', datefmt='%Y-%m-%d %I:%M:%S %p') ch.setFormatter(formatter) fh.setFormatter(formatter) # add handler to logger logger.addHandler(ch) logger.addHandler(fh) # record logs logger.debug('this message should only show up in log file {}'.format('test is')) logger.info('so do this one ') logger.warning('this one will also show up in the console') logger.error('show up twice') 但是这种格式的时候怎么实现跨文件传输呢？ 再同个文件中我们只需要进行getlogger使用同一个名字即可获得同一个logger，但是跨文件的话可能还是需要传递logger把。 我认为应该传递该logger,然后通过该Logger进行统一的输出，遇到不同的输出要求的时候，我们可以对handler进行不一样的处理从而能够得到多样化的logger输出 5.3. Logger与装饰器的组合使用 参见后续的装饰器解读模块 6. FIles System 6.1. Import manage 6.1.1. init.py 文件的作用 init.py 文件使用 其实主要就是控制着包的导入功能，使用__all__来对应from package import *的功能，我们可以在init中批量的导入我们需要的模块，这样我们就不在需要一个个的进行导入了，基于这种特性，我们也可以编写一个manage fuction，通过config来进行选择性的导入。 主要的左右是python中package的标识，不能删除 定义__all__用来进行模糊导入 编写python代码，会在import package的时候同时import，但是推荐在包中创建另外的模块来写，尽量保证该文件简单 6.2. Path Manage 6.2.1. 路径切分 将路径切分成地址和文件： import os p,f = os.path.split(origin) print(\"path == \",p) print(\"file == \",f) 切分出文件的盘符和文件名 dev,left = os.path.splitdrive(origin) 切分出文件和拓展名 f,ext = os.path.splittext(origin) 6.3. 文件遍历 6.3.1. os.walk() 简单好用的目录遍历器用于在目录树中游走输出目录中的文件名，向上或者向下。 os.walk(top,topdown,onerror,followlinks) top：遍历的目录地址 （option）topdown：True的话优先遍历top目录，否则会优先遍历子目录 （option）onerror：当需要异常的时候，会调用 （option）followlinks：是否要跳转到快捷方式（或者软连接遍历） RETURN：（root，dirs，files） root：根目录 dirs：文件夹中所有目录的名字（子文件夹） files：目录中所有文件的名字 层序遍历每次是该层的所有文件和目录的名字 6.3.2. Glob.glob() 文件遍历方法 6.4. 文件读写 7. Data Structural 7.1. 位运算 位运算判断奇偶一致性 # T：奇偶性不一致 F：奇偶性一致 (a ^ b) & 1 7.2. 二进制操作 与或非就不用多说，主要是介绍一个module bitarray 7.3. 序列构成的数组 这一部分主要有几个重点：列表推导式的使用、元组特性和使用 7.3.1. 列表推导式的使用 # 将字符串变成Unicode码位 symbols = 'sdac@#' codes = [ord(symbol) for symbol in symbols if ord(symbol) >127] # 与map和filter的比较 lists(filter(lambda c:c>127, map(ord,symbols))) 可以看出列表推导式的表达更为简洁易懂，而且实际上运行的效率也不低 7.3.2. 使用列表推导式生成笛卡尔积 举例：每个size有不同的颜色 colors = ['black','blue','red'] sizes = ['S','M','L'] # 先按颜色循环再按size循环，内外层循环的关系 tshirts = [(color,size) for color in colors for size in sizes] 7.3.3. 生成器表达式 我们可以使用列表推导来初始化元组、数组、或者其他的数据类型，但是生成器表达式符合了迭代器的协议，可以逐个的产出元素，而不是先建立一个完整的列表，能够节省内存 语法上和列表推导差不多，只不过把方括号换成圆括号而已 tuple(ord(symbol) for symbol in symbols) import array array.array('I',ord(symbol) for symbol in symbols) 利用生成器表达式来计算笛卡尔积 # 这样可以更好的体现逐个生成的特性？但是实际上列表推导式也可以把？ # 但是总之是由这样的特性的，能够避免额外的内存占用 for tshirt in ('%s %s' %(c,s) for c in colors for s in sizes): print(tshirt) 7.3.4. 元组不仅是不可变的列表 7.4. 列表的基本操作 7.4.1. 列表的条件加和 有不少类似和条件语句相关的操作，列举一些基本实例如下： # np.random.randint? A = np.random.randint(0,3,5) B = np.random.randint(0,3,5) print('origin A　is {} \\n And B is {}'.format(A,B)) # style 1 相当于转换成一个ToF的list，然后对这样的list直接进行sum same = (A == B).sum() print('\\nthe num of same element in same posi is', same) '''列表的+=，也就是简单拼接操作''' [1,2,3]+[2,3,4] 7.4.2. 列表的数乘 列表的数乘是对列表的项数进行一个重复性的扩充，但是注意这种重复不能针对那种特殊类型（也就是赋值会直接基于地址的：引用？） 所以这是对于项数的操作而不是对列表中数值的直接操作，参考变量赋值的部分 value = 5 unlist = [value] outlist = unlist * 5 print('the output is like that : {}'.format(outlist)) 7.4.3. range函数常用操作 https://docs.python.org/zh-cn/3.7/library/stdtypes.html?highlight=range#range range生成的并不是列表，而是一个range组而已 reallist = list(range(20)) # range的步长设置 for i in range(0,20,5): print(i) 7.5. Universal Method 7.5.1. Sort（）对列表进行排序 sort用于对源列表进行排序，如果指定参数，则使用指定的比较函数 参考资料：https://www.runoob.com/python/att-list-sort.html # 纯数字的情况就按基本方式进行排列 list1 = [1,2,4,5,6,23,4] list1.sort() list1 # 类似的string就按找字母表进行逐项排序吧，我是这样理解的 7.6. 队列queue & deque 7.7. SET集合 https://www.runoob.com/python3/python3-set.html {}可以定义字典，也可以用于创建集合 但是空的集合只能用set()定义（因为{}定义的是空字典） 基本的method： add、remove、discard（也是移除，但是假如元素不存在的话也不会报错） len，clear 主要是可以利用其中不会重复的元素的特性来进行特殊的操作 basker = {'apple', 'organge', 'apple', 'pear'} print('basker:', basker) 'orange' in basker a = set('go straight forward') # 可以在集合中做交并等等集合的操作 7.7.1. 利用set进行去重 如何利用set对unhashable的data structure进行去重，这里采取的方式是使用tuple对数组进行变换； 实际上unhashable的原因在于对象是可变对象：比如np.array，所以我们将其转换为不可变的tuple之后就可以进行hash的计算从而进行去重了。 # 二维数组为例 array1 = np.random.rand(3,4) array1_t = tuple(map(tuple,array1)) resume = np.array(array1_t) # 进行转换的时候注意不要进行过度的拆分，上述的方法只适用于二维数组的情况， text = ['abcsd','dsdc','cdsda'] text = tuple(text) # 即可，不然可能会将其中的文本全部拆分出来 # 后续补充一下map的其他用法。[func,iterator?] 7.8. Dict，Hashmap 实际上python中的字典就是hashmap的具体实现，是一个无序的结构 7.8.1. 判断字典中的key是否存在的方法 首先如果我们调用的key不存在的话： keyerror >>> 'key1' in dict1 false 或者使用get方法，能给不存在的key赋予默认的value,在这个时候出现的则是nameerror >>> d.get('key1') >>> d.get('key1', -1) -1 7.8.2. 字典中的常用方法 … 7.8.3. collections.defaultdict 指定dict中未定义key的value 通过指定的默认值，在一些使用场景下可以对dict进行简化的定义 同时也能针对一些特殊的情况，比如说未见数据的情况，进行定义 # 指定list类型用于未定义类别的填充 from collections import defaultdict dict1 = {} dict2 = defaultdict(list) try: print(dict1['a']) except: print('dict1 print key error') print('dict2 is like ', dict2['a']) dict1 print key error dict2 is like [] # 用法2，避免keyerror更容易对其进行赋值 from collections import defaultdict bags = ['apple', 'orange', 'cherry', 'apple','apple', 'cherry', 'blueberry'] count = defaultdict(int) for fruit in bags: count[fruit] += 1 print('the count output is like \\n', count) # print(locals().keys()) the count output is like defaultdict(, {'apple': 3, 'orange': 1, 'cherry': 2, 'blueberry': 1}) # 用法3：可以自定义函数作为初始化的函数参数 # 基于这样的方法我们可以定义各种各样的默认值 from collections import defaultdict def defaultvalue(value=2): return value dict3 = defaultdict(defaultvalue) dict3['hello'] 7.9. Vars（） vars() 函数返回对象object的属性和属性值的字典对象。 7.10. Python中的数字日期时间计算 @Aiken 2020 @Source：《Python Cookbook》 Chapter3 数字日期和时间 主要针对Python中的数字数字运算的运算做一个笔记 7.10.1. 获取本机时间的几种方法 主要为了方便格式化时间输出，我们需要将机器时间转换成指定的年月日之类的。 分别来自于time 和 datatime，这两种方式的时间复杂度好像实际上并没有太大的差别，姑且用着把暂时。 import time from datetime import datetime def get_time(type=1): if type == 0: now = time.strftime('%m/%d:%H:%M') else: now = datetime.now().strftime('%m/%d:%H:%M') return now get_time(0) 7.10.2. 精确数字运算 我们知道python中的计算不是绝对精准的， 浮点的精度是有限的，但是当我们需要进行金融领域或者数学领域的一些高精度要求的计算，可以为其牺牲一定的复杂度的时候&#x1F449;decimal模块 from decimal import Decimal a = Decimal('4.2') b = Decimal('2.1') print(a + b, a+b==Decimal('6.3')) # 注意数据的类型实际上也是Decimal # 能控制计算的每一方面，包括数字位数和四舍五入之类的，需要创建一个本地的上下文 from decimal import localcontext # 精确度控制 with localcontext() as ctx: ctx.prec = 3 print(a/b) 计算方法中的大数吃小数的情况 (运算中的量纲差异超过17位的浮点数精度的情况)使用math.fsum()函数 import math nums = [1.23e+18，1，-1.23e+18] assert sum(nums) != math.fsum(nums), 'the correct ans is fsum {}, error ans is sum {}'.format(math.fsum(nums),sum(nums)) # we can find it what we meet and waht we want. 7.10.3. 数字的格式化输出 控制输出的格式（精确度，对齐，千分位分割符）format x = 1234.56789 anslist = [] value = format(x, '0.2f') # &#x1F448; 两位小数 anslist.append(value) Untitled 进制转换： 2,8,16 -> bin（） oct（） hex（） OR format(x, ‘b’) format(x, ‘o’) format(x, ‘h’) 复数运算 complex(real, imag)``numpy好像能处理复数cmath一些math无法处理的复数运算 正负无穷于NaN（非数字） inf，-inf，nan， 可以使用float(‘inf’)创建验证 math.isinf() 分数运算 Fractions（5，4）==5/4.numerator 分子 .denominator 分母 8. 迭代器 主要包括迭代的模块和解包的一些相关操作： enumerate 、items、zip enumerate可以将可迭代对象，除了dict，解压出来，并自带序号（多加入一个维度）。 字典的解包主要靠items（） zip将可迭代对象作为参数，把每一项可迭代对象中取出一个值，组合成一个个元组，，然后返回。 for a,b,c in zip(A,B,C): ... 9. 元编程 9.1. Some Rules ->in python: 常常出现在python函数定义的函数名后面，为函数添加元数据，描述函数的返回类型，从而方面开发人员使用。 拓展：进行函数内的参数定义的时候也可以用冒号指定类型，以及默认值 def func(isPre: bool = True): pass - 9.2. 装饰器模块 装饰器在我个人的理解里面更像是一个嵌套的函数结构，编写装饰器实际上是为了给函数套壳，最根本的目的仍然是为了repeat coding，而这样的写法最直接适用的有以下的几种情况： Timing or Logging 当成函数指针进行函数的传递（但是这点上实际上用类传递的方式可能会更常见一点） 9.2.1. Basic Type 最基本的编写样例： import time from functional import wraps def timethis(func): '''Decorator that report the execution time. this Decorator can not accept parameters''' # 通过下面这个内置的装饰器来保留func的元信息 __name__ __doc___之类的 @wraps(func) def wrapper(*args,**kwargs): # * ** 来保证可以对func传入各种参数 start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end - start) return result return wrapper 9.2.2. 接受参数传递 但是这个装饰器实际上不满足我们的需求，我们希望装饰器能接受传入的参数，这样的话，我们才能更好的进行print或者是使用logging这个模块。 Then we can write it like this : # 实际上直观的理解的话，就是在外面再多嵌套一层函数，通过这个函数来对我们的decorator传递需要的参数 from functional import wraps import logging # 实现对装饰器的参数传递，同时和 def logged(level, name=None,message=None): '''通过最外层接受参数并将其传递到内层的装饰器中''' def decorate(func): # setting paramter we passing here logname = name if name else func.__moudule__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args,**kwargs): log.log(level,logmsg) return func(*args,**kwargs) return wrapper return decorate # 但是实际上我们要传递的就是一些输出结果，所以我们不需要用到这一点，只要再内部赋予logging就行了，所以这里我们设定的就是基本的level和logger_nanme 9.2.3. 修改装饰器参数 对上面这个装饰器模块进行简单的改进，就能使得用包装器包装的函数，能够调用附加函数来修改装饰器的参数 （相当于赋予被装饰方法一个对装饰器的类外访问函数） # 这里有个模块就比较猎奇了，以前倒是没见过 from functional import wraps,partial import logging # utility decorator to attach a functional as an attribute of obj def attach_wrapper(obj, func=None): if func is None: return partial(attach_wrapper, obj) setattr(obj, func.__name__,func) return func #原有装饰器上面添加东西即可 def logged(level, name=None,message=None): '''通过最外层接受参数并将其传递到内层的装饰器中''' def decorate(func): # setting paramter we passing here logname = name if name else func.__moudule__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args,**kwargs): log.log(level,logmsg) return func(*args,**kwargs) '''使用nonlocal添加属性修改的模块''' @attach_wrapper(wrapper) def set_level(newlevel): nonlocal level level = newlevel @attach_wrapper(wrapper) def set_message(newmsg): nonlocal logmsg logmsg = newmsg return wrapper return decorate @logged(logging.DEBUG) def add(x,y): return x + y @logged(logging.CRITICAL,'example') def spam(): print('Spam') # 使用范例:可以再类外调用内内的属性设置了 add.set_message('add called') add(2,3) add.set_level(logging.WARNING) add(2,3) 9.2.4. 带可选参数的修饰器 # 感觉没太理解这个文章中说到的不带参数的意思，难道可以不传入函数吗 # 先把模板放在这 def logged(func = None, *, level=logging.DEBUG,name=None, message=None): if func is None: return partial(logged,level=level,name=name,message=message) logname = name if name else func.__moudule__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args,**kwargs): log.log(level,logmsg) return func(*args,**kwargs) return wrapper 9.2.5. @property的用法 将类别方法转换为类别属性，可以直接用.获取属性值或者对属性进行赋值。 具体的实现和要求在后面再看看 10. Exception @Aiken 2020 Python的异常处理操作：主要内容包括捕捉异常，抛出异常，基于异常进行判断处理等。 基本原理: 参考资料： python3_错误和异常 、 python3_错误和异常2 10.1. Python3 错误和异常 错误：一般语法解析器的解析错误，换句话说也就是基本的语法错误。 异常：语法正确，但是运行期间出现的错误， 异常有很多种类：未定义，类型异常，除数0异常，etc. ...在附录附加常用常见的错误类型 10.2. 异常捕捉try except 通过try exception 捕捉可能会出现的异常，然后用except，指定当该异常出现时候要执行的命令，可以指定多种异常。 基本的算法流程是： 首先，执行 try 子句（在关键字 try 和关键字 except 之间的语句）。 如果没有异常发生，忽略 except 子句，try 子句执行后结束。 如果在执行 try 子句的过程中发生了异常，那么 try 子句余下的部分将被忽略。如果异常的类型和 except 之后的名称相符，那么对应的 except 子句将被执行。 如果一个异常没有与任何的 except 匹配，那么这个异常将会传递给上层的 try 中。 完整的算法逻辑如图所示&#x1F447;，通常可以指使用t-e部分即可 Image1 #主要依赖模块 import time import sys 写法的优点 在可预见的Exception出现的时候不会中断程序的进行， 可以以指定的方式进行规避，或者针对该情况进行特定的异常处理。 else的优势 如果try中出现了多个异常，我们可能会忽视其中的一些异常。 可以针对性的进行异常算法设计，这样会使得可读性和便于分析。 # 用try except的方式最好的一点在于，他不会终端程序的执行。 try: x = int(input(\"please type in NUMBER \")) except ValueError: print('your input is not NUMBER') # if we donot use t-e x = int(input(\"repeat you input\")) # 通过对比，我们可以知道这样执行的好处，在一些无关紧要的地方， # 可以让程序继续运行而不必因为这些而中断。 10.2.1. exception的多种写法和多异常分支 try中的语句可能有多种异常抛出的情况： 针对不同的异常进行处理。 统一处理不同异常。 统一处理所有类型 以上面的代码为例： try except except ... EXCEPT (TUPLE) except不带任何参数 # type 1 try: x = 123 except ValueError: t = 123 except TypeError: y = 123 # type 2 try: x = int(input(\"please type in NUMBER \")) except (ValueError,TypeError,NameError): print('your input is not NUMBER') Try Import 使用try的结构来避免import过程中出现的问题： Python try import (programcreek.com) def try_import(package, message=None): try: return __import__(package) except ImportError as e: if not message: raise e raise ImportError(m) 10.3. 抛出异常 Raise Exception 使用raise 语句能够抛出指定类型的异常，从而终止程序的运行，和assert断言起到相似的作用。 关键用法：设置异常抛出，然后用try except捕捉，然后进行指定的分支操作。 raise [Exception [, args [, traceback]]] raise_exception x = 10 if x >= 5: raise Exception('x 不能大于5，当前值为 {} '.format(x)) 11. Numpy Tips 11.1. reshape 和numpy格式的reshape的相关内容整合 基本reshape的使用 reshape不改变原数据 bk1_a = np.array([1,2,3,4,5,6,7,8]) bk1_b = np.array([[1,1,1,1],[2,2,2,2]]) bk1_c = bk1_a.reshape(bk1_b.shape) print(\"b's datashpe is {}\".format(bk1_b.shape)) print(\"reshape by b。shape is ↓ \\n {}\".format(bk1_c)) # 测试是否改变原数据 print(\"b's shape is {} \".format(bk1_b.shape)) assert bk1_a == bk1_c, 'do not change the origin data, a is like {}'.format(bk1_a) 11.2. tolist numpy array 和list之间的互相转换，在大规模编程中有比较广泛的应用场景。 **有^次方的意思 arange 包含下限，不包含上线 bk2_a= (2 ** np.arange(4,6)) # bk2_a bk2_b = bk2_a.tolist() # bk2_b 11.3. 用array给list中的元素赋值 以下是这种方式建立一个类似one-hot的函数介绍 可以很容易的从输出看出规律，而且最外层仍然是列表，也就是其中的元素是array list1 = [1,2,3,4,12,3,4] for i in range(len(list1)): temp = list1[i]-1 list1[i] = np.zeros(13) list1[i][temp] = 1 list1 11.4. flatten & flat operation flatten：将数据摊开降维成一维的数组/矩阵，以副本形式生成，不影响原数据 flat，生成一个迭代器，按行的形式迭代 # A:flatten function B:flat function bk3_a = np.random.rand(2,3) print('A is just like\\n {}'.format(bk3_a)) bk3_a2 = bk3_a.flatten() print('A2 is just like\\n {}'.format(bk3_a2)) printz('********************clip*************************') bk3_b = np.random.rand(2,3) print('B is just like \\n{}'.format(bk3_b)) print(bk3_b.flat, 'as we can see, this is a iter') for i in bk3_b.flat: print(i) 11.5. Numpy.pad pad，就是拓展原本数据的维度，方便后面机器学习中的其他步骤，主要用处包括： 维度保持 增加对图像边界的重视 ## numpy.pad x = np.random.randint(0,5,(3,4,4)) x = np.pad(x,2) print(x.shape) h,w = x.shape[1:] new_h,new_w = 3,3 top = np.random.randint(0,h-new_h) left = np.random.randint(0,w-new_w) x = x[:,top: top+new_h,left:left+new_w] x.shape 11.6. :: Numpy 索引中双冒号的实际用途 参照该文章进行分析，主要用途包括：对图像进行反转等操作 https://blog.csdn.net/GracePro/article/details/102079331 a = np.random.rand(3,2,2) print(a) print('----------------------------') a = a[:,::-1] print(a) 11.7. Argpartition() 借助于 argpartition()，Numpy 可以找出 N 个最大数值的索引，也会将找到的这些索引输出。然后我们根据需要对数值进行排序。 x = np.array([12, 10, 12, 0, 6, 8, 9, 1, 16, 4, 6, 0]) index_val = np.argpartition(x, -5)[-5:] index2 = np.argmin(x) print(index2) index_val 基于numpy的sort函数，输出找出的最大的几个数，要全体排序的话，还是考sort np.sort(x[index_val]) 11.8. Allclose() allclose() 用于匹配两个数组，并得到布尔值表示的输出。如果在一个公差范围内（within a tolerance）两个数组不等同， 则 allclose() 返回 False。该函数对于检查两个数组是否相似非常有用。 array1 = np.array([0.12,0.17,0.24,0.29]) array2 = np.array([0.13,0.19,0.26,0.31]) # with a tolerance of 0.1, it should return False: print(np.allclose(array1,array2,0.1)) # with a tolerance of 0.2, it should return True: print(np.allclose(array1,array2,0.2)) 11.9. Clip() 使得一个数组中的数值保持在一个区间内。有时，我们需要保证数值在上下限范围内。为此，我们可以借助 Numpy 的 clip() 函数实现该目的。给定一个区间，则区间外的数值被剪切至区间上下限（interval edge）。 x = np.array([3, 17, 14, 23, 2, 2, 6, 8, 1, 2, 16, 0]) np.clip(x,2,5) 11.10. extract() 顾名思义，extract() 是在特定条件下从一个数组中提取特定元素。 借助于 extract()，我们还可以使用 and 和 or 等条件。 array = np.random.randint(20, size=12) print('basic array is {} '.format(array)) # Divide by 2 and check if remainder is 1 cond = np.mod(array, 2)==1 print('是否符合条件的list，条件list\\n {}'.format(cond)) # Use extract to get the values # 提取出表现为True的哪些元素 print('按照条件提取出元素:\\n {}'.format(np.extract(cond, array))) # Apply condition on extract directly # 更直接的指定条件 print('复杂条件下的表现情况') print(np.extract(((array 15)), array)) 11.11. where() Where() 用于从一个数组中返回满足特定条件的元素。比如，它会返回满足特定条件的数值的索引位 Where() 与 SQL 中使用的 where condition 类似，如以下示例所示： y = np.array([1,5,6,8,1,7,3,6,9]) # Where y is greater than 5, returns index position print(np.where(y>5)) # First will replace the values that match the condition, # second will replace the values that does not print(np.where(y>5, \"Hit\", \"Miss\")) 12. DEBUG 记录一些典型错误，便于后续Debug的时候查找原因 12.1. 避免重复/冲突的import 在工程实现中，对于同一个module。最好能做到不需要重复的import，但是在跨文件的工程项目中，或者说是一些跨文件调用的情况下，可能有一些基本的module会需要这样的时候，那我们最好做到不冲突，以同样的形式来进行import，不然有时候这样的重定义方式会出现一定的问题或者bug。 for example from time import time 和 import time同时出现的情况。 12.2. 内存调用与Method的定义 在较为复杂的工程项目中，应该使用Method（Function）模块化的解决问题；这样做的优势可以从一下几点来看： 易于阅读分析，写好相关method的Doc，然后做好注释，方便阅读和后续修改 能够在迭代过程中有效的释放暂态的变量，节约在主进程中无效的参数存储空间，节省内存或者显存。 12.3. TypeError： 一：cannnot unpack not-iterable NoneType object（无法解包非迭代类型的NoneType对象） def test(): if value == 1: a=b=1 return a, b a,b = test() print(a,b) 原因分析，当python函数没有确定的return的时候默认的返回值是None，这样在进行检查的时候，就会到导致编译的错误 解决：指定默认的return，或者使用else方法完善所有情况下的return值的个数是一致的 二：missing 1 required positional argument： “self” 对象的声明需要括号，我们可能在调用类内函数的时候，用错了变量，用了类而不是类的实例去调用这个函数，导致执行出现了错误。 三：builtin_function_or_method error 很多时候都是由于前面的数据操作少加了()导致的问题 四：bad operand type for unary -: ‘NoneType’ 输入的数据存在着值为空的情况，可能没定义之类的，问题要根据后面的具体报错来进行分析。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-10 16:41:32 "},"Langs/PythonTips.html":{"url":"Langs/PythonTips.html","title":"1.1 Python Tips","keywords":"","body":"1. What the f*ck Python! &#x1F40D;2. Table of Contents/目录3. Structure of the Examples/示例结构4. Usage/用法5. &#x1F440; Examples/示例5.1. Section: Strain your brain!/大脑运动!5.1.1. > Strings can be tricky sometimes/微妙的字符串 *5.1.2. > Time for some hash brownies!/是时候来点蛋糕了!5.1.3. > Return return everywhere!/到处返回！5.1.4. > Deep down, we're all the same./本质上,我们都一样. *5.1.5. > For what?/为什么?5.1.6. > Evaluation time discrepancy/执行时机差异5.1.7. > is is not what it is!/出人意料的is!5.1.8. > A tic-tac-toe where X wins in the first attempt!/一蹴即至!5.1.9. > The sticky output function/麻烦的输出5.1.10. > is not ... is not is (not ...)/is not ... 不是 is (not ...)5.1.11. > The surprising comma/意外的逗号5.1.12. > Backslashes at the end of string/字符串末尾的反斜杠5.1.13. > not knot!/别纠结!5.1.14. > Half triple-quoted strings/三个引号5.1.15. > Midnight time doesn't exist?/不存在的午夜?5.1.16. > What's wrong with booleans?/布尔你咋了?5.1.17. > Class attributes and instance attributes/类属性和实例属性5.1.18. > yielding None/生成 None5.1.19. > Mutating the immutable!/强人所难5.1.20. > The disappearing variable from outer scope/消失的外部变量5.1.21. > When True is actually False/真亦假5.1.22. > From filled to None in one instruction.../从有到无...5.1.23. > Subclass relationships/子类关系 *5.1.24. > The mysterious key type conversion/神秘的键型转换 *5.1.25. > Let's see if you can guess this?/看看你能否猜到这一点?5.2. Section: Appearances are deceptive!/外表是靠不住的!5.2.1. > Skipping lines?/跳过一行?5.2.2. > Teleportation/空间移动 *5.2.3. > Well, something is fishy.../嗯，有些可疑...5.3. Section: Watch out for the landmines!/小心地雷!5.3.1. > Modifying a dictionary while iterating over it/迭代字典时的修改5.3.2. > Stubborn del operator/坚强的 del *5.3.3. > Deleting a list item while iterating/迭代列表时删除元素5.3.4. > Loop variables leaking out!/循环变量泄漏!5.3.5. > Beware of default mutable arguments!/当心默认的可变参数!5.3.6. > Catching the Exceptions/捕获异常5.3.7. > Same operands, different story!/同人不同命!5.3.8. > The out of scope variable/外部作用域变量5.3.9. > Be careful with chained operations/小心链式操作5.3.10. > Name resolution ignoring class scope/忽略类作用域的名称解析5.3.11. > Needle in a Haystack/大海捞针5.4. Section: The Hidden treasures!/隐藏的宝藏!5.4.1. > Okay Python, Can you make me fly?/Python, 可否带我飞? *5.4.2. > goto, but why?/goto, 但为什么? *5.4.3. > Brace yourself!/做好思想准备 *5.4.4. > Let's meet Friendly Language Uncle For Life/让生活更友好 *5.4.5. > Even Python understands that love is complicated/连Python也知道爱是难言的 *5.4.6. > Yes, it exists!/是的, 它存在!5.4.7. > Inpinity/无限 *5.4.8. > Mangling time!/修饰时间! *5.5. Section: Miscellaneous/杂项5.5.1. > += is faster/更快的 +=5.5.2. > Let's make a giant string!/来做个巨大的字符串吧！5.5.3. > Explicit typecast of strings/字符串的显式类型转换5.5.4. > Minor Ones/小知识点6. Contributing/贡献7. Acknowledgements/致谢8. &#x1F393; License/许可8.1. Help/帮助8.2. Surprise your geeky pythonist friends?/想给你的极客朋友一个惊喜?8.3. Need a pdf version?/需要来一份pdf版的?8.4. Follow Commit/追踪Commit8.5. 996.icu 1. What the f*ck Python! &#x1F40D; 一些有趣且鲜为人知的 Python 特性. English | 中文 Python, 是一个设计优美的解释型高级语言, 它提供了很多能让程序员感到舒适的功能特性. 但有的时候, Python 的一些输出结果对于初学者来说似乎并不是那么一目了然. 这个有趣的项目意在收集 Python 中那些难以理解和反人类直觉的例子以及鲜为人知的功能特性, 并尝试讨论这些现象背后真正的原理! 虽然下面的有些例子并不一定会让你觉得 WTFs, 但它们依然有可能会告诉你一些你所不知道的 Python 有趣特性. 我觉得这是一种学习编程语言内部原理的好办法, 而且我相信你也会从中获得乐趣! 如果您是一位经验比较丰富的 Python 程序员, 你可以尝试挑战看是否能一次就找到例子的正确答案. 你可能对其中的一些例子已经比较熟悉了, 那这也许能唤起你当年踩这些坑时的甜蜜回忆 :sweat_smile: PS: 如果你不是第一次读了, 你可以在这里获取变动内容. 那么, 让我们开始吧... 2. Table of Contents/目录 Table of Contents/目录 Structure of the Examples/示例结构 Usage/用法 &#x1F440; Examples/示例 Section: Strain your brain!/大脑运动! > Strings can be tricky sometimes/微妙的字符串 * > Time for some hash brownies!/是时候来点蛋糕了! > Return return everywhere!/到处返回！ > Deep down, we're all the same./本质上,我们都一样. * > For what?/为什么? > Evaluation time discrepancy/执行时机差异 > is is not what it is!/出人意料的is! > A tic-tac-toe where X wins in the first attempt!/一蹴即至! > The sticky output function/麻烦的输出 > is not ... is not is (not ...)/is not ... 不是 is (not ...) > The surprising comma/意外的逗号 > Backslashes at the end of string/字符串末尾的反斜杠 > not knot!/别纠结! > Half triple-quoted strings/三个引号 > Midnight time doesn't exist?/不存在的午夜? > What's wrong with booleans?/布尔你咋了? > Class attributes and instance attributes/类属性和实例属性 > yielding None/生成 None > Mutating the immutable!/强人所难 > The disappearing variable from outer scope/消失的外部变量 > When True is actually False/真亦假 > From filled to None in one instruction.../从有到无... > Subclass relationships/子类关系 * > The mysterious key type conversion/神秘的键型转换 * > Let's see if you can guess this?/看看你能否猜到这一点? Section: Appearances are deceptive!/外表是靠不住的! > Skipping lines?/跳过一行? > Teleportation/空间移动 * > Well, something is fishy.../嗯, 有些可疑... Section: Watch out for the landmines!/小心地雷! > Modifying a dictionary while iterating over it/迭代字典时的修改 > Stubborn del operator/坚强的 del * > Deleting a list item while iterating/迭代列表时删除元素 > Loop variables leaking out!/循环变量泄漏! > Beware of default mutable arguments!/当心默认的可变参数! > Catching the Exceptions/捕获异常 > Same operands, different story!/同人不同命! > The out of scope variable/外部作用域变量 > Be careful with chained operations/小心链式操作 > Name resolution ignoring class scope/忽略类作用域的名称解析 > Needle in a Haystack/大海捞针 Section: The Hidden treasures!/隐藏的宝藏! > Okay Python, Can you make me fly?/Python, 可否带我飞? * > goto, but why?/goto, 但为什么? * > Brace yourself!/做好思想准备 * > Let's meet Friendly Language Uncle For Life/让生活更友好 * > Even Python understands that love is complicated/连Python也知道爱是难言的 * > Yes, it exists!/是的, 它存在! > Inpinity/无限 * > Mangling time!修饰时间! * Section: Miscellaneous/杂项 > += is faster/更快的 += > Let's make a giant string!/来做个巨大的字符串吧! > Explicit typecast of strings/字符串的显式类型转换 > Minor Ones/小知识点 Contributing/贡献 Acknowledgements/致谢 &#x1F393; License/许可 Help/帮助 Surprise your geeky pythonist friends?/想给你的极客朋友一个惊喜? Need a pdf version?/需要来一份pdf版的? Follow Commit/追踪Commit 996.icu 3. Structure of the Examples/示例结构 所有示例的结构都如下所示: > 一个精选的标题 * 标题末尾的星号表示该示例在第一版中不存在，是最近添加的. # 准备代码. # 释放魔法... Output (Python version): >>> 触发语句 出乎意料的输出结果 (可选): 对意外输出结果的简短描述. &#x1F4A1; 说明: 简要说明发生了什么以及为什么会发生.如有必要, 举例说明 Output:>>> 触发语句 # 一些让魔法变得容易理解的例子 # 一些正常的输入 注意: 所有的示例都在 Python 3.5.2 版本的交互解释器上测试过, 如果不特别说明应该适用于所有 Python 版本. 4. Usage/用法 我个人建议, 最好依次阅读下面的示例, 并对每个示例: 仔细阅读设置例子最开始的代码. 如果您是一位经验丰富的 Python 程序员, 那么大多数时候您都能成功预期到后面的结果. 阅读输出结果, 确认结果是否如你所料. 确认你是否知道这背后的原理. 如果不知道, 深呼吸然后阅读说明 (如果你还是看不明白, 别沉默! 可以在这提个 issue). 如果知道, 给自己点奖励, 然后去看下一个例子. PS: 你也可以在命令行阅读 WTFpython. 我们有 pypi 包 和 npm 包(支持代码高亮).(译: 这两个都是英文版的) 安装 npm 包 wtfpython $ npm install -g wtfpython 或者, 安装 pypi 包 wtfpython $ pip install wtfpython -U 现在, 在命令行中运行 wtfpython, 你就可以开始浏览了. 5. &#x1F440; Examples/示例 5.1. Section: Strain your brain!/大脑运动! 5.1.1. > Strings can be tricky sometimes/微妙的字符串 * 1. >>> a = \"some_string\" >>> id(a) 140420665652016 >>> id(\"some\" + \"_\" + \"string\") # 注意两个的id值是相同的. 140420665652016 2. >>> a = \"wtf\" >>> b = \"wtf\" >>> a is b True >>> a = \"wtf!\" >>> b = \"wtf!\" >>> a is b False >>> a, b = \"wtf!\", \"wtf!\" >>> a is b True # 3.7 版本返回结果为 False. 3. >>> 'a' * 20 is 'aaaaaaaaaaaaaaaaaaaa' True >>> 'a' * 21 is 'aaaaaaaaaaaaaaaaaaaaa' False # 3.7 版本返回结果为 True 很好理解, 对吧? &#x1F4A1; 说明: 这些行为是由于 Cpython 在编译优化时, 某些情况下会尝试使用已经存在的不可变对象而不是每次都创建一个新对象. (这种行为被称作字符串的驻留[string interning]) 发生驻留之后, 许多变量可能指向内存中的相同字符串对象. (从而节省内存) 在上面的代码中, 字符串是隐式驻留的. 何时发生隐式驻留则取决于具体的实现. 这里有一些方法可以用来猜测字符串是否会被驻留: 所有长度为 0 和长度为 1 的字符串都被驻留. 字符串在编译时被实现 ('wtf' 将被驻留, 但是 ''.join(['w', 't', 'f']) 将不会被驻留) 字符串中只包含字母，数字或下划线时将会驻留. 所以 'wtf!' 由于包含 ! 而未被驻留. 可以在这里找到 CPython 对此规则的实现. 当在同一行将 a 和 b 的值设置为 \"wtf!\" 的时候, Python 解释器会创建一个新对象, 然后同时引用第二个变量(译: 仅适用于3.7以下, 详细情况请看这里). 如果你在不同的行上进行赋值操作, 它就不会“知道”已经有一个 wtf！ 对象 (因为 \"wtf!\" 不是按照上面提到的方式被隐式驻留的). 它是一种编译器优化, 特别适用于交互式环境. 常量折叠(constant folding) 是 Python 中的一种 窥孔优化(peephole optimization) 技术. 这意味着在编译时表达式 'a'*20 会被替换为 'aaaaaaaaaaaaaaaaaaaa' 以减少运行时的时钟周期. 只有长度小于 20 的字符串才会发生常量折叠. (为啥? 想象一下由于表达式 'a'*10**10 而生成的 .pyc 文件的大小). 相关的源码实现在这里. 如果你是使用 3.7 版本中运行上述示例代码, 会发现部分代码的运行结果与注释说明相同. 这是因为在 3.7 版本中, 常量折叠已经从窥孔优化器迁移至新的 AST 优化器, 后者可以以更高的一致性来执行优化. (由 Eugene Toder 和 INADA Naoki 在 bpo-29469 和 bpo-11549 中贡献.) (译: 但是在最新的 3.8 版本中, 结果又变回去了. 虽然 3.8 版本和 3.7 版本一样, 都是使用 AST 优化器. 目前不确定官方对 3.8 版本的 AST 做了什么调整.) 5.1.2. > Time for some hash brownies!/是时候来点蛋糕了! hash brownie指一种含有大麻成分的蛋糕, 所以这里是句双关 1. some_dict = {} some_dict[5.5] = \"Ruby\" some_dict[5.0] = \"JavaScript\" some_dict[5] = \"Python\" Output: >>> some_dict[5.5] \"Ruby\" >>> some_dict[5.0] \"Python\" >>> some_dict[5] \"Python\" \"Python\" 消除了 \"JavaScript\" 的存在? &#x1F4A1; 说明: Python 字典通过检查键值是否相等和比较哈希值来确定两个键是否相同. 具有相同值的不可变对象在Python中始终具有相同的哈希值.>>> 5 == 5.0 True >>> hash(5) == hash(5.0) True 注意: 具有不同值的对象也可能具有相同的哈希值（哈希冲突）. 当执行 some_dict[5] = \"Python\" 语句时, 因为Python将 5 和 5.0 识别为 some_dict 的同一个键, 所以已有值 \"JavaScript\" 就被 \"Python\" 覆盖了. 这个 StackOverflow的 回答 漂亮地解释了这背后的基本原理. 5.1.3. > Return return everywhere!/到处返回！ def some_func(): try: return 'from_try' finally: return 'from_finally' Output: >>> some_func() 'from_finally' &#x1F4A1; 说明: 当在 \"try...finally\" 语句的 try 中执行 return, break 或 continue 后, finally 子句依然会执行. 函数的返回值由最后执行的 return 语句决定. 由于 finally 子句一定会执行, 所以 finally 子句中的 return 将始终是最后执行的语句. 5.1.4. > Deep down, we're all the same./本质上,我们都一样. * class WTF: pass Output: >>> WTF() == WTF() # 两个不同的对象应该不相等 False >>> WTF() is WTF() # 也不相同 False >>> hash(WTF()) == hash(WTF()) # 哈希值也应该不同 True >>> id(WTF()) == id(WTF()) True &#x1F4A1; 说明: 当调用 id 函数时, Python 创建了一个 WTF 类的对象并传给 id 函数. 然后 id 函数获取其id值 (也就是内存地址), 然后丢弃该对象. 该对象就被销毁了. 当我们连续两次进行这个操作时, Python会将相同的内存地址分配给第二个对象. 因为 (在CPython中) id 函数使用对象的内存地址作为对象的id值, 所以两个对象的id值是相同的. 综上, 对象的id值仅仅在对象的生命周期内唯一. 在对象被销毁之后, 或被创建之前, 其他对象可以具有相同的id值. 那为什么 is 操作的结果为 False 呢? 让我们看看这段代码. class WTF(object): def __init__(self): print(\"I\") def __del__(self): print(\"D\") Output: >>> WTF() is WTF() I I D D False >>> id(WTF()) == id(WTF()) I D I D True 正如你所看到的, 对象销毁的顺序是造成所有不同之处的原因. 5.1.5. > For what?/为什么? some_string = \"wtf\" some_dict = {} for i, some_dict[i] in enumerate(some_string): pass Output: >>> some_dict # 创建了索引字典. {0: 'w', 1: 't', 2: 'f'} &#x1F4A1; 说明: Python 语法 中对 for 的定义是: for_stmt: 'for' exprlist 'in' testlist ':' suite ['else' ':' suite] 其中 exprlist 指分配目标. 这意味着对可迭代对象中的每一项都会执行类似 {exprlist} = {next_value} 的操作. 一个有趣的例子说明了这一点: for i in range(4): print(i) i = 10 Output: 0 1 2 3 你可曾觉得这个循环只会运行一次? &#x1F4A1; 说明: 由于循环在Python中工作方式, 赋值语句 i = 10 并不会影响迭代循环, 在每次迭代开始之前, 迭代器(这里指 range(4)) 生成的下一个元素就被解包并赋值给目标列表的变量(这里指 i)了. 在每一次的迭代中, enumerate(some_string) 函数就生成一个新值 i (计数器增加) 并从 some_string 中获取一个字符. 然后将字典 some_dict 键 i (刚刚分配的) 的值设为该字符. 本例中循环的展开可以简化为: >>> i, some_dict[i] = (0, 'w') >>> i, some_dict[i] = (1, 't') >>> i, some_dict[i] = (2, 'f') >>> some_dict 5.1.6. > Evaluation time discrepancy/执行时机差异 1. array = [1, 8, 15] g = (x for x in array if array.count(x) > 0) array = [2, 8, 22] Output: >>> print(list(g)) [8] 2. array_1 = [1,2,3,4] g1 = (x for x in array_1) array_1 = [1,2,3,4,5] array_2 = [1,2,3,4] g2 = (x for x in array_2) array_2[:] = [1,2,3,4,5] Output: >>> print(list(g1)) [1,2,3,4] >>> print(list(g2)) [1,2,3,4,5] &#x1F4A1; 说明 在生成器表达式中, in 子句在声明时执行, 而条件子句则是在运行时执行. 所以在运行前, array 已经被重新赋值为 [2, 8, 22], 因此对于之前的 1, 8 和 15, 只有 count(8) 的结果是大于 0 的, 所以生成器只会生成 8. 第二部分中 g1 和 g2 的输出差异则是由于变量 array_1 和 array_2 被重新赋值的方式导致的. 在第一种情况下, array_1 被绑定到新对象 [1,2,3,4,5], 因为 in 子句是在声明时被执行的， 所以它仍然引用旧对象 [1,2,3,4](并没有被销毁). 在第二种情况下, 对 array_2 的切片赋值将相同的旧对象 [1,2,3,4] 原地更新为 [1,2,3,4,5]. 因此 g2 和 array_2 仍然引用同一个对象(这个对象现在已经更新为 [1,2,3,4,5]). 5.1.7. > is is not what it is!/出人意料的is! 下面是一个在互联网上非常有名的例子. >>> a = 256 >>> b = 256 >>> a is b True >>> a = 257 >>> b = 257 >>> a is b False >>> a = 257; b = 257 >>> a is b True &#x1F4A1; 说明: is 和 == 的区别 is 运算符检查两个运算对象是否引用自同一对象 (即, 它检查两个运算对象是否相同). == 运算符比较两个运算对象的值是否相等. 因此 is 代表引用相同, == 代表值相等. 下面的例子可以很好的说明这点,>>> [] == [] True >>> [] is [] # 这两个空列表位于不同的内存地址. False 256 是一个已经存在的对象, 而 257 不是 当你启动Python 的时候, 数值为 -5 到 256 的对象就已经被分配好了. 这些数字因为经常被使用, 所以会被提前准备好. Python 通过这种创建小整数池的方式来避免小整数频繁的申请和销毁内存空间. 引用自 https://docs.python.org/3/c-api/long.html 当前的实现为-5到256之间的所有整数保留一个整数对象数组, 当你创建了一个该范围内的整数时, 你只需要返回现有对象的引用. 所以改变1的值是有可能的. 我怀疑这种行为在Python中是未定义行为. :-) >>> id(256) 10922528 >>> a = 256 >>> b = 256 >>> id(a) 10922528 >>> id(b) 10922528 >>> id(257) 140084850247312 >>> x = 257 >>> y = 257 >>> id(x) 140084850247440 >>> id(y) 140084850247344 这里解释器并没有智能到能在执行 y = 257 时意识到我们已经创建了一个整数 257, 所以它在内存中又新建了另一个对象. 当 a 和 b 在同一行中使用相同的值初始化时，会指向同一个对象. >>> a, b = 257, 257 >>> id(a) 140640774013296 >>> id(b) 140640774013296 >>> a = 257 >>> b = 257 >>> id(a) 140640774013392 >>> id(b) 140640774013488 当 a 和 b 在同一行中被设置为 257 时, Python 解释器会创建一个新对象, 然后同时引用第二个变量. 如果你在不同的行上进行, 它就不会 \"知道\" 已经存在一个 257 对象了. 这是一种特别为交互式环境做的编译器优化. 当你在实时解释器中输入两行的时候, 他们会单独编译, 因此也会单独进行优化. 如果你在 .py 文件中尝试这个例子, 则不会看到相同的行为, 因为文件是一次性编译的. 5.1.8. > A tic-tac-toe where X wins in the first attempt!/一蹴即至! # 我们先初始化一个变量row row = [\"\"]*3 #row i['', '', ''] # 并创建一个变量board board = [row]*3 Output: >>> board [['', '', ''], ['', '', ''], ['', '', '']] >>> board[0] ['', '', ''] >>> board[0][0] '' >>> board[0][0] = \"X\" >>> board [['X', '', ''], ['X', '', ''], ['X', '', '']] 我们有没有赋值过3个 \"X\" 呢？ &#x1F4A1; 说明: 当我们初始化 row 变量时, 下面这张图展示了内存中的情况。 而当通过对 row 做乘法来初始化 board 时, 内存中的情况则如下图所示 (每个元素 board[0], board[1] 和 board[2] 都和 row 一样引用了同一列表.) 我们可以通过不使用变量 row 生成 board 来避免这种情况. (这个issue提出了这个需求.) >>> board = [['']*3 for _ in range(3)] >>> board[0][0] = \"X\" >>> board [['X', '', ''], ['', '', ''], ['', '', '']] 5.1.9. > The sticky output function/麻烦的输出 funcs = [] results = [] for x in range(7): def some_func(): return x funcs.append(some_func) results.append(some_func()) # 注意这里函数被执行了 funcs_results = [func() for func in funcs] Output: >>> results [0, 1, 2, 3, 4, 5, 6] >>> funcs_results [6, 6, 6, 6, 6, 6, 6] 即使每次在迭代中将 some_func 加入 funcs 前的 x 值都不相同, 所有的函数还是都返回6. // 再换个例子 >>> powers_of_x = [lambda x: x**i for i in range(10)] >>> [f(2) for f in powers_of_x] [512, 512, 512, 512, 512, 512, 512, 512, 512, 512] &#x1F4A1; 说明: 当在循环内部定义一个函数时, 如果该函数在其主体中使用了循环变量, 则闭包函数将与循环变量绑定, 而不是它的值. 因此, 所有的函数都是使用最后分配给变量的值来进行计算的. 可以通过将循环变量作为命名变量传递给函数来获得预期的结果. 为什么这样可行? 因为这会在函数内再次定义一个局部变量. funcs = [] for x in range(7): def some_func(x=x): return x funcs.append(some_func) Output: >>> funcs_results = [func() for func in funcs] >>> funcs_results [0, 1, 2, 3, 4, 5, 6] 5.1.10. > is not ... is not is (not ...)/is not ... 不是 is (not ...) >>> 'something' is not None True >>> 'something' is (not None) False &#x1F4A1; 说明: is not 是个单独的二元运算符, 与分别使用 is 和 not 不同. 如果操作符两侧的变量指向同一个对象, 则 is not 的结果为 False, 否则结果为 True. 5.1.11. > The surprising comma/意外的逗号 Output: >>> def f(x, y,): ... print(x, y) ... >>> def g(x=4, y=5,): ... print(x, y) ... >>> def h(x, **kwargs,): File \"\", line 1 def h(x, **kwargs,): ^ SyntaxError: invalid syntax >>> def h(*args,): File \"\", line 1 def h(*args,): ^ SyntaxError: invalid syntax &#x1F4A1; 说明: 在Python函数的形式参数列表中, 尾随逗号并不一定是合法的. 在Python中, 参数列表部分用前置逗号定义, 部分用尾随逗号定义. 这种冲突导致逗号被夹在中间, 没有规则定义它.(译:这一句看得我也很懵逼,只能强翻了.详细解释看下面的讨论帖会一目了然.) 注意: 尾随逗号的问题已经在Python 3.6中被修复了. 而这篇帖子中则简要讨论了Python中尾随逗号的不同用法. 5.1.12. > Backslashes at the end of string/字符串末尾的反斜杠 Output: >>> print(\"\\\\ C:\\\\\") \\ C:\\ >>> print(r\"\\ C:\") \\ C: >>> print(r\"\\ C:\\\") File \"\", line 1 print(r\"\\ C:\\\") ^ SyntaxError: EOL while scanning string literal &#x1F4A1; 说明: 在以 r 开头的原始字符串中, 反斜杠并没有特殊含义.>>> print(repr(r\"wt\\\"f\")) 'wt\\\\\"f' 解释器所做的只是简单的改变了反斜杠的行为, 因此会直接放行反斜杠及后一个的字符. 这就是反斜杠在原始字符串末尾不起作用的原因. 5.1.13. > not knot!/别纠结! x = True y = False Output: >>> not x == y True >>> x == not y File \"\", line 1 x == not y ^ SyntaxError: invalid syntax &#x1F4A1; 说明: 运算符的优先级会影响表达式的求值顺序, 而在 Python 中 == 运算符的优先级要高于 not 运算符. 所以 not x == y 相当于 not (x == y), 同时等价于 not (True == False), 最后的运算结果就是 True. 之所以 x == not y 会抛一个 SyntaxError 异常, 是因为它会被认为等价于 (x == not) y, 而不是你一开始期望的 x == (not y). 解释器期望 not 标记是 not in 操作符的一部分 (因为 == 和 not in 操作符具有相同的优先级), 但是它在 not 标记后面找不到 in 标记, 所以会抛出 SyntaxError 异常. 5.1.14. > Half triple-quoted strings/三个引号 Output: >>> print('wtfpython''') wtfpython >>> print(\"wtfpython\"\"\") wtfpython >>> # 下面的语句会抛出 `SyntaxError` 异常 >>> # print('''wtfpython') >>> # print(\"\"\"wtfpython\") &#x1F4A1; 说明: Python 提供隐式的字符串连接, 例如,>>> print(\"wtf\" \"python\") wtfpython >>> print(\"wtf\" \"\") # or \"wtf\"\"\" wtf ''' 和 \"\"\" 在 Python中也是字符串定界符, Python 解释器在先遇到三个引号的的时候会尝试再寻找三个终止引号作为定界符, 如果不存在则会导致 SyntaxError 异常. 5.1.15. > Midnight time doesn't exist?/不存在的午夜? from datetime import datetime midnight = datetime(2018, 1, 1, 0, 0) midnight_time = midnight.time() noon = datetime(2018, 1, 1, 12, 0) noon_time = noon.time() if midnight_time: print(\"Time at midnight is\", midnight_time) if noon_time: print(\"Time at noon is\", noon_time) Output: ('Time at noon is', datetime.time(12, 0)) midnight_time 并没有被输出. &#x1F4A1; 说明: 在Python 3.5之前, 如果 datetime.time 对象存储的UTC的午夜时间(译: 就是 00:00), 那么它的布尔值会被认为是 False. 当使用 if obj: 语句来检查 obj 是否为 null 或者某些“空”值的时候, 很容易出错. 5.1.16. > What's wrong with booleans?/布尔你咋了? 1. # 一个简单的例子, 统计下面可迭代对象中的布尔型值的个数和整型值的个数 mixed_list = [False, 1.0, \"some_string\", 3, True, [], False] integers_found_so_far = 0 booleans_found_so_far = 0 for item in mixed_list: if isinstance(item, int): integers_found_so_far += 1 elif isinstance(item, bool): booleans_found_so_far += 1 Output: >>> integers_found_so_far 4 >>> booleans_found_so_far 0 2. another_dict = {} another_dict[True] = \"JavaScript\" another_dict[1] = \"Ruby\" another_dict[1.0] = \"Python\" Output: >>> another_dict[True] \"Python\" 3. >>> some_bool = True >>> \"wtf\"*some_bool 'wtf' >>> some_bool = False >>> \"wtf\"*some_bool '' &#x1F4A1; 说明: 布尔值是 int 的子类 >>> isinstance(True, int) True >>> isinstance(False, int) True 所以 True 的整数值是 1, 而 False 的整数值是 0. >>> True == 1 == 1.0 and False == 0 == 0.0 True 关于其背后的原理, 请看这个 StackOverflow 的回答. 5.1.17. > Class attributes and instance attributes/类属性和实例属性 1. class A: x = 1 class B(A): pass class C(A): pass Output: >>> A.x, B.x, C.x (1, 1, 1) >>> B.x = 2 >>> A.x, B.x, C.x (1, 2, 1) >>> A.x = 3 >>> A.x, B.x, C.x (3, 2, 3) >>> a = A() >>> a.x, A.x (3, 3) >>> a.x += 1 >>> a.x, A.x (4, 3) 2. class SomeClass: some_var = 15 some_list = [5] another_list = [5] def __init__(self, x): self.some_var = x + 1 self.some_list = self.some_list + [x] self.another_list += [x] Output: >>> some_obj = SomeClass(420) >>> some_obj.some_list [5, 420] >>> some_obj.another_list [5, 420] >>> another_obj = SomeClass(111) >>> another_obj.some_list [5, 111] >>> another_obj.another_list [5, 420, 111] >>> another_obj.another_list is SomeClass.another_list True >>> another_obj.another_list is some_obj.another_list True &#x1F4A1; 说明: 类变量和实例变量在内部是通过类对象的字典来处理(译: 就是 __dict__ 属性). 如果在当前类的字典中找不到的话就去它的父类中寻找. += 运算符会在原地修改可变对象, 而不是创建新对象. 因此, 在这种情况下, 修改一个实例的属性会影响其他实例和类属性. 5.1.18. > yielding None/生成 None some_iterable = ('a', 'b') def some_func(val): return \"something\" Output: >>> [x for x in some_iterable] ['a', 'b'] >>> [(yield x) for x in some_iterable] at 0x7f70b0a4ad58> >>> list([(yield x) for x in some_iterable]) ['a', 'b'] >>> list((yield x) for x in some_iterable) ['a', None, 'b', None] >>> list(some_func((yield x)) for x in some_iterable) ['a', 'something', 'b', 'something'] &#x1F4A1; 说明: 来源和解释可以在这里找到: https://stackoverflow.com/questions/32139885/yield-in-list-comprehensions-and-generator-expressions 相关错误报告: http://bugs.python.org/issue10544 这个bug在3.7以后的版本中不被推荐使用, 并在3.8中被修复. 因此在3.8中尝试在推导式中使用 yield, 只会得到一个 SyntaxError. 详细内容可以看3.7更新内容, 3.8更新内容. 5.1.19. > Mutating the immutable!/强人所难 some_tuple = (\"A\", \"tuple\", \"with\", \"values\") another_tuple = ([1, 2], [3, 4], [5, 6]) Output: >>> some_tuple[2] = \"change this\" TypeError: 'tuple' object does not support item assignment >>> another_tuple[2].append(1000) # 这里不出现错误 >>> another_tuple ([1, 2], [3, 4], [5, 6, 1000]) >>> another_tuple[2] += [99, 999] TypeError: 'tuple' object does not support item assignment >>> another_tuple ([1, 2], [3, 4], [5, 6, 1000, 99, 999]) 我还以为元组是不可变的呢... &#x1F4A1; 说明: 引用 https://docs.python.org/2/reference/datamodel.html 不可变序列 不可变序列的对象一旦创建就不能再改变. (如果对象包含对其他对象的引用，则这些其他对象可能是可变的并且可能会被修改; 但是，由不可变对象直接引用的对象集合不能更改.) += 操作符在原地修改了列表. 元素赋值操作并不工作, 但是当异常抛出时, 元素已经在原地被修改了. (译: 对于不可变对象, 这里指tuple, += 并不是原子操作, 而是 extend 和 = 两个动作, 这里 = 操作虽然会抛出异常, 但 extend 操作已经修改成功了. 详细解释可以看这里) 5.1.20. > The disappearing variable from outer scope/消失的外部变量 e = 7 try: raise Exception() except Exception as e: pass Output (Python 2.x): >>> print(e) # prints nothing Output (Python 3.x): >>> print(e) NameError: name 'e' is not defined &#x1F4A1; 说明: 出处: https://docs.python.org/3/reference/compound_stmts.html#except 当使用 as 为目标分配异常的时候, 将在except子句的末尾清除该异常. 这就好像 except E as N: foo 会被翻译成 except E as N: try: foo finally: del N 这意味着异常必须在被赋值给其他变量才能在 except 子句之后引用它. 而异常之所以会被清除, 则是由于上面附加的回溯信息(trackback)会和栈帧(stack frame)形成循环引用, 使得该栈帧中的所有本地变量在下一次垃圾回收发生之前都处于活动状态.(译: 也就是说不会被回收) 子句在 Python 中并没有独立的作用域. 示例中的所有内容都处于同一作用域内, 所以变量 e 会由于执行了 except 子句而被删除. 而对于有独立的内部作用域的函数来说情况就不一样了. 下面的例子说明了这一点: def f(x): del(x) print(x) x = 5 y = [5, 4, 3] Output: >>>f(x) UnboundLocalError: local variable 'x' referenced before assignment >>>f(y) UnboundLocalError: local variable 'x' referenced before assignment >>> x 5 >>> y [5, 4, 3] 在 Python 2.x 中, Exception() 实例被赋值给了变量 e, 所以当你尝试打印结果的时候, 它的输出为空.（译: 正常的Exception实例打印出来就是空） Output (Python 2.x): >>> e Exception() >>> print e # 没有打印任何内容! 5.1.21. > When True is actually False/真亦假 True = False if True == False: print(\"I've lost faith in truth!\") Output: I've lost faith in truth! &#x1F4A1; 说明: 最初, Python 并没有 bool 型 (人们用0表示假值, 用非零值比如1作为真值). 后来他们添加了 True, False, 和 bool 型, 但是, 为了向后兼容, 他们没法把 True 和 False 设置为常量, 只是设置成了内置变量. Python 3 由于不再需要向后兼容, 终于可以修复这个问题了, 所以这个例子无法在 Python 3.x 中执行! 5.1.22. > From filled to None in one instruction.../从有到无... some_list = [1, 2, 3] some_dict = { \"key_1\": 1, \"key_2\": 2, \"key_3\": 3 } some_list = some_list.append(4) some_dict = some_dict.update({\"key_4\": 4}) Output: >>> print(some_list) None >>> print(some_dict) None &#x1F4A1; 说明: 大多数修改序列/映射对象的方法, 比如 list.append, dict.update, list.sort 等等. 都是原地修改对象并返回 None. 这样做的理由是, 如果操作可以原地完成, 就可以避免创建对象的副本来提高性能. (参考这里) 5.1.23. > Subclass relationships/子类关系 * Output: >>> from collections import Hashable >>> issubclass(list, object) True >>> issubclass(object, Hashable) True >>> issubclass(list, Hashable) False 子类关系应该是可传递的, 对吧? (即, 如果 A 是 B 的子类, B 是 C 的子类, 那么 A 应该 是 C 的子类.) &#x1F4A1; 说明: Python 中的子类关系并不一定是传递的. 任何人都可以在元类中随意定义 __subclasscheck__. 当 issubclass(cls, Hashable) 被调用时, 它只是在 cls 中寻找 __hash__ 方法或者从继承的父类中寻找 __hash__ 方法. 由于 object is 可散列的(hashable), 但是 list 是不可散列的, 所以它打破了这种传递关系. 在这里可以找到更详细的解释. 5.1.24. > The mysterious key type conversion/神秘的键型转换 * class SomeClass(str): pass some_dict = {'s':42} Output: >>> type(list(some_dict.keys())[0]) str >>> s = SomeClass('s') >>> some_dict[s] = 40 >>> some_dict # 预期: 两个不同的键值对 {'s': 40} >>> type(list(some_dict.keys())[0]) str &#x1F4A1; 说明: 由于 SomeClass 会从 str 自动继承 __hash__ 方法, 所以 s 对象和 \"s\" 字符串的哈希值是相同的. 而 SomeClass(\"s\") == \"s\" 为 True 是因为 SomeClass 也继承了 str 类 __eq__ 方法. 由于两者的哈希值相同且相等, 所以它们在字典中表示相同的键. 如果想要实现期望的功能, 我们可以重定义 SomeClass 的 __eq__ 方法. class SomeClass(str): def __eq__(self, other): return ( type(self) is SomeClass and type(other) is SomeClass and super().__eq__(other) ) # 当我们自定义 __eq__ 方法时, Python 不会再自动继承 __hash__ 方法 # 所以我们也需要定义它 __hash__ = str.__hash__ some_dict = {'s':42} Output: >>> s = SomeClass('s') >>> some_dict[s] = 40 >>> some_dict {'s': 40, 's': 42} >>> keys = list(some_dict.keys()) >>> type(keys[0]), type(keys[1]) (__main__.SomeClass, str) 5.1.25. > Let's see if you can guess this?/看看你能否猜到这一点? a, b = a[b] = {}, 5 Output: >>> a {5: ({...}, 5)} &#x1F4A1; 说明: 根据 Python 语言参考, 赋值语句的形式如下 (target_list \"=\")+ (expression_list | yield_expression) 赋值语句计算表达式列表(expression list)(牢记 这可以是单个表达式或以逗号分隔的列表, 后者返回元组)并将单个结果对象从左到右分配给目标列表中的每一项. (target_list \"=\")+ 中的 + 意味着可以有一个或多个目标列表. 在这个例子中, 目标列表是 a, b 和 a[b] (注意表达式列表只能有一个, 在我们的例子中是 {}, 5). 表达式列表计算结束后, 将其值自动解包后从左到右分配给目标列表(target list). 因此, 在我们的例子中, 首先将 {}, 5 元组并赋值给 a, b, 然后我们就可以得到 a = {} 且 b = 5. a 被赋值的 {} 是可变对象. 第二个目标列表是 a[b] (你可能觉得这里会报错, 因为在之前的语句中 a 和 b 都还没有被定义. 但是别忘了, 我们刚刚将 a 赋值 {} 且将 b 赋值为 5). 现在, 我们将通过将字典中键 5 的值设置为元组 ({}, 5) 来创建循环引用 (输出中的 {...} 指与 a 引用了相同的对象). 下面是一个更简单的循环引用的例子 >>> some_list = some_list[0] = [0] >>> some_list [[...]] >>> some_list[0] [[...]] >>> some_list is some_list[0] True >>> some_list[0][0][0][0][0][0] == some_list True 我们的例子就是这种情况 (a[b][0] 与 a 是相同的对象) 总结一下, 你也可以把例子拆成 a, b = {}, 5 a[b] = a, b 并且可以通过 a[b][0] 与 a 是相同的对象来证明是循环引用 >>> a[b][0] is a True 5.2. Section: Appearances are deceptive!/外表是靠不住的! 5.2.1. > Skipping lines?/跳过一行? Output: >>> value = 11 >>> valuе = 32 >>> value 11 什么鬼? 注意: 如果你想要重现的话最简单的方法是直接复制上面的代码片段到你的文件或命令行里. &#x1F4A1; 说明: 一些非西方字符虽然看起来和英语字母相同, 但会被解释器识别为不同的字母. >>> ord('е') # 西里尔语的 'e' (Ye) 1077 >>> ord('e') # 拉丁语的 'e', 用于英文并使用标准键盘输入 101 >>> 'е' == 'e' False >>> value = 42 # 拉丁语 e >>> valuе = 23 # 西里尔语 'e', Python 2.x 的解释器在这会抛出 `SyntaxError` 异常 >>> value 42 内置的 ord() 函数可以返回一个字符的 Unicode 代码点, 这里西里尔语 'e' 和拉丁语 'e' 的代码点不同证实了上述例子. 5.2.2. > Teleportation/空间移动 * import numpy as np def energy_send(x): # 初始化一个 numpy 数组 np.array([float(x)]) def energy_receive(): # 返回一个空的 numpy 数组 return np.empty((), dtype=np.float).tolist() Output: >>> energy_send(123.456) >>> energy_receive() 123.456 谁来给我发个诺贝尔奖? &#x1F4A1; 说明: 注意在 energy_send 函数中创建的 numpy 数组并没有返回, 因此内存空间被释放并可以被重新分配. numpy.empty() 直接返回下一段空闲内存，而不重新初始化. 而这个内存点恰好就是刚刚释放的那个(通常情况下, 并不绝对). 5.2.3. > Well, something is fishy.../嗯，有些可疑... def square(x): \"\"\" 一个通过加法计算平方的简单函数. \"\"\" sum_so_far = 0 for counter in range(x): sum_so_far = sum_so_far + x return sum_so_far Output (Python 2.x): >>> square(10) 10 难道不应该是100吗? 注意: 如果你无法重现, 可以尝试运行这个文件mixed_tabs_and_spaces.py. &#x1F4A1; 说明: 不要混用制表符(tab)和空格(space)! 在上面的例子中, return 的前面是\"1个制表符\", 而其他部分的代码前面是 \"4个空格\". Python是这么处理制表符的: 首先, 制表符会从左到右依次被替换成8个空格, 直到被替换后的字符总数是八的倍数 因此, square 函数最后一行的制表符会被替换成8个空格, 导致return语句进入循环语句里面. Python 3 很友好, 在这种情况下会自动抛出错误. Output (Python 3.x): TabError: inconsistent use of tabs and spaces in indentation 5.3. Section: Watch out for the landmines!/小心地雷! 5.3.1. > Modifying a dictionary while iterating over it/迭代字典时的修改 x = {0: None} for i in x: del x[i] x[i+1] = None print(i) Output (Python 2.7- Python 3.5): 0 1 2 3 4 5 6 7 是的, 它运行了八次然后才停下来. &#x1F4A1; 说明: Python不支持对字典进行迭代的同时修改它. 它之所以运行8次, 是因为字典会自动扩容以容纳更多键值(我们有8次删除记录, 因此需要扩容). 这实际上是一个实现细节. (译: 应该是因为字典的初始最小值是8, 扩容会导致散列表地址发生变化而中断循环.) 在不同的Python实现中删除键的处理方式以及调整大小的时间可能会有所不同.(译: 就是说什么时候扩容在不同版本中可能是不同的, 在3.6及3.7的版本中到5就会自动扩容了. 以后也有可能再次发生变化. 这是为了避免散列冲突. 顺带一提, 后面两次扩容会扩展为32和256. 即8->32->256.) 更多的信息, 你可以参考这个StackOverflow的回答, 它详细的解释一个类似的例子. 5.3.2. > Stubborn del operator/坚强的 del * class SomeClass: def __del__(self): print(\"Deleted!\") Output: 1. >>> x = SomeClass() >>> y = x >>> del x # 这里应该会输出 \"Deleted!\" >>> del y Deleted! 唷, 终于删除了. 你可能已经猜到了在我们第一次尝试删除 x 时是什么让 __del__ 免于被调用的. 那让我们给这个例子增加点难度. 2. >>> x = SomeClass() >>> y = x >>> del x >>> y # 检查一下y是否存在 >>> del y # 像之前一样, 这里应该会输出 \"Deleted!\" >>> globals() # 好吧, 并没有. 让我们看一下所有的全局变量 Deleted! {'__builtins__': , 'SomeClass': , '__package__': None, '__name__': '__main__', '__doc__': None} 好了，现在它被删除了 :confused: &#x1F4A1; 说明: del x 并不会立刻调用 x.__del__(). 每当遇到 del x, Python 会将 x 的引用数减1, 当 x 的引用数减到0时就会调用 x.__del__(). 在第二个例子中, y.__del__() 之所以未被调用, 是因为前一条语句 (>>> y) 对同一对象创建了另一个引用, 从而防止在执行 del y 后对象的引用数变为0. 调用 globals 导致引用被销毁, 因此我们可以看到 \"Deleted!\" 终于被输出了. (译: 这其实是 Python 交互解释器的特性, 它会自动让 _ 保存上一个表达式输出的值, 详细可以看这里.) 5.3.3. > Deleting a list item while iterating/迭代列表时删除元素 list_1 = [1, 2, 3, 4] list_2 = [1, 2, 3, 4] list_3 = [1, 2, 3, 4] list_4 = [1, 2, 3, 4] for idx, item in enumerate(list_1): del item for idx, item in enumerate(list_2): list_2.remove(item) for idx, item in enumerate(list_3[:]): list_3.remove(item) for idx, item in enumerate(list_4): list_4.pop(idx) Output: >>> list_1 [1, 2, 3, 4] >>> list_2 [2, 4] >>> list_3 [] >>> list_4 [2, 4] 你能猜到为什么输出是 [2, 4] 吗? &#x1F4A1; 说明: 在迭代时修改对象是一个很愚蠢的主意. 正确的做法是迭代对象的副本, list_3[:] 就是这么做的. >>> some_list = [1, 2, 3, 4] >>> id(some_list) 139798789457608 >>> id(some_list[:]) # 注意python为切片列表创建了新对象. 139798779601192 del, remove 和 pop 的不同: del var_name 只是从本地或全局命名空间中删除了 var_name (这就是为什么 list_1 没有受到影响). remove 会删除第一个匹配到的指定值, 而不是特定的索引, 如果找不到值则抛出 ValueError 异常. pop 则会删除指定索引处的元素并返回它, 如果指定了无效的索引则抛出 IndexError 异常. 为什么输出是 [2, 4]? 列表迭代是按索引进行的, 所以当我们从 list_2 或 list_4 中删除 1 时, 列表的内容就变成了 [2, 3, 4]. 剩余元素会依次位移, 也就是说, 2 的索引会变为 0, 3 会变为 1. 由于下一次迭代将获取索引为 1 的元素 (即 3), 因此 2 将被彻底的跳过. 类似的情况会交替发生在列表中的每个元素上. 参考这个StackOverflow的回答来解释这个例子 关于Python中字典的类似例子, 可以参考这个Stackoverflow的回答. 5.3.4. > Loop variables leaking out!/循环变量泄漏! 1. for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') Output: 6 : for x inside loop 6 : x in global 但是 x 从未在循环外被定义... 2. # 这次我们先初始化x x = -1 for x in range(7): if x == 6: print(x, ': for x inside loop') print(x, ': x in global') Output: 6 : for x inside loop 6 : x in global 3. x = 1 print([x for x in range(5)]) print(x, ': x in global') Output (on Python 2.x): [0, 1, 2, 3, 4] (4, ': x in global') Output (on Python 3.x): [0, 1, 2, 3, 4] 1 : x in global &#x1F4A1; 说明: 在 Python 中, for 循环使用所在作用域并在结束后保留定义的循环变量. 如果我们曾在全局命名空间中定义过循环变量. 在这种情况下, 它会重新绑定现有变量. Python 2.x 和 Python 3.x 解释器在列表推导式示例中的输出差异, 在文档 What’s New In Python 3.0 中可以找到相关的解释: \"列表推导不再支持句法形式 [... for var in item1, item2, ...]. 取而代之的是 [... for var in (item1, item2, ...)]. 另外, 注意列表推导具有不同的语义: 它们更接近于 list() 构造函数中生成器表达式的语法糖(译: 这一句我也不是很明白), 特别是循环控制变量不再泄漏到周围的作用域中.\" 5.3.5. > Beware of default mutable arguments!/当心默认的可变参数! def some_func(default_arg=[]): default_arg.append(\"some_string\") return default_arg Output: >>> some_func() ['some_string'] >>> some_func() ['some_string', 'some_string'] >>> some_func([]) ['some_string'] >>> some_func() ['some_string', 'some_string', 'some_string'] &#x1F4A1; 说明: Python中函数的默认可变参数并不是每次调用该函数时都会被初始化. 相反, 它们会使用最近分配的值作为默认值. 当我们明确的将 [] 作为参数传递给 some_func 的时候, 就不会使用 default_arg 的默认值, 所以函数会返回我们所期望的结果. def some_func(default_arg=[]): default_arg.append(\"some_string\") return default_arg Output: >>> some_func.__defaults__ # 这里会显示函数的默认参数的值 ([],) >>> some_func() >>> some_func.__defaults__ (['some_string'],) >>> some_func() >>> some_func.__defaults__ (['some_string', 'some_string'],) >>> some_func([]) >>> some_func.__defaults__ (['some_string', 'some_string'],) 避免可变参数导致的错误的常见做法是将 None 指定为参数的默认值, 然后检查是否有值传给对应的参数. 例: def some_func(default_arg=None): if not default_arg: default_arg = [] default_arg.append(\"some_string\") return default_arg 5.3.6. > Catching the Exceptions/捕获异常 some_list = [1, 2, 3] try: # 这里会抛出异常 ``IndexError`` print(some_list[4]) except IndexError, ValueError: print(\"Caught!\") try: # 这里会抛出异常 ``ValueError`` some_list.remove(4) except IndexError, ValueError: print(\"Caught again!\") Output (Python 2.x): Caught! ValueError: list.remove(x): x not in list Output (Python 3.x): File \"\", line 3 except IndexError, ValueError: ^ SyntaxError: invalid syntax &#x1F4A1; 说明: 如果你想要同时捕获多个不同类型的异常时, 你需要将它们用括号包成一个元组作为第一个参数传递. 第二个参数是可选名称, 如果你提供, 它将与被捕获的异常实例绑定. 例, some_list = [1, 2, 3] try: # 这里会抛出异常 ``ValueError`` some_list.remove(4) except (IndexError, ValueError), e: print(\"Caught again!\") print(e) Output (Python 2.x): Caught again! list.remove(x): x not in list Output (Python 3.x): File \"\", line 4 except (IndexError, ValueError), e: ^ IndentationError: unindent does not match any outer indentation level 在 Python 3 中, 用逗号区分异常与可选名称是无效的; 正确的做法是使用 as 关键字. 例, some_list = [1, 2, 3] try: some_list.remove(4) except (IndexError, ValueError) as e: print(\"Caught again!\") print(e) Output: Caught again! list.remove(x): x not in list 5.3.7. > Same operands, different story!/同人不同命! 1. a = [1, 2, 3, 4] b = a a = a + [5, 6, 7, 8] Output: >>> a [1, 2, 3, 4, 5, 6, 7, 8] >>> b [1, 2, 3, 4] 2. a = [1, 2, 3, 4] b = a a += [5, 6, 7, 8] Output: >>> a [1, 2, 3, 4, 5, 6, 7, 8] >>> b [1, 2, 3, 4, 5, 6, 7, 8] &#x1F4A1; 说明: a += b 并不总是与 a = a + b 表现相同. 类实现 op= 运算符的方式 也许 是不同的, 列表就是这样做的. 表达式 a = a + [5,6,7,8] 会生成一个新列表, 并让 a 引用这个新列表, 同时保持 b 不变. 表达式 a += [5,6,7,8] 实际上是使用的是 \"extend\" 函数, 所以 a 和 b 仍然指向已被修改的同一列表. 5.3.8. > The out of scope variable/外部作用域变量 a = 1 def some_func(): return a def another_func(): a += 1 return a Output: >>> some_func() 1 >>> another_func() UnboundLocalError: local variable 'a' referenced before assignment &#x1F4A1; 说明: 当你在作用域中对变量进行赋值时, 变量会变成该作用域内的局部变量. 因此 a 会变成 another_func 函数作用域中的局部变量, 但它在函数作用域中并没有被初始化, 所以会引发错误. 可以阅读这个简短却很棒的指南, 了解更多关于 Python 中命名空间和作用域的工作原理. 想要在 another_func 中修改外部作用域变量 a 的话, 可以使用 global 关键字. def another_func() global a a += 1 return a Output: >>> another_func() 2 5.3.9. > Be careful with chained operations/小心链式操作 >>> (False == False) in [False] # 可以理解 False >>> False == (False in [False]) # 可以理解 False >>> False == False in [False] # 为毛? True >>> True is False == False False >>> False is False is False True >>> 1 > 0 >> (1 > 0) >> 1 > (0 &#x1F4A1; 说明: 根据 https://docs.python.org/2/reference/expressions.html#not-in 形式上, 如果 a, b, c, ..., y, z 是表达式, 而 op1, op2, ..., opN 是比较运算符, 那么除了每个表达式最多只出现一次以外 a op1 b op2 c ... y opN z 就等于 a op1 b and b op2 c and ... y opN z. 虽然上面的例子似乎很愚蠢, 但是像 a == b == c 或 0 就很棒了. False is False is False 相当于 (False is False) and (False is False) True is False == False 相当于 True is False and False == False, 由于语句的第一部分 (True is False) 等于 False, 因此整个表达式的结果为 False. 1 > 0 相当于 1 > 0 and 0 , 所以最终结果为 True. 表达式 (1 > 0) 相当于 True 且>>> int(True) 1 >>> True + 1 # 与这个例子无关，只是好玩 2 所以, 1 等于 False 5.3.10. > Name resolution ignoring class scope/忽略类作用域的名称解析 1. x = 5 class SomeClass: x = 17 y = (x for i in range(10)) Output: >>> list(SomeClass.y)[0] 5 2. x = 5 class SomeClass: x = 17 y = [x for i in range(10)] Output (Python 2.x): >>> SomeClass.y[0] 17 Output (Python 3.x): >>> SomeClass.y[0] 5 &#x1F4A1; 说明: 类定义中嵌套的作用域会忽略类内的名称绑定. 生成器表达式有它自己的作用域. 从 Python 3.X 开始, 列表推导式也有自己的作用域. 5.3.11. > Needle in a Haystack/大海捞针 1. x, y = (0, 1) if True else None, None Output: >>> x, y # 期望的结果是 (0, 1) ((0, 1), None) 几乎每个 Python 程序员都遇到过类似的情况. 2. t = ('one', 'two') for i in t: print(i) t = ('one') for i in t: print(i) t = () print(t) Output: one two o n e tuple() &#x1F4A1; 说明: 对于 1, 正确的语句是 x, y = (0, 1) if True else (None, None). 对于 2, 正确的语句是 t = ('one',) 或者 t = 'one', (缺少逗号) 否则解释器会认为 t 是一个字符串, 并逐个字符对其进行迭代. () 是一个特殊的标记，表示空元组. 5.4. Section: The Hidden treasures!/隐藏的宝藏! This section contains few of the lesser-known interesting things about Python that most beginners like me are unaware of (well, not anymore). 5.4.1. > Okay Python, Can you make me fly?/Python, 可否带我飞? * 好, 去吧. import antigravity Output: 嘘.. 这是个超级秘密. &#x1F4A1; 说明: antigravity 模块是 Python 开发人员发布的少数复活节彩蛋之一. import antigravity 会打开一个 Python 的经典 XKCD 漫画页面. 不止如此. 这个复活节彩蛋里还有一个复活节彩蛋. 如果你看一下代码, 就会发现还有一个函数实现了 XKCD's geohashing 算法. 5.4.2. > goto, but why?/goto, 但为什么? * from goto import goto, label for i in range(9): for j in range(9): for k in range(9): print(\"I'm trapped, please rescue!\") if k == 2: goto .breakout # 从多重循环中跳出 label .breakout print(\"Freedom!\") Output (Python 2.3): I'm trapped, please rescue! I'm trapped, please rescue! Freedom! &#x1F4A1; 说明: 2004年4月1日, Python 宣布 加入一个可用的 goto 作为愚人节礼物. 当前版本的 Python 并没有这个模块. 就算可以用, 也请不要使用它. 这里是为什么Python中没有 goto 的原因. 5.4.3. > Brace yourself!/做好思想准备 * 如果你不喜欢在Python中使用空格来表示作用域, 你可以导入 C 风格的 {}, from __future__ import braces Output: File \"some_file.py\", line 1 from __future__ import braces SyntaxError: not a chance 想用大括号? 没门! 觉得不爽, 请去用java. &#x1F4A1; 说明: 通常 __future__ 会提供 Python 未来版本的功能. 然而，这里的 “未来” 是一个讽刺. 这是一个表达社区对此类问题态度的复活节彩蛋. 5.4.4. > Let's meet Friendly Language Uncle For Life/让生活更友好 * Output (Python 3.x) >>> from __future__ import barry_as_FLUFL >>> \"Ruby\" != \"Python\" # 这里没什么疑问 File \"some_file.py\", line 1 \"Ruby\" != \"Python\" ^ SyntaxError: invalid syntax >>> \"Ruby\" <> \"Python\" True 这就对了. &#x1F4A1; 说明: 相关的 PEP-401 发布于 2009年4月1日 (所以你现在知道这意味着什么了吧). 引用 PEP-401 意识到 Python 3.0 里的 != 运算符是一个会引起手指疼痛的恐怖错误, FLUFL 将 <> 运算符恢复为唯一写法. Uncle Barry 在 PEP 中还分享了其他东西; 你可以在这里获得他们. (译: 虽然文档中没写，但应该是只能在交互解释器中使用.) 5.4.5. > Even Python understands that love is complicated/连Python也知道爱是难言的 * import this 等等, this 是什么? this 是爱 :heart: Output: The Zen of Python, by Tim Peters Beautiful is better than ugly. 优美胜于丑陋（Python 以编写优美的代码为目标） Explicit is better than implicit. 明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似） Simple is better than complex. 简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现） Complex is better than complicated. 复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁） Flat is better than nested. 扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套） Sparse is better than dense. 间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题） Readability counts. 可读性很重要（优美的代码一定是可读的） Special cases aren't special enough to break the rules. 没有特例特殊到需要违背这些规则（这些规则至高无上） Although practicality beats purity. 尽管我们更倾向于实用性 Errors should never pass silently. 不要安静的包容所有错误 Unless explicitly silenced. 除非你确定需要这样做（精准地捕获异常，不写 except:pass 风格的代码） In the face of ambiguity, refuse the temptation to guess. 拒绝诱惑你去猜测的暧昧事物 There should be one-- and preferably only one --obvious way to do it. 而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法） Although that way may not be obvious at first unless you're Dutch. 虽然这并不容易，因为你不是 Python 之父（这里的 Dutch 是指 Guido ） Now is better than never. 现在行动好过永远不行动 Although never is often better than *right* now. 尽管不行动要好过鲁莽行动 If the implementation is hard to explain, it's a bad idea. 如果你无法向人描述你的方案，那肯定不是一个好方案； If the implementation is easy to explain, it may be a good idea. 如果你能轻松向人描述你的方案，那也许会是一个好方案（方案测评标准） Namespaces are one honking great idea -- let's do more of those! 命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召） 这是 Python 之禅! >>> love = this >>> this is love True >>> love is True False >>> love is False False >>> love is not True or False True >>> love is not True or False; love is love # 爱是难言的 True &#x1F4A1; 说明: this 模块是关于 Python 之禅的复活节彩蛋 (PEP 20). 如果你认为这已经够有趣的了, 可以看看 this.py 的实现. 有趣的是, Python 之禅的实现代码违反了他自己 (这可能是唯一会发生这种情况的地方). * 至于 love is not True or False; love is love, 意外却又不言而喻. 5.4.6. > Yes, it exists!/是的, 它存在! 循环的 else. 一个典型的例子: def does_exists_num(l, to_find): for num in l: if num == to_find: print(\"Exists!\") break else: print(\"Does not exist\") Output: >>> some_list = [1, 2, 3, 4, 5] >>> does_exists_num(some_list, 4) Exists! >>> does_exists_num(some_list, -1) Does not exist 异常的 else . 例, try: pass except: print(\"Exception occurred!!!\") else: print(\"Try block executed successfully...\") Output: Try block executed successfully... &#x1F4A1; 说明: 循环后的 else 子句只会在循环没有触发 break 语句, 正常结束的情况下才会执行. try 之后的 else 子句也被称为 \"完成子句\", 因为在 try 语句中到达 else 子句意味着try块实际上已成功完成. 5.4.7. > Inpinity/无限 * 英文拼写是有意的, 请不要为此提交补丁. (译: 这里是为了突出 Python 中无限的定义与Pi有关, 所以将两个单词拼接了.) Output (Python 3.x): >>> infinity = float('infinity') >>> hash(infinity) 314159 >>> hash(float('-inf')) -314159 &#x1F4A1; 说明: infinity 的哈希值是 10⁵ x π. 有意思的是, float('-inf') 的哈希值在 Python 3 中是 \"-10⁵ x π\" , 而在 Python 2 中是 \"-10⁵ x e\". 5.4.8. > Mangling time!/修饰时间! * class Yo(object): def __init__(self): self.__honey = True self.bitch = True Output: >>> Yo().bitch True >>> Yo().__honey AttributeError: 'Yo' object has no attribute '__honey' >>> Yo()._Yo__honey True 为什么 Yo()._Yo__honey 能运行? 只有印度人理解.(译: 这个梗可能是指印度音乐人Yo Yo Honey Singh) &#x1F4A1; 说明: 名字修饰 用于避免不同命名空间之间名称冲突. 在 Python 中, 解释器会通过给类中以 __ (双下划线)开头且结尾最多只有一个下划线的类成员名称加上_NameOfTheClass 来修饰(mangles)名称. 所以, 要访问 __honey 对象,我们需要加上 _Yo 以防止与其他类中定义的相同名称的属性发生冲突. 5.5. Section: Miscellaneous/杂项 5.5.1. > += is faster/更快的 += # 用 \"+\" 连接三个字符串: >>> timeit.timeit(\"s1 = s1 + s2 + s3\", setup=\"s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000\", number=100) 0.25748300552368164 # 用 \"+=\" 连接三个字符串: >>> timeit.timeit(\"s1 += s2 + s3\", setup=\"s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000\", number=100) 0.012188911437988281 &#x1F4A1; 说明: 连接两个以上的字符串时 += 比 + 更快, 因为在计算过程中第一个字符串 (例如, s1 += s2 + s3 中的 s1) 不会被销毁.(译: 就是 += 执行的是追加操作，少了一个销毁新建的动作.) 5.5.2. > Let's make a giant string!/来做个巨大的字符串吧！ def add_string_with_plus(iters): s = \"\" for i in range(iters): s += \"xyz\" assert len(s) == 3*iters def add_bytes_with_plus(iters): s = b\"\" for i in range(iters): s += b\"xyz\" assert len(s) == 3*iters def add_string_with_format(iters): fs = \"{}\"*iters s = fs.format(*([\"xyz\"]*iters)) assert len(s) == 3*iters def add_string_with_join(iters): l = [] for i in range(iters): l.append(\"xyz\") s = \"\".join(l) assert len(s) == 3*iters def convert_list_to_string(l, iters): s = \"\".join(l) assert len(s) == 3*iters Output: >>> timeit(add_string_with_plus(10000)) 1000 loops, best of 3: 972 µs per loop >>> timeit(add_bytes_with_plus(10000)) 1000 loops, best of 3: 815 µs per loop >>> timeit(add_string_with_format(10000)) 1000 loops, best of 3: 508 µs per loop >>> timeit(add_string_with_join(10000)) 1000 loops, best of 3: 878 µs per loop >>> l = [\"xyz\"]*10000 >>> timeit(convert_list_to_string(l, 10000)) 10000 loops, best of 3: 80 µs per loop 让我们将迭代次数增加10倍. >>> timeit(add_string_with_plus(100000)) # 执行时间线性增加 100 loops, best of 3: 9.75 ms per loop >>> timeit(add_bytes_with_plus(100000)) # 二次增加 1000 loops, best of 3: 974 ms per loop >>> timeit(add_string_with_format(100000)) # 线性增加 100 loops, best of 3: 5.25 ms per loop >>> timeit(add_string_with_join(100000)) # 线性增加 100 loops, best of 3: 9.85 ms per loop >>> l = [\"xyz\"]*100000 >>> timeit(convert_list_to_string(l, 100000)) # 线性增加 1000 loops, best of 3: 723 µs per loop &#x1F4A1; 说明: 你可以在这获得更多 timeit 的相关信息. 它通常用于衡量代码片段的执行时间. 不要用 + 去生成过长的字符串, 在 Python 中, str 是不可变的, 所以在每次连接中你都要把左右两个字符串复制到新的字符串中. 如果你连接四个长度为10的字符串, 你需要拷贝 (10+10) + ((10+10)+10) + (((10+10)+10)+10) = 90 个字符而不是 40 个字符. 随着字符串的数量和大小的增加, 情况会变得越发的糟糕 (就像add_bytes_with_plus 函数的执行时间一样) 因此, 更建议使用 .format. 或 % 语法 (但是, 对于短字符串, 它们比 + 稍慢一点). 又或者, 如果你所需的内容已经以可迭代对象的形式提供了, 使用 ''.join(可迭代对象) 要快多了. add_string_with_plus 的执行时间没有像 add_bytes_with_plus 一样出现二次增加是因为解释器会如同上一个例子所讨论的一样优化 +=. 用 s = s + \"x\" + \"y\" + \"z\" 替代 s += \"xyz\" 的话, 执行时间就会二次增加了. def add_string_with_plus(iters): s = \"\" for i in range(iters): s = s + \"x\" + \"y\" + \"z\" assert len(s) == 3*iters >>> timeit(add_string_with_plus(10000)) 100 loops, best of 3: 9.87 ms per loop >>> timeit(add_string_with_plus(100000)) # 执行时间二次增加 1 loops, best of 3: 1.09 s per loop 5.5.3. > Explicit typecast of strings/字符串的显式类型转换 a = float('inf') b = float('nan') c = float('-iNf') # 这些字符串不区分大小写 d = float('nan') Output: >>> a inf >>> b nan >>> c -inf >>> float('some_other_string') ValueError: could not convert string to float: some_other_string >>> a == -c #inf==inf True >>> None == None # None==None True >>> b == d #但是 nan!=nan False >>> 50/a 0.0 >>> a/a nan >>> 23 + b nan &#x1F4A1; 说明: 'inf' 和 'nan' 是特殊的字符串(不区分大小写), 当显示转换成 float 型时, 它们分别用于表示数学意义上的 \"无穷大\" 和 \"非数字\". 5.5.4. > Minor Ones/小知识点 join() 是一个字符串操作而不是列表操作. (第一次接触会觉得有点违反直觉) &#x1F4A1; 说明: 如果 join() 是字符串方法 那么它就可以处理任何可迭代的对象(列表，元组，迭代器). 如果它是列表方法, 则必须在每种类型中单独实现. 另外, 在 list 对象的通用API中实现一个专用于字符串的方法没有太大的意义. 看着奇怪但能正确运行的语句: [] = () 语句在语义上是正确的 (解包一个空的 tuple 并赋值给 list) 'a'[0][0][0][0][0] 在语义上也是正确的, 因为在 Python 中字符串同时也是序列(可迭代对象支持使用整数索引访问元素). 3 --0-- 5 == 8 和 --5 == 5 在语义上都是正确的, 且结果等于 True.(译: 3减负0等于3，再减负5相当于加5等于8；负的负5等于5.) 鉴于 a 是一个数字, ++a 和 --a 都是有效的 Python 语句, 但其效果与 C, C++ 或 Java 等不一样. >>> a = 5 >>> a 5 >>> ++a 5 >>> --a 5 &#x1F4A1; 说明: python 里没有 ++ 操作符. 这其实是两个 + 操作符. ++a 被解析为 +(+a) 最后等于 a. --a 同理. 这个 StackOverflow 回答 讨论了为什么 Python 中缺少增量和减量运算符. Python 使用 2个字节存储函数中的本地变量. 理论上, 这意味着函数中只能定义65536个变量. 但是，Python 内置了一个方便的解决方案，可用于存储超过2^16个变量名. 下面的代码演示了当定义了超过65536个局部变量时堆栈中发生的情况 (警告: 这段代码会打印大约2^18行文本, 请做好准备!): import dis exec(\"\"\" def f(): \"\"\" + \"\"\" \"\"\".join([\"X\"+str(x)+\"=\" + str(x) for x in range(65539)])) f() print(dis.dis(f)) 你的 Python 代码 并不会多线程同时运行 (是的, 你没听错!). 虽然你觉得会产生多个线程并让它们同时执行你的代码, 但是, 由于 全局解释锁的存在, 你所做的只是让你的线程依次在同一个核心上执行. Python 多线程适用于IO密集型的任务, 但如果想要并行处理CPU密集型的任务, 你应该会想使用 multiprocessing 模块. 列表切片超出索引边界而不引发任何错误 >>> some_list = [1, 2, 3, 4, 5] >>> some_list[111:] [] int('١٢٣٤٥٦٧٨٩') 在 Python 3 中会返回 123456789. 在 Python 中, 十进制字符包括数字字符, 以及可用于形成十进制数字的所有字符, 例如: U+0660, ARABIC-INDIC DIGIT ZERO. 这有一个关于此的 有趣故事. 'abc'.count('') == 4. 这有一个 count 方法的相近实现, 能更好的说明问题 def count(s, sub): result = 0 for i in range(len(s) + 1 - len(sub)): result += (s[i:i + len(sub)] == sub) return result 这个行为是由于空子串('')与原始字符串中长度为0的切片相匹配导致的. 6. Contributing/贡献 欢迎各种补丁! 详情请看CONTRIBUTING.md.(译: 这是给原库提贡献的要求模版) 你可以通过新建 issue 或者在上 Gitter 与我们进行讨论. (译: 如果你想对这个翻译项目提供帮助, 请看这里) 7. Acknowledgements/致谢 这个系列最初的想法和设计灵感来自于 Denys Dovhan 的项目 wtfjs. 社区的强大支持让它成长为现在的模样. Some nice Links!/一些不错的资源 https://www.youtube.com/watch?v=sH4XF6pKKmk https://www.reddit.com/r/Python/comments/3cu6ej/what_are_some_wtf_things_about_python https://sopython.com/wiki/Common_Gotchas_In_Python https://stackoverflow.com/questions/530530/python-2-x-gotchas-and-landmines https://stackoverflow.com/questions/1011431/common-pitfalls-in-python https://www.python.org/doc/humor/ https://www.codementor.io/satwikkansal/python-practices-for-efficient-code-performance-memory-and-usability-aze6oiq65 8. &#x1F393; License/许可 © Satwik Kansal 8.1. Help/帮助 如果您有任何想法或建议，欢迎分享. 8.2. Surprise your geeky pythonist friends?/想给你的极客朋友一个惊喜? 您可以使用这些快链向 Twitter 和 Linkedin 上的朋友推荐 wtfpython, Twitter | Linkedin 8.3. Need a pdf version?/需要来一份pdf版的? 我收到一些想要pdf版本的需求. 你可以快速在这获得. 8.4. Follow Commit/追踪Commit 这是中文版 fork 时所处的原库 Commit, 当原库更新时我会跟随更新. 8.5. 996.icu © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 12:54:26 "},"Langs/fluent_python.html":{"url":"Langs/fluent_python.html","title":"1.2 Fluent Python","keywords":"","body":"1. fluent python1.1. Chapter 1 数据模型1.1.1. Se1 magic method1.1.2. Se2 using it and show more1.2. Chapter 2 数据结构1.2.1. se 1 序列构成的数组1.2.2. Se2 字典和集合1.3. Chapter 3 把函数视作对象1.3.1. se 3 函数装饰器和闭包1. fluent python AikenHong2021 h.aiken.970@gmail.com Time: Sat 06 Nov 2021 12:53:47 AM HKT Describe: systematic learning of python 1.1. Chapter 1 数据模型 1.1.1. Se1 magic method 数据模型在这里的定义是对python框架的描述,他规范了python构建模块的接口；这些接口对应解释器中对一些特殊句法(常用句法)的激活和使用.本章节的核心就在于对这些特殊句法的理解和使用. 特殊方法带来的一些主要交互场景: 迭代 属性访问 集合类 函数和方法的调用 对象的创建和销毁 字符串的表示形式和格式化 上下文管理模块(with模块) 这些特殊方法的存在实际上，是为了让python的解释器调用，除非我们有大量的元编程，否则我们一般不调用他，通过内置的len等函数进行调用的话，他们的速度更快 下面我们通过最典型的__getitem__和__len__对其有简单的介绍, 并介绍各个魔术方法的使用场景 1.1.2. Se2 using it and show more 最常用也最典型的magic method 不外乎__getitem__和__len__; len即对当前对象提供对于通用的len()方法的接口,通常用于查看对象的length or size getitem除了提供obj[index]的索引方式的同时, 他也会对python内置的那些迭代方法提供支持for i in range(b) 对于依托于这些迭代的方法也能够得以支持from random import choice 切片操作 Se2.1 overwrite operator 这一部分我们主要介绍如下的一些特殊方法,他们将实现+,*,abs,print,bool 对应的特殊方法可以从下面的代码中领会 from math import hypot # hypot will calculate the eud-dis from 0-point(x,y) # we using vector as an example class Vector: def __init__(self, x=0, y=0): self.x = x self.y = y def __repr__(self): # this method -> print return \"Vertor({},{})\".format(self.x, self.y) def __abs__(self): return hypot(self.x, self.y) def __add__(self, other:Vector): x = self.x + other.x y = self.y + other.y return Vector(x,y) def __mul__(self, scalar): return Vertor(self.x*scalar, self.y*scalar) def __bool__(self): return bool(self.x or self.y) 在这里需要注意的是,中缀运算符像add,mul的原则都是不改变操作对象,而是生成一个新的对象. se2.2 show more 这一部分我们会展示更多的一些特殊方法，对这些特殊方法的了解也方便我们对python中方法运行的一些掌握 这些特殊方法的使用场景很容易从命名中可以得到一个基本的认知，像迭代，调用，集合等等的这些属性可能是相对常用的，要熟练进行掌握 例如__str__ | __repr__的方法,就可以方便我们进行print和相应的调试输出. _iadd_ 对应的是+=的这种原地操作, 这个也是我们需要注意的 1.2. Chapter 2 数据结构 python的一个重要特点就在于多种数据结构上的操作通用性, 无论是字符串,列表,字节序列,XML元素,他们都公用一套丰富的操作. 此外在python的函数中, 当我们对一个对象进行就地改动, 那他就应该返回None, 这样可以让调用者知道传入的参数发生了改变, 并未产生新的对象, 这种约定俗成的定义方法, 实际上是一种好的习惯. 迭代, 切片, 排序, 拼接 在这里我们还是要熟悉一下各种类型支持的一些操作, 在python中我们可以通过help(list)查看, 当然最好还是做一个简单的总结熟悉对应的数据结构的一些默认方法. 1.2.1. se 1 序列构成的数组 se 1.1 类型概念 python标准库用C实现了丰富的序列类型, 列举如下. 序列类型 具体类型 类型特点 容器序列 list, tuple, collections.deque 存放不同数据的类型 扁平序列 str, bytes, bytearray, memoryview, array.array 只能容纳一种类型 说明 上面是第一种分类情况, 下面会介绍另一种分类.具体的特点会在下面介绍 可变序列 list, bytearray, array.array, collections.deque, memoryview 支持增删改 不可变序列 tuple, str, bytes 不支持增删改 通过这些类别中共有和非公有的特性，我们对这些序列的不同概念进行理解。 se 1.2 列表推导&生成器表达式 生成器表达式和列表推导式的区别， list是python中重要的一个可变序列类型，列表推导式是我们构建列表的快捷方式，生成器表达式则是创建其他任何类型的序列。 列表推导式： # 这种方式实际上相比于循环来说更加易读 symbols = “&*（@￥” unicode = [ord(symbol) for symbol in symbols] clothes = [(color, size) for color in colors for size in sizes] # 双层循环,写在里面的是外层循环 # 列表推导式 with conditions, 条件语句应该放在后面 res = [num(i) for i in range(10) if i >= 2] # filter和map 也能完成列表推导式做得到的事情，当然lambda也可以 # 具体的map和filter的定义在后面详细解读 unicode = list(filter(lambda c : c>127, map(ord, symbols))) 生成器表达式： 首先理解生成器和推导式的区别，推导式是直接完成整体的构建，而生成器是逐步的餐厨我们需要的元素，需要几个就产生几个，使用yield方法进行创建，而生成器表达式可以在没有该表达式的情况下及时创建简化生成器。 而在编写的时候和列表推导式的区别是，用()代替[]即可 for tshirt in ('{} {}'.format(c,s) for c in colors for s in sizes): print(tshirt) 生成器会逐个产出元素而不会一次性生成，所以实际上这种方法更有内存效率 se 1. 3 循环的本质 和迭代器有重要的关系，有iter方法都能进行迭代，我们可以用hasattr检查 hasattr(str, '__iter__') 循环的后台会发生如下的流程： iter()将对象转换为迭代器对象 next()逐渐获取序列的下一个元素 stopiteration没有要调用的元素引发异常 se 1.4 tuple不止不可变 tuple的不可变属性更应该作为一个record这种信息载体来使用，他的这种不可变的特性，让他的position和对应的存储都显得有意义，我们可以通过_占位*解包等操作，来对我们的数据进行读取和管理。 这里拆包的灵活使用是一个重点，用*ignore来跳过我们不需要的那些元素，也可以通过fmt来灵活的拆出嵌套元素 metro_areas = [ ('Tokyo', 'JP', 36, (x1, x2)), ('Tokyo', 'JP', 36, (x1, x2)), ('Tokyo', 'JP', 36, (x1, x2)) ] # 这种format的编写方式也值得我们学习 fmt = '{:15} | {:9.4f} | {:9.4f}' for name,cc,pop,(lati,longti) in metro_areas: if longtitude 元组本身已经设计得不错, 但是作为记录来用的话, 我们通常需要给字段一个名称, nametuple就为我们解决这个问题: collections.namedtuple用来构建一个带字段名的元组和一个有名字的类,这个有名字的类帮助我们调试. from collections import namedtuple contact = namedtuple('contact', key1, key2, key3) # first dimension is the typename aiken = contact(v1,v2,v3) 该collections.namedtuple除了元组的属性以外还行增了一些特性, 常用的有如下两个 _fields 返回包含所有字段的元组 _asdict() 将具名元组转换为collections.OrderedDict _make() 接受可迭代对象来生成类的一个实例, 也就是可以用*解包的方式来生成一个实例, 而这种方法在元组中是不可用的 name_data = (v1,v2,v3) name = contact._make(*name_data) se 1 5 切片 这一块切片的设计逻辑讲的很好, 为何从零开始且不包括最后一个, 会为我们带来很多操作上和可读性上的便利, 具体看书. slice(start, stop, step)实现切片对象, 或者slice(stop) myslice = slice(1,5,2) arr = list(range(10)) print(arr[myslice]) 可以使用切片对象的方式能够将我们的切片规则参数化, 方便我们去设置和约定. 省略号在python中的表示是..., 这种用来作为多维切片的时候可能会用到 x[i, ...] = x[i, :, :, :] se 1.6 对序列使用+ * 序列使用*n的时候, 通常是建立一个新的对象, 但是如果序列中是对其他可变对象的引用的到时候, 实际上会得到n个引用结果. # 可以通过如下的代码测试 mylist = [['_']] * 3 mylist[1][2] = 0 # print [['_', '_', 0], ['_', '_', 0], ['_', '_', 0]] python的传值类型大多数情况下都是reference assign, 有时候我们对这种对象进行赋值的时候, 我们需要使用深拷贝来进行值的拷贝. +=是否有_iadd_的实现, 有很大的区别, a+b对应的是实现一个新的对象的产生, 我们可以通过id来进行检查. 如果不断没有必要的产生新的对象的话, 会存在比较大的浪费. se 1.7 list.sort 和sorted list.sort 就地修改, 返回None sort(list) 新建一个列表作为返回值, 可接受任何形式的可迭代对象, 最终返回的都是列表. 函数有两个关键词reverse, key: 指定排序的标准. 用bisect来管理已排序的序列: 主要有两个函数bisect和insort, 通过这两个函数使用二分查找法在有序序列中查找或插入元素. se 1.8 其他序列数据结构 列表虽然好用，但也不是万能的，再一些特定的情况下，有特定的数据结构能更好的支持对应的操作。 如果我们要存放大量的数据，这种时候array的效率要高得多（存放的是机器翻译），而吐过我们频繁的进行先进先出的策略deque的速度应该更快； 同样，如果我们需要频繁的检查一个元素是否出现在一个集合中，要考虑使用set（无序） 数组ARRAY： 如果我们只需要一个包含数字的列表，那么array.array会比list更加高效，同样他也支持所有可变序列相关的操作，pop,insert,extend,也提供共文件读取和存储的更快的操作：.frombytes, .tofild array在使用的过程中需要指定存储的类型码(具体使用的时候查阅): floats = array('d', (random() for i in range(10**7))) 存取大数组到array的二进制文件比存储到文本文件中快60倍, 因为这种操作中避免了float函数的执行, 写入同样的也快7倍, 同时占用的空间更少. 同样pickle处理浮点数组的方式几乎和array一样快, 且pickle可以处理各种各样的数字类型, 甚至简单的自定义类型, 是一个快速序列化数字类型的方法. memoryview: 内存视图使得用户在不复制内容的情况下操作同一个数组的不同切片. 实际上是Numpy一些功能的简单实现 numpy & scipy: Numpy实现了多维同质数组和矩阵 Scipy是基于Numpy的科学计算库, 为线性代数,数值积分和统计学设计 collections.deque 线程安全, 可以从两端添加或者删除元素. 常用来作为最近用到的几个元素 queue同步类Queue, LifoQueue, PriorityQueue multiprocessing设计为了进程之间的通信 heapq当作堆和优先队列来用 1.2.2. Se2 字典和集合 泛映射类型 1.3. Chapter 3 把函数视作对象 1.3.1. se 3 函数装饰器和闭包 这一部分不同于之前的 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-07 13:49:51 "},"Langs/Cpp.html":{"url":"Langs/Cpp.html","title":"2 Cpp","keywords":"","body":"1. C++ NoteBook（Cherno）1.1. Introduction About C++1.2. Part 1 编译器基本工作原理1.2.1. 基本信息1.2.2. Build ：Compiler + linker的基本原理1.2.3. Build： 头文件（Header Files）1.2.4. Build：代码存储的文件结构1.2.5. Build: 宏（Macros）2. define: 实际上就是在代码中搜索指定的文本进行替换2.1. Part 2 “变量”的使用和定义2.1.1. 变量（Variables）2.1.2. 数组、多维变量（Array）2.1.3. 字符串（String）2.1.4. 枚举类型（ENUMS）2.1.5. 自动类别指定（Auto Keyword）2.1.6. 模板（Templates）2.1.7. 操作符与操作符重载（Operators and operator overloading）2.1.8. 命名空间（Namespace）2.1.9. 左值与右值（lvalue and rvalue）2.1.10. 移动语义（move semantics）2.1.11. 可选数据（Optional Data）（new in C++17）2.1.12. 多类型变量（MultiType Variable）（new in C++ 17）2.1.13. 任意类型数据的存储（store any type）（new in C++17）2.1.14. 多维矩阵（2D+Array）2.1.15. 隐式转换和显式转换（Implicit Conversion and the Explicit Keyword）2.1.16. 类型转换（Type Casting）2.1.17. 类型双关（Type Punning）2.2. Part 3 Poniter & References指针与引用2.2.1. 指针基础（pointer）2.2.2. 引用基础（references）2.2.3. 指针的->操作符(Arrow Operator )2.2.4. 函数指针（Function Pointer）2.2.5. 智能指针（Smart Pointer）2.3. Part 4 Class & Struct 面向对象2.3.1. 类（Class）2.3.2. “This” 指针2.3.3. 复制构造函数以及浅拷贝深拷贝2.3.4. 单例（singleton）2.3.5. 结构体（Structure）2.3.6. 公用体（Unions）2.3.7. 结构体绑定（Structured binding）2.4. Part 5 WorkFlow&逻辑控制2.4.1. Func： 基本的函数定义（Functions）2.4.2. Func： 匿名函数（Lambdas）2.4.3. Func：三元运算符（Ternary Operator）2.4.4. Func: 多值输出 (Multiple Return)2.4.5. Threads：线程操作2.4.6. Threads：多线程管理2.4.7. Benchmark：基准测试2.4.8. Switch：case 分支2.4.9. Workflow：Conditions and Branches 条件和分支2.4.10. Loops: For and While 循环定义2.4.11. Workflow: Control Flow (contiune, break , return) 循环控制2.4.12. Workflow： Iterators迭代器2.4.13. Workflow: Continuous Integration CI （持续集成）2.4.14. Workflow: Static Analysis(静态代码分析)2.5. Part 6 Memory 资源管理2.5.1. Stack vs Heap: C++中的内存栈与堆2.5.2. New：Keyword For Mem内存关键词2.5.3. Safety：使用智能指针的情景2.5.4. Track Memory Allocation：内存申请跟踪2.6. Part 7 How to make C++ run Faster2.6.1. run string faster 优化string的运行速度2.7. Somthing Else 无题2.7.1. Argument Evaluation Order 参数输入顺序（面试？）1. C++ NoteBook（Cherno） @Aiken 2020 this notebook is based on Cherno‘s Video Class in YouTube； if there is sth get confused，I can recheck the video which talk about it, or just google it. this is not totally for newbie, so some basic information we should search it And this is a important websize to tell us basic info about C++. ToDo: using c++ and Python to finish the leetcode. review data structure when we code. reorganize the notebook by onenote and leetcode. 后续可能会添加Effetive C++中的相关内容 C++ Switch语句 Attention： 为了防止一些遗漏，或者搜索上的困难，虽然我们会尽量避免内容的overlap，但是如果和两者都特别相关的话，可能会在两个地方出现完全一致的内容。 后续补充：如果是confused的内容（不确信的话）就加入:question: 或者 still in puzzle: 内容的格式添加把 Typora常用快捷键 1.1. Introduction About C++ C++是一种强类型的语言，也就是我们需要实现指定数据的类型，同时我们没法随意的改变类型或者混杂类型把。（其实也是可以的就是需要执行特定的代码，相对而言没有那么方便而已。） 有疑惑就去查cppreference，永远的神； 此外当我们对某个不知道什么时候用的时候就google xxx when why how 1.2. Part 1 编译器基本工作原理 This Chapter 主要介绍Visual Studio中C++的compiler，linker的基本工作原理；以及在Visual Studio中一些相关工作环境的设置，比如输入输出配置，debugging环境之类的；此外建议使用VsCode的键盘映射，对于自己来说比较熟悉）；同时也会介绍一些和编译器原理相关的预处理模块；以及Library 1.2.1. 基本信息 STD：standard library C++中的标准库，包含了一些最基础的标准操作，包括cin; cout; #\\:hash tag（预处理符号）+预处理语句（基本的就是include，define之类的） = 预处理器 1.2.2. Build ：Compiler + linker的基本原理 编译和连接的规则： 原则1：.Cpp都会被编译，但是.h不会被编译，他是以#后接的指定模式(include被嵌入(copy)到指定的.Cpp中的指定位置再进行编译。（下面是一些预处理指令） include：直接复制粘贴到指定的位置，所以你也可以定义}之类的进入h。 define：搜索并替换 可以用做简单的函数定义 也可以使用成定义pi的值之类的，指定一个名称 #if 0/1 … #endif:中间的内容将根据if后的true or false 来决定是否存在。（是否编译） 上述的三种操作都是文本级别的操作，也就是针对编码文本进行处理后再送入编译器进行编译的。 原则2：每个cpp都会被编译成.obj文件然后由Linker，将这些obj连接起来成为一个.exe THIS IS IMPORTANT：如果我们想要将功能和主题main（entrypoint）分离开来，除了用header的方式，我们也可以写在另外的.cpp中，然后再主要文件中进行declaration（定义函数名和指定的传入参数即可），也就是声明该函数是存在的， 而不用具体定义（具体定义在另外的cpp中）， 这样在build的过程中，linker就会在我们的工程项目文件夹中搜索其他cpp中的指定function （we just declaration in the main cpp）。这样也能成功的编译。 所以如果我们一个函数在多个cpp中定义（多重定义），或者由于header中的include h，这样可能会导致compile的时候出现代码（链接？）混淆的问题（bug），再不济也是个冗余的编译操作。 如果需要多次使用.h 我们可以将其中的函数定义成static的方式，这样在每个cpp中都会有自己版本的.h中的函数，就不会有重复编写导致模棱两可的问题了。 inline前缀也能解决同样的问题：内联函数 inline：内联标识符适用于结构简单的小型函数。 增加了 inline 关键字的函数称为“内联函数”。内联函数和普通函数的区别在于：当编译器处理调用内联函数的语句时，不会将该语句编译成函数调用的指令，而是直接将整个函数体的代码插人调用语句处，就像整个函数体在调用处被重写了一遍一样。 所以在这里我们推荐在header中只实现Declaration，而具体的Definition就只在某一个cpp中进行编写。 机器码：Visual Studio中可以将输出的obj之类的，包括代码中转化成汇编代码去看，就能知道我器的实际运行逻辑，在VisualStudio中我们也可以设置针对汇编的自动优化来提升算法的运行速度。（一般是在release中会自动优化，而Debug中便于定位问题就没有优化） Debug标识符： C：Compile Error；LINK：Linker Error Static标识符：在变量的部分细讲，实际上对于编译过程也是一个很重要的关键词 1.2.3. Build： 头文件（Header Files） 一般在header 中写入declaration，然后把definition写在Cpp files里面。 #Include 命令 \"\" 或 <> \"[content]\" : content是文件的相对路径，可以使用类似..去索引 <>:一般用来索引标准库之类的，用\"\"来存储.h之类的文件 有.h后缀的导入一般是C，Cpp的就没有后缀 #Pragma once 这个hash tag的作用是让预处理仅仅只编译头文件一次，就是多次import也不会重复编译把，这个东西不要删除它。 原理上是取代了原本的ifnder；实际上也是一个宏 #ifndef Tag1_H void functionA(int var1); #endif 基本概念：使用通用的header文件，将一些include放入header中，然后对这些header进行预编译，生成二进制文件。这样的话， 我们就不需要每个cpp中的#include .h 都对其中的所有的include在进行copy paste然后进行编译，这样的话，加入了很多不必要的编译过程。 也不用每次修改代码进行编译测试的时候，都有繁重的编译工作要去做了。 使用情景： 在大型工程或者文件的时候使用头文件的预编译器是非常重要的。用它 对于一些通用的常用的操作或者文件可以放进去，但是频繁更改的那些内容就不要放进去了，每次进行重新的预编译是浪费时间的 。 主要是一些include，declaration就别放在这了把？ 预编译的header可以对project中的所有cpp负责吗？还是需要include 但是那些特定的依赖项，对于专门操作或者环境的，我们还是放到特定的cpp中，这样会使得代码更加易读，也不会增加负担。实际上就是两方面的考量，特定的和通用的两种处理方式。实际上取决于依赖程度； Visual Studio中的使用方法 创建需要预编译的头文件pch.h;创建pch.cpp 包含#include \"pch.h\" 右键pch.cpp属性：c/c++ ->预编译头->预编译头（使用） 右键项目属性： c/c++ ->预编译头->预编译头（使用/create） c/c++ ->预编译头->预编译头文件（pch.h） 试一下把，不对的话，再回来检查视频。（可以在vscode的设置里，project c++中设置编译时间输出） 1.2.4. Build：代码存储的文件结构 补充说明：VS侧栏的文件夹实际上只是分组，不是真实文件夹，所以我们在哪创建h和cpp都一样 Rule1: 推荐在header中只实现Declaration，而具体的Definition就只在某一个cpp中进行编写。 Relu2：在大型或者规范的Project中建议修改Vscode 的文件保存设置，当然我们也可以根据自己的需求去修改。 输出目录：$(SolutionDir)bin$(Platform)$(Configuration)\\ 中间目录：$(SolutionDir)bin\\intermediate$(Platform)$(Configuration)\\ 1.2.5. Build: 宏（Macros） #define的各种用法；实际上就是通过预处理器对某些操作进行宏处理，“我不喜欢过度使用宏，这样可能比较不方便阅读”这点上其实和template是一样的道理。 2. define: 实际上就是在代码中搜索指定的文本进行替换 是一种文本级别的操作 推荐的使用方法： 约定俗称的名称或者表达，或者一些简单的函数（但是实际上为了便于阅读，并不是太推荐，见仁见智把） #define pi=3.14159265:类似的一种约定俗称的value #define LOG(x) std::cout Debug：条件与动作断点 基本操作： 设置breakpoint 内存信息读取：从debug-windows中可以调出各种窗口，从变量名或者&para找到内存地址也可以 条件与动作断点： 当然我们可以在代码中嵌入循环来使用普通断点来实现这些，简单的就是在断点的地方使用右键。主要的优势在于，我们无需暂停我们正在运行的程序，就可以执行这样的Break. Conditional：条件断点就不说了。 Action：就是不需要stop我们的运行，我们添加了以后，就能在执行我们断点的操作的时候，在terminal输出，我们设置的action（简单的来说就是用于监控运行过程中的参数变化） Lib： Using Libraries 外部依赖的使用 Static Linking and Dynamic Linking静态与动态链接库 基本思想：希望在C++中不需要进行Package Manage 也就是不需要自己再去一个个的下载依赖项，也就是希望能pull下来就能用。 以GLFW为例，我们下载的时候可以下载2进制文件也可以下载源代码；下载的时候是x86还是x64与编写的目标代码有关，和OS无关。 动态静态连接的基本概念 静态链接在编译的时候实现，而动态链接是在运行的时候才操作； 静态链接，意味着这个库直接放到可执行文件中（比如说exe） 静态链接在一些情况下会更快，而且我们可以进行各种优化，所以这个作者比较喜欢静态优化。 静态链接可以实现更多的优化操作，实际上是你exe中的一部分 动态链接，在运行的时候加载DLL（动态链接库）之类的。 询问dll载入调用到的函数，静态lib就是一次性把所有的都载入了 它实现在运行过程中，linking一个外部文件，而不是可执行文件的一部分。 当然也可以在你启动电脑的时候就启动，也可以设置为require，就是没链接就会完全报错。 静态和动态连接的具体实现 静态链接的具体实现：Head File，Include file的形式 创建dependency文件夹然后->libraries子文件夹(归属关系) Copy include 和相关vs版本的lib文件夹进去 如果我们使用静态链接就和lib有关，如果我们使用动态链接就和DLL有关。 在VS 设置中c++ general +额外包含文件夹+相对文件夹（${SolutionDir} ->include文件夹）的路径 如果需要我们也能在这看到其他的指代路径的意思 然后include (其实使用双引号也没问题) 这个头文件实际上支持动态链接和静态链接（include中的） 这里提供了declaration，但是没有实际的definition，所以我们还需要 VS设置Linker/general/ Addition Libraries设置相对路径(Lib-vsxxx)和刚刚类似 VS设置Linker/Input/Additional Dependencies设置具体lib文件的地址 Over 第二种静态链接的方法(不推荐) 在前面的库设置好了以后，我们也可以不include，但是我们要声明（declaration）这个函数存在，注意类型不能错（这就好麻烦） 需要注意的是，我们链接的有的函数实际上是c而不是c++，这种时候就要添加前缀： Extern \"C\" 动态链接的具体实现：基于静态的实现来分析 dll和dll.liB需要同时使用，lib提供了一堆指向dll文件中函数的指针，两者直接相关。 也就是相对于静态链接改变dependency中lib文件为dll.lib 然后要把dll放在我们需要运行的exe的同一个文件夹下。 Lib：Multiple Project 编写自己的Lib或Dll 在解决方案Title下可以添加项目，一个解决方案下多个project 将主project的属性中配置类型设置为exe 将依赖文件设置为lib 或者dll，所有的配置和平台 同样的在h中下写declaration在cpp中definition，然后在include的时候，由于路径不在一个文件夹下，所以我们可以用相对路径的方式设置，但是这样就比较傻，正确的做法：&#x1F447; 属性，c++通用，额外的包含目录，把该目录放进去就行了 但是这样其实我们没link，所以我们可以通过再主项目中右键添加引用，把我们要的引用添加进去，就可以了。（当然我们也可以生成lib再用） 但是这种情况好像只适用于同一个项目文件夹的时候。 这种自动处理同时会自动执行engineer的build 在这里使用了namespace的写法，也许就是和类中的函数是特别像的把，也就是std那样的双冒号 Lib：Timing 计时器 C++中的计时器功能：主要用于统计程序运行时间和控制进程等等的操作。 具体还有timer的cast还有一些单位转化的工作，去cpp中查找或者看benchmark那课的视频 主要使用的库：#include // 还有一个暂时不知道起什么作用的namespace using namespace std::literals::chrono_literals; // 获取当前的时间 auto start = std::chrono::high_resolution_clock::now(); // 获取时间间隔的方式，这里的type比较特别 std::chrono::duration duration = end - start; 上面这个方式,如果每次都要调用的话，就写得比较麻烦，如果我们希望能够比较简单的得到比如某个function运行的时间，我们可以利用lifetime签署一个类别，在function开始的时候定义一个就可以了。 看下面的实例： struct Timer { std::chrono::time_point start, end; std::chrono::duration duration; Timer() { start = std::chrono::high_resolution_clock::now(); } ~Timer() { end = std::chrono::high_resolution_clock::now(); duration = end -start; // std::endl 实际上比较慢 std::cout Lib：Sort排序iterator std::sort need to include ；这是c++标准库中一个对Iterator进行排序的库。 复杂度 O(N·log(N))，其中N=std::distance(first,last) 参考网站：https://zh.cppreference.com/w/cpp/algorithm/sort //简单实例 #include #include #include //其中有一些分类的标准可以调用 int main(){ std::verctor values = {1,3,4,5,2,6} std::sort(values.bagin(),values.end()); //空载或者是基本的用法 //如果我们试图自定义函数的话 std::sort(values.bagin(),values.end(),[](int a, int b){ if (a==1) return False; //1放到其他所有的后面 if (b==1) return true; // 同上 return a 2.1. Part 2 “变量”的使用和定义 In this section we‘ll introduce variables（data structure） in C++，主要是变量的声明和使用方式和用途，生存周期，存储空间，各种Keyword，类型转换。 2.1.1. 变量（Variables） As we know all datatype in machine is different number 实际上就是给机器指定存储的空间和解析的类型。 Char 实际上只是内联了“数字到字符的转换”，所以我们可以用各种类型来输入字符或数字（主要是内存空间占用），但是最后表达的类型会根据我们预定义的类型相关，有内联的数字和字符的转换存在。 基本的数据类型 实际上就是预先定义了内存的分配了表达的类型，大小实际上取决于编译器。 可以用sizeof()查看各种类型占用的内存空间大小 Keyword： 整型： ​ char（1 byte）；shot；int（4 byte）；Long；Long long（8 btyte） ​ unsigned 非整型： ​ Float（4 byte）；double（8 byte） ​ 实际上是精度类型，比如再数据后面+f指定精度类型 BOOL： ​ bool （true， false == 0）；!0 即True Void： ​ 类型未指定 特殊类型 指针类型（Pointers）：再类型后面+ *: int* variables 引用类型和取值符号（reference）： 引用： 在类型后面+&:int& refer 取址：int* a = &variable; 变量的作用域（{}）和生命周期 作用域：在哪个范围内能访问到该变量 生命周期： 在内存中存在的范围（stack变量一般是活不过}） 需要跨越作用域的生存的话通常需要指定存储heap object“对象”的生命周期 （不使用New关键词的时候）生命周期只到栈或者说是大括号内（可以使用空的大括号组），是存在的 （使用new关键词的时候）如果我们不delete它，就只能在程序终止的时候才退出了。 :x: 下面这是一种十分错误的写法 int* CreateArray() { int array[50]; return array; } int main() { int array[50]; CreateArray(array); } //完全错误，在函数结束的时候这个指针会被完全销毁，所以指向的地址是没有值的， 如果我们希望延长声明周期，我们可以将数据分配到heap上，或者通过传入指针，对指针调用的地址的值进行修改。 编写一个会自己销毁的在heap上的指针（实际上和智能指针又异曲同工之妙） class ScopedPtr { // 该ScopedPtr的申明是存在stack上的，所以销毁的时候调用delete就直接。 private: Entity* m_ptr; public: ScopedPtr(Entity* ptr) :m_ptr(ptr){} ~ScopedPtr（） { delete m_ptr; } } int main() { { ScopedPtr e = new Entity(); //申明周期也只到内部的大括号，出不去。会顺便把实例也销毁了。这个还稍微有点疑问。:question } } const 常量修饰符 C++ const 关键字小结 | 菜鸟教程 (runoob.com) 语义含义：不可变，不可修改； 可以令值不可变，也可以令指针不可变；定义一些常量之类的东西 可以在函数传入值中定义，使得传入值不可被修改，或者防止传入的指针或者是引用被不正确的修改导致一些奇怪的问题。 要注意根据位置的不同是指针不可修改还是值不可修改的含义是不一样的。 本质含义：实际上是一种代码的可见性机制，只是个promise，用来简化我们的代码。所以我们应该遵守它：就是我们不去修改这个const。（避免使用强制类型转换去修改它） 几种基本的定义方式： 这种形式指的是我们没法修改地址的值，但是我们可以改变指针所指向的地址。 //这两种都是一样的，在指针*的前面 const int* a = new int ; int const* a = new int ; 这种形式指的是无法修改指针引用的那个地址，但是可以随意的修改值。 int* const a = new int 复用就是都不能改 在类中的public函数declaration的括号后面加 const，指的是我们没法在类内函数中修改类内的private的值。这样的操作会被定义为illegal。 类内指针的话就有意思了，要有三个const全用 :question:这一点后面慢慢补充，没搞清楚来着，记得太模糊了。 const Entity& e 指的是我们没法修改指向的地址。 实例说明： 对于传入函数的Instance（Entity）也是一样的，如果我们不希望进行内存上的copy，我们就加上&，如果我们不希望改变值就加入const。 用一些例子来说明一些其他情况的Const用法。 需要注意！ class Entity { private: int m_x, m_y; public: int GetX() const //需要保证函数不能修改类内的private的值 { return m_x; } } // 但是还是有一些问题 在这种情况下的const最前面的就是指的指针的地址不能变，但是其中的值能变。那么在这个时候，涉及到实体的那些函数，如果我们再类中没有加入const后缀的话，那么我们就不能使用这个get函数，因为编译器不能确保他不修改我们不能动的那些数据。 所以有时候会出现一个+const的定义和一个不加const的定义。 如果我们对一些private需要指定在一些const后缀的情况下可以改变，那么我们可以再声明的时候加入mutable前缀：允许的可变。 void PrintEntity(const Entity& e) { std::cout Mutable可变标识符 用途1：基本上对于一些private类，在有const后缀的情况下我们又希望修改其中的某个值，才会用到这个关键词。 class A { private: std::string m_name; mutable int m_DebugCount = 0; public: // 结合上面的范例可以看出这个返回类型为啥回事这个const+& const std::string& GetName() const { m_DebugCount++; return m_name; } } 用途2：使用lambda的时候。我们希望能够修改传入值本身，（但是lambda是不允许修改的所以需要） int x = 8; auto f = [=]() mutable { x++; std::cout Static静态标识符 静态标识符类型： class or struct 内部，外部，function 内 类外的static： 对于Linker起作用的修饰符，表示为局部的，也就是只对它本身的obj起作用，不会和别的文件连接起来，只对本身可见。 如果在function的前面添加Static 就表示该Function 只被该.cpp调用，也不会考虑外部的Link，所以当我们如果希望被外部调用的时候就不能加static标识符，在要调用的地方进行declaration. 实际上全局的参数定义，对于Linker来说也可以是跨文件的，所以如果我们定义全局参数的话，我们要考虑是否是通用的（需要重复include的常量），就可只在cpp定义一次。同时我们要在引用（另一个文件）的时候添加修饰符。（比如在头文件中用Extern Declaration参数） Extern（也是用于声明：在外部已经定义过了，定义参数独有的） https://www.cnblogs.com/lulululu/p/3693865.html 类内的static： 这部分*memory对于这个类别的所有实例是共享的，换句话说也就是，无论你定义了多少个instances，这个变量或者方法也是唯一的，对于所有的类别是通用的。你改变了一个，也就改变了所有。 所以，类内的静态方法可以在没有类实例的情况下被调用，而且在这种静态方法中不能refer to 一个具体的类实例。 也不能通过实例对静态变量进行调用处理，这样Linker会找不到我们实例对应的变量，因为那几个变量是类的变量而不是实例的变量。那么怎么去调用或者修改呢。 使用作用域(type ClassName:: x)的方式去声明,同样也以这样的方式去调用（ClassName（同样也能类比成namespace）:: x）才是真正正确的使用方法，类内的静态函数也是这样定义和操作的。但是不用像参数一样在类外声明 Function内的Static： 其实和类内的Static是一样的，当我们第一次调用这个function的时候就会存储这样一个对于所有的function的静态变量，后续调用的时候这个参数就不会被重新创建了。 也就是说这种参数对于函数来说是恒定的，在该参数上进行的变化会被继承下来，也就是会被迭代运作。 某种程度上来说也算是延长了参数的生命周期，通常来说需要&的return值的一些情况下就需要用static关键词定义的vars。(:question:这一点是啥意思来着） 2.1.2. 数组、多维变量（Array） 经典数组 数组实际上是一组连续的变量，在内存上存储指定长度的空间，本质是指针 定义方式：type var[n],定义n个连续的type内存空间，这里的var实际上是相应的指针 int example [5]; int* ptr = example; 用var[idx]去索引指针相应地址的值：（实际上就是在初始地址上加上相应的偏移） 默认在stack上：没有new就是在stack上，需要heap就＋new把 记得delete[] 同时也可以用New关键字去声明数组，同时这个数组就会被存储到Heap中，这样的话该数组的声明周期就能活过大括号了，需要我们手动调用delete命令去删除它，由于定义的形式是数组所以在delete的时候记得使用的是delete[] var 命令 int* another = new int[5]; for (int i ; i 和传统的array好像有一定的同质性；找到数组长度的方法： int count = sizeof(a) / sizeof(int); // 用std::array的会自己保存数据的array 静态数组（std::array） 数组的长度或者大小没办法自动（动态）改变的，我们应该用这种方式来代替传统的定义方式，有很多好处。 https://blog.csdn.net/thinkerleo1997/article/details/80415059 https://blog.csdn.net/fengbingchun/article/details/72809699 #include std::array data; 什么时候我们应该用这种array来取代传统的int array？ 现在这种方式有很多的集成函数：比如说size，sort，began好像还有iterator之类的方法。 因为传统的使用New，关键词是slow的，这种方法也会快一点，而且长度是不知道的。 verctor是heap上的，而array和传统的int，array都是存在在stack上的，（非new关键词） 有很多优化，同时这种方法有自动的边界检测？ 在函数传入array的时候，建议可以使用template的方法。 BTW：快不是标准库（STD）的基本目的和最求，所以很多时候需要资源最大化利用的情况下，很多project都会编写自己的数据类型：（可以从Cherno的最后两课去看看） 动态数组（std::vector） 更详细的一些操作指南可以google或者看HR的vector.svg(附件) 就是个不指定大小的Array，可以改变数组的大小，其实就是自动执行内存的重新分配（内存动态分配），牺牲性能来得到更好的便捷性。 虽然C++命名Vector，但是实际上是个动态的ArrayList，而不是向量。 vector移动数据而不是Copy的方式在很大程度上提升了效率（没超过默认大小的情况下），但是在超过了指定大小的时候，还是需要用到copying（内存的动态分配）这就不是一个非常理想的情况。 Vector：当append超过了现有的容量，找到一个足够大的内存位置，然后把原本的参数copy迁移过去，然后加上我们要添加的参数，然后删除原本占用的内存空间。 这样就会造成运行缓慢，那么我们如何避免这样的copy操作， https://blog.csdn.net/theonegis/article/details/50533770 基本的声明方式 #include std::vectorname; //type 也可以是自己定义的class；实际上存储的就是数据的内存顶点（起始点） // using vector = std::vector以后 vector a; vector b(a); //声明容器b, 用容器a初始化b vector b(a.begin(),a.begin()+3); //用0-2个元素来初始化 vector a(num); // vector a (num,value); 基本的一些method Push_back({v1,v2,v3}) 就相当于append；size（）获取长度；clear（）将长度设置为0 索引还是[]； eraser（）：需要在括号中设置一个迭代器，比如我们需要移除第二个参数 ​ ↑：vertices.begin() +1 使用示例： std::vector vertices; vertices.push_back({1,2,3}); vertices.push_back({4,5,6}); for (int i = 0; i 存储空间 默认应该是在heap上的，但是会自动删除的。 优化vector的使用 issue1：我们会先调用最原始的构造函数，在main function的栈中构造一个vectex，然后copy it to Vertor类所在的空间中，如何直接在指定的地方添加（或者说只进行一次构造）呢？ 解决方法：用emplace_back取代push_back,直接传入构造对象需要的参数即可 vetrices.emplace_back(1,2,3); vetrices.push_back(vetrices(4,5,6)); issue2:空间超过以后进行了复制和迁移操作，也就是我们每一次添加都需要把原本的空间进行resize（copy and move） 解决方法：直接在开始的时候指定可能的最大size，就是给定一个预留空间 std::vector Vertices; Vertices.reserve(3); 2.1.3. 字符串（String） String - stream of text 一组字符串 == array of characters 一些额外的信息：字符串相关的一些其他事项 实际上\"\"定义的就是固定（const）的type为char的指针，也就是 const char*；换句话说也就是占据固有长度的char array 实际上这是c中编写string的风格，为了熟悉基本的原理才这么编写的，现在就直接用string库了 原本定义的时候还需要在末尾指定ascii码终止符，但是在新的版本中不需要特地指定，也就是下述的两者是一样的。 char name2[7] = { 'A','i','k','e','n',0}; char name2[7] = { 'A','i','k','e','n' }; 基本信息: 使用string的基本注意事项 基本的定义方式也就是和int之类的没区别 string类别就可以使用find size append +=之类的操作了，所以拥抱string； 官方的参考链接地址（看看其中的function的作用）： \"\"定义的类别实际上就是const char[]，本身就是一个不可修改的指针了，各种意义上的不可修改 ''定义的才是像普通的123这样的char value string实际上也是std中的一部分: 同时include它包含了重载 +=不能再“”中执行，但是可以在string和“”中进行。 将string 传入函数的时候，也是不修改原值的，所以我们还是使用&的传入参数设置去使得不需要赋值一个新的，同时如果不希望修改的话，就在最前面加入const String Literals 实际上我们在上面说的“”定义的就是String Literals，字符常量，这种方式定义的后面会自带一个休止符的位置，再内存中也就是00；我们也可以手动定义休止符\\0 实际上我们修改string都是再内存中获取一个Copy去进行的。所以我们需要善用&符号 一些函数： Strlen（）:返回string长度，char array，要注意自己手写休止符的特殊情况 基本的“”实际上也是utf-8类型的。 const char* name = u8\"Aiken\"; //1 byte const wchar* name2 = L\"Aiken\"; // (2/4) byte const char16_t* name3 = u\"Aiken\"; // 2 byte const char32_t* name4 = U\"Aiken\"; //4 byte 可以使用 using namespace std::string_literals使得我们对 “” -> string的类型转换可以从 std::string(\"\")变成只需要\"\"s R\"\" 也很有用，:question:但是我忘了这个是用来干嘛的了 2.1.4. 枚举类型（ENUMS） 枚举类型：也就是set of value 根据第一个var = x; 后面每一项的默认值在前一项的基础上+1，自动匹配对应的value 本质上就是一组指定的别名和其对应的value class example { public: enum Level: unsigned char { L_error = 0,L_Warning,L_info }; //取代了下面这种表达其实 /* const int L_error = 0; const int L_warning = 1; const int L_info = 2; */ private: Level m_logLevel =L_info; } 2.1.5. 自动类别指定（Auto Keyword） 可以结合模板来使用 实际上就是根据我们键入的等式右边的内容，自动指定int float or any other type。 但是作者不是很推荐这种用途，很简单的情况，就很没必要，而且不利于阅读和维护auto a = 4; 比较推荐的用法： 调用函数时的返回值前缀： 在这种情况下我们修改function（API）的return type的时候就不需要重新在修复赋值的定义了，这种时候auto还是很有用的，或者返回类型不明确的时候。 // 比如function的type 我们可能经常会动，或者有几个相似的函数的情况 char* GetName() { return \"aiken\" } int main() { auto name = Getname(); } 缺点：但是这种方式的话实际上也会让我们是否修改了代码的指示比较不明确，比如有时候会发生隐式转换。 替代特别长的数据类型： 比如使用像vector这种的时候，参数名实在是太长了，就你懂的了，推荐使用（当然这种可以用using 来取代也是一样的。） // 使用 using的话就是。 using DeviceMap = std::unordered_map>; DeviceManager dm; // 话说下面这个应该是 值不可被修改的别名（因为&本来就无法修改地址，所以只有一种可能把） coust DeviceMap& devices = dm.GetDevices(); 2.1.6. 模板（Templates） 模板可以理解为一种指代，简单的例子就是通过这种模板的定义我们可以定义一个类别通用的函数； 模板实际上是在编译的时候就实现的一种机制，而不是到了具体的运行的时候才实现的。 简单使用场景： 当我们需要多种类型的输入，来进行相似的function操作，比如说Print的时候，这种时候我们定义一种TypeName的模板；然后我们就可以使用如下方式调用定义的函数了。 template void Print(T value) { std::cout(5); //其他类型的也可以，只要函数内部支持就行。 除了不确定的类别，我们也可以针对不确定的size去做（其实这里的int类型可以改成size_t） template class Array { private: T m_array[N]; public: int Getsize() const {return N;} }; //main Array array; 但是不要过度使用，因为可读性会比较差；但是更关键的是，模板在编译和执行的时候是两回事，这样会让我们很难定位问题。所以不要乱用。但是在编写loging的system之类的地方，就比较合适。 2.1.7. 操作符与操作符重载（Operators and operator overloading） 更多的表现是符号而不是函数，new ， + - ,之类的都是 括号 与或非： || && ！ 一些优先级设定： ++之类的运算符号的优先级> 取值，所以我们要*加入括号，使其首先解引用，防止改变的是地址的值而不是value。 重载的用途： 比如我们实现向量类别的时候，我们就可以重载+，来实现这个加号，就是不用写一个Add函数（麻烦），主要是比较大规模的情况下为了使用方便来写的吧。 Example： Vector2 Add(const Vector2& other) const { return Vector2(x + other.x,); } Vector2 operator+(const Vector2& other) const { return Add(other); } 为了更方便的cout重载 std::ostream& operator 对bool的判断进行重载： bool operator==(const Vector2& other) const { return x==other.x && y== other.y } bool operator!=(const Vector2& other) const { return !(*this == other); } 重载迭代器的索引 char& operator[](unsigned int index) { return m_buffer[index]; } 2.1.8. 命名空间（Namespace） Using namespace apple (导入apple中所有的定义) Using apple：：func1（只导入func1） Namespace a = 定义 类似的查看cppreference网页即可 2.1.9. 左值与右值（lvalue and rvalue） 其实就是赋值等式左边的变量和右边的常量的关系把？在这部分还会讲到相应的reference。 rvalue就是 i=10中的10，这种值不能被更改，是一种临时的变量值，没有location和space，我们会将它分配到i，也就是左值。才是可分配和可改变的。实际上就是这样的。 rvalue可以用来创建lvalue，lvalue才有reference，但是有特殊规则； int& a =10 // error,这样写是错的，rvalue没有直接的引用 const int& a =10; //right 了解这点的意义在于（为啥我们要使用const） std:string fName = \"H\"; std:string Lname = \"Aiken\"; std:string Fullname = \"Aiken\" + \"H\"; // 这个等式左边的全lvalue, 等式右边的（整体）都是rvalue，于是我们调用下面函数的时候，无法写输入值为\"Aiken\" + \"H\"，因为rvalue没有& void PrintFuc(std:string& name){} // 我们稍微修改一下，就能得到一个通用的Print,能够对临时变量rvalue进行传入。 void PrintFuc(const std:string& name){} 假如我们需要一个只能传入rvalue的函数，那么我们可以将表达修改为，这是一种特殊的方式 void PrintFuc(std:string&& name){} 这种方式有什么用呢？对于下面的移动语义很有用，因为临时的变量不需要考虑livetime或者memory之类的东西，同时我们可以简单的获取其中的值，不用担心他和很多其他的地方产生关联。 2.1.10. 移动语义（move semantics） Question：为什么&引用符号不能解决这个问题，好像是对传入object的情况进行处理的一种方法：就像string，我们在某个地方需要她的时候，我们可能需要重新构造它。 Ans：移动语义针对的对象实际上是Rvalue，也就是临时变量，临时变量的生命周期短，特别是像“”到string的情况，也是需要申请空间的，这样在我们将rvalue传入function或者class的时候，就会发生一次不必要的copy（因为rvalue没有&所以你懂的） Move Semantic： move objects around 为了避免类似的不必要copy的操作，我们就使用移动语义的编程思想来做。&#x1F447; move就是获取原本存储地址的指针，然后再将原本的指针赋值为nullptr（这样会使得其自动调用析构函数，也就不会有泄露等错误了）同时将size设置为0。 相比较于深拷贝实际上就是一种浅拷贝的操作。 具体的实现思路其实就是针对rvalue重构copy constructor //copy constructor 如下,以class string为例 //基础类别的部分 string(const String& other) { printf(\"Copird!\"); m_size = other.size; m_data = new_char[m_size]; memcpy(m_data, other.m_data,other.m_size); } //重构rvalue情况下的.. string(const String&& other) noexpect { printf(\"Move!\"); m_size = other.size; m_data = other.m_data; // 清除原本的指针 other.size = 0; other.m_data = nullptr; } // Entity部分，要添加一个针对rvalue的构造函数 Class Entity { public： Entity(const String& name) :m_name(name) {} // 这里注意要手动把转换写出来，不然还是会进行copy的情况(执行上面那个),move 或者(string&&) Entity(const String&& name) :m_name(std::move(name)) {} private： string m_name; } // main 部分 Entity entity(\"aiken\"); entity.print()// .... std::move()左右值转换 参考资料： 详细解析；Anthor one 2.1.11. 可选数据（Optional Data）（new in C++17） 基本设置：项目，设置，C++，语言，c++语言标准>17 针对那些我们可能会使用也可能不使用的可选数据。这也是一个c++ 17的内容：https://zh.cppreference.com/w/cpp/utility/optional 传统的就是通过引用传入一个bool flag，然后通过flag去判断是否存在之类的。但是有了optional我们就可以如下的方式去做 #include #include std::optional readfile(cosnt std::string& filepath) { std::ifstream stream(filepath); if(stream){return string1} return {}; //这种写法其实是空tuple？还是要学一下的。 } auto data = readfile(); //或者写那一长串optional string //然后就可以使用 if(data) 或者 if(data.have_value()) 还有另一种使用方式就是，用于设定不存在数据的默认值。 std::string value = data.value_or(\"sdsds\"); //如果data是空的救会取到这个，相当于默认值把。 //比如 std::optional count; int c = count.value_or(100); 2.1.12. 多类型变量（MultiType Variable）（new in C++ 17） 换句话说就是数据的类型是在指定的范围内可选的，依托于#include通过指定数据可能的Type，然后用特殊的方式取出来。 它实际上存储的是所有类型的长度相加的空间；虽然能实现和unions类似的功能，但是实际上是更安全的。就是存储空间的占用更大？两者相比推荐这个把。 std::get_if：对于这类型的数据很经常被拿来使用，我们可以参考这里的用法 std::variant data; data = \"Aiken\"; // 这种数值取出的方式只有在类型正确的时候才会起作用，不然会造成exception std::cout(data)(data)(&data); // and we could use it like that if(auto value = std::get_if(&data)) {} 针对Get_if的具体实例可以写成这样： #include #include int main() { std::variant v{12}; if(auto pval = std::get_if(&v)) std::cout 此外我们也可以通过这种类型来定义函数：这种的使用方式可以像这样看（对于可选的话，只是返回一个空值，这样可以更加具体的定义我们为什么访问不到文件），但是这种方式的话，不能用auto来代替吗 //因为我们要当成类型传入，所以需要class但是这里不用return值吗 enum class ErrorCode { None = 0, NotFound = 1, NoAccess =2 }; std:: variant ReadFileAsString () { return {}; //这里应该也是需要修改的把，改成ErrorCode类型 } 2.1.13. 任意类型数据的存储（store any type）（new in C++17） 我们也可以使用空指针来存储任意类型的数据；但是这不是这一块讨论的内容，这里讨论的是std::any 就是一个能存储任意类型的variable，实际上和variant很像，但是那个更安全，因为我们知道所有的可能类型；同时这样的方式也会避免any可能会带来的动态内存分配，这个我们知道是相当影响性能的。 #include std::any data; data = 2; data = \"aiken\"; // variant 指定string的时候实际上会发生const char * 到string的隐式转换，但是any是不会的。 //any 取出变量的方式如下,在这种情况下数据类型不匹配的话，是不会成功取出的，我们还是需要知道我们当前的any type 以及按照指定的方式取出，所以实际上variant是一种更为安全的方式 std::string string = std::any_cast(data); //但是这样的话我们还是会有一个copy的操作，我们是否能够直接返回一种引用&#x1F447;（别名） std::string& string = std::any_cast(data); 使用的情境： 实际上是存储空间是有默认的小规模存储空间和大规模存储空间(限制);超过了小规模（32byte？）的情况下会使用动态的内存分配的机制。 当我们需要用any存储类似struct之类的大数据的时候，any可能就会调用new来动态的内存分配了。 能用variant就用，不行，导致非要用void*（空指针）的情况下就用any；但是最好的话，我们是不需要这种东西的。 如何使用： 2.1.14. 多维矩阵（2D+Array） n维数组实际上就是n-1维数组的堆叠：array的array 也就是其实是指针机制：指针指向的地址存放一组指针，然后这组指针再指向各个Array，这就是2d array了。关键就是 指针的指针** 多维矩阵第一次取出的时候实际上是指针类型，多重取出的最层才是数据。 int main() { // 实际上就是用指针的指针的方式分配多维度的数组，更多维度也是一样的（星星更多了）。 int* array = new int[50]; // 分配50个存放int指针（int是返回的类型不是指针的类型）的空间 int** a2d = new int*[50]; //绑定了50个内存位置 ------------------------------------------------------------------------------------- //实际的定义多维数组的方式,更高维度的就要嵌入更深的循环。 int** a2d = new int*[50]; for(int i=0;i 问题来到了下一步如何删除多维度的数组？ 如果我们只delete最外层的指针，那么内部的所有指针地址，将会发生内存泄漏。 所以：我们需要像定义那样，反向的对每个指针都进行delete for(int i=0;i 这些50*k的存储空间实际上不一定是连续的，但可能是接近的，实际上是，再内存中随机的分配50个buffer来存放50个array，这种方式会越来越慢，（因为缓存的命中问题？），连续的存放可以使得缓存有更高的命中率来提升速度。 再某些情况使用1d array来代替2d array比如下（这不用看了，谁不会啊，手动换算行号呗），但是这样的代码在执行的时候，快很多 int* array = new int[5*5]; //我还以为，是类似的写法也可以，这样的话，谁不会啊。 for (int y=0;y 2.1.15. 隐式转换和显式转换（Implicit Conversion and the Explicit Keyword） 隐式转换和显式关键词 隐式：不用告诉他他究竟要做什么 很多时候由于类的构造函数实现，基于类的输入类型，我们可以将函数表示的初始化，转化为等号的。同时也能在一些特定的场景下执行内置的类型转换。 但是如果是我的话，我尽量不会这么去做&#x1F51C;，因为这样会增加阅读的负担。 显式关键词：不让执行隐式转换 Explicit 加在构造函数的最前方，这就是让构造函数只能被显式调用，不能执行隐式调用。 #include using String = std::string; class Entity { private: String m_name; int m_age; public: Entity(const String& name) :m_name(name){} Entity(int age) :m_age(age){} }; void PrintEntity(const String& name) { // print } int main() { // 在这一步中C++将根据构造函数进行隐式转换，前提是对应的类别要是正确的 Entity a = 22; Entity b = String(\"aiken\"); // 第二种自动进行隐式转换的场景，实际上和上面是完全一致的 printEntity(String('asdad')); std::cin.get(); } cast：类型转换从typeA-> type B int（22）之类的 所有的类名也能这么做（借助隐式调用这样的） 2.1.16. 类型转换（Type Casting） cast实际上会为我们检查类型是否正确等等 显式转换：显式的指定我们希望将当前类型强制转换成什么目标类型 double s = static_cast(value)+ 5.3 隐式转换：不需要我们显式的指定转换的数据类型，根据输入输出会自动转换 // 反过来也是可以的 double value = 5.25; int a = value; // 我们同样也可以显式的指定,但是这种转换不是强制的，只是显式的表达 double b = (int)value;//safe style csat的样式 double b = (int)value +5.2 //我们可以看看没有这个int的结果 实际上存在四种不同的Casting：static_cast，const_cast，dynamic_cast，reinterpret_cast ，这几种cast的使用请 GoogleIt来补充基本的含义，以及一些使用情况，（CPP reference是真的牛逼） 在一些情况下转换失败的话会return null，所以也可以用做派生类的检查。 const：用来从const到非const dynamic：用于从基类到子类的指针转换，（反过来可以，但是实际上不需要显式转换） 其实是一种function，实际上会有一些额外的操作。 如果这个转换是错误的，那么这个指针会返回null，所以实际上，这个指针可以用作类型之间继承关系的验证作用。 // 基本定义 class Entity {}; class Player : public Entity {}; class Enemy : public Entity{}; 实际上由于存储了运行的中间状态（runtime type infomation默认是启用的，关闭会报错），所以是可以知道该类到底是啥的，也就是支持从基类推导到该子类到底是啥。 Player* player = new Player(); Entity* e = player; //毫无问题 Entity* e1 = new Enemy(); Player* p0 = dynamic_cast(e1); //这种转换不可行，会return null Player* p1 = dynamic_cast(e); //从基类转换到子类 ok，但是这种情况下我们需要指定多态，也就是virtual 为了避免冲突，这其实是一个多态的用法 if(dynamic_cast(e1)){} //验证类型的用法 reinterpret:用于无关类型的转换，还是需要再搜索一下，不太常用，推荐的可以使用的情况，类型转换转换回原本类型的时候 这种类型转换实际上是更可靠的。 2.1.17. 类型双关（Type Punning） 实际上就是获取某种类型变量的指针，然后转化成另一种类型的指针的操作。 Google it 当然接下来我们也可以解引用,其他之类的。 首先看一串示例代码（double）实际上是为了让到double的隐式转换更加清晰，并没有真正的操作指令。 // 这样的操作实际上就是a-b的隐式转换，但是这样的话，内存空间实际上是没有公用的，b是用的另外赋值的双精度的5 int a = 5; double b = (double)a; // 反过来也是一样的道理 如果我们想要直接使用指针转换，将double指针转换到int指向的内存地址，这样的话，由于两种类型的长度不同，所以会导致输出错，严重的话还会导致崩溃。 但是实际上，这样的操作，我们可以通过同样长度的内存操作，来直接的对内存进行操作，但是正常人没有人这么干。 struct Entity { int x,y; int* GetPositions(){return &x;} }; int main() { Entity e = {5,8}; int* position = (int*)&e; //将struct的指针转化为int指针 std::cout 2.2. Part 3 Poniter & References指针与引用 从指针的含义出发，对各种不同指针的用法，引用场景，内在含义，进行分析，记录世界记录你。永远的神，指针。 指针实际上就是一个1byte的整型值，它就是一个地址，指示你在内存中存储该值的位置，和类型没有半毛钱关系，类型只是指示你可能放了个啥类型的数据在那个地址Void* ptr = &var; 引用和指针本质上是一回事，用法上会有所区别它实际上是基于指针的一种高级修饰，是对某个已经存在的var的引用。他并不是一个真正的变量。Int& ref = var。 引用能做的指针都能做，实际上是一种代码的优化和简化过程（moew clean），主要的用处就在将var而不是value传入function（达到能够直接修改var的作用） 2.2.1. 指针基础（pointer） ->的访问方式：实际上等同于(*e1).method，Arrow->只能在左边是指针的时候使用，而.调用的方式左边只能是实体。 Entity_virtual e1 = new Entity_virtual(); std::coutGetName() 基本的定义和使用方式： 用*定义一个指针类型（用来存放地址）= &var（用该符号取出后接变量存储所在的内存地址）var 然后我们可以在Debug Stage从windows的memory找到该变量在内存中的值。 实际上也可以用type* name = Value，这样的话name还是指的是value所在的地址。但是这种时候type就需要写好了。 在指针类型变量前加表示我们*访问该地址所存储的Var，我们可以对该var进行读取写入或者修改，但是在这个时候，我们写入的值就和之前所提到的类型有关了（指针本身是无关的） 因为类型会告诉内存，我们写入的数据要在内存中占用多少个字节，多少位之类的信息，而如果我们使用void，那当我们给该指针取到的数据赋值的时候，compiler就不知道怎么存储该数据，也就会导致error。 也可以用**定义指针的指针，也就是指针所在的内存空间的地址 BTW:从内存窗口看到的地址是逆序的 可以将指针定义为nullptr，后续再赋值，而引用必须马上引用一个已经存在的变量，他不是一个新的变量。 在同一行里定义多个指针变量的时候要在每一个前面+*千万别忘了 注意事项： ++之类的运算符号的优先级> 取值，所以我们要*加入括号，使其首先解引用，防止改变的是地址的值而不是value。 “0”不是一个有效的内存地址 指针偏移值实际上取决于指针前面的type：如下图就是加入了两个int长度的地址。 int example[5]; int* ptr = example; for (int i=0; i 2.2.2. 引用基础（references） 用type& ref = var定义一个对var的引用，不需要其他的操作符 实际上ref就是一个别称，他不是实际存在的，只是var的另一种表达形式。 需要立即赋值。 具体的用途除了创建别名方便读写以外：主要用于需要修改原值的参数引用定义上。 function中通常情况下，是传值，而不是传递变量的地址。所以会有额外的内存拷贝的操作发生；所以通过function中的value产生的变化实际上是不会影响传入的变量的，这时候我们需要使用引用将变量传入，而不是值传入。（if we need this operation，也就是我们需要影响原值的时候）那么实现的方法有下面的三种。 要这么做的话实际上就是，我们将内存地址传入，然后通过地址取值进行操作，而不是只将这个值copy一下传进去也就是def fun(int* var); func(&var);当然这种方式也适用于直接传入指针， def fun(int* var); func(&var); 对1进行修正，更优雅的写法，简洁，就是使用reference，接受传入的是别名，也就是具体的变量，而不是值。 def func(type& value); func(var); 当然我们也可以通过return来改变原值，但是这样会需要temp value来影响内存效率之类的东西，也比较傻逼。 无法改变引用的对象。 再func前面类型定义为Type&,那么我们正常的return就是返回一个原值的引用。 2.2.3. 指针的->操作符(Arrow Operator ) →用来取代解引用后取值，就是用于指针直接调用参数或者函数，免去用*解引用的过程。 但是所有的操作符都可以重载，我们可以在自己的类别中定义他：比如当我们用一个Class 装载别的Class的指针的时候（比如我们为了让他能自己delete），如果我们希望能够直接指向最底层的那个Class 的function的时候。 // ScopedPtr存放另一个class（Entity）的指针，和构造析构函数。 // 重载使得直接调用底层类别中的函数。 Entity* operator->() { return m_obj; } ScopedPtr entity = new Entity(); entity->Print(); 获取类中参数的内存偏移量（可能是特殊的用法把，和第一点最基础的代码完全不一样）： struct Vector3 { float x,y,z; } int main() { int offset = (int)&((Vector3*)nullptr)->x; std::count 2.2.4. 函数指针（Function Pointer） 主要目的就是：获取函数所在的内存空间的地址； https://zh.cppreference.com/w/cpp/language/pointer 函数指针的补充资料：link1，Link2，还需要通过编程来加深理解 实际上应该还有其他类型的表达； 定义和使用 记住这里1不加括号（有参数输入的时候才加入括号），这就等同于在HW卡面+& auto function = HelloWorld; //记住这里不加括号（有参数输入的时候才加入括号），这就等同于在HW卡面+& 下面这种定义方式便于我们理解： //等于下面这种方式 void(*cherno)() = HelloWorld; cherno(); // 调用函数。 // 通过4-5的参数对比，我们可以知道cherno就是一个函数的别名。 // 这种方式其实更规范的可以写成 :这里添加了参数的输入所以比较不一样 typedef void(*HelloWorldFunction)(int); HelloWorldFunction function = HelloWorld; function(8) 函数指针的使用场景： 主要用于把function传入function，和lambda匿名函数好像有比较好的结合 void PrintValue(int value) { //Print cout } void ForEach(const std::vector& values, void(*func)(int)) { for (int value : values) func(value); } int main() { std::vector values = {1,2,3,4,5}; ForEach(values, PrintValue); } 2.2.5. 智能指针（Smart Pointer） 非常重要，能用智能指针的情况下我们就不用传统的指针 作用域:{} 参考资料1；参考资料2 唯一指针，能够自动的在作用域外就进行销毁（最基本的智能指针） #include // 下面这个是错误的，给个范例 std::unique_ptr entity = new Entity(); // 这个是错误的！！！！！！！ //只能显式调用构造函数：正确↓ std::unique_ptr entity(new Entity()); //另一种 写法：推荐写法：最好就这么写 std::unique_ptr entity = std::make_unique(); 共享指针，另一种智能指针， 使用reference_count来进行引用指针的计数，对象的所有引用消除了以后（count=0），才进行销毁（delete） 主要功能： 管理动态创建的对象的销毁。它的基本原理就是记录对象被引用的次数，当引用次数为 0 的时候，也就是最后一个指向某对象的共享指针析构的时候，共享指针的析构函数就把指向的内存区域释放掉。就是一个对象可以有多个引用。 std::share_ptr sharedEntity = std::make_shared(); 弱指针weak_ptr: 是一种弱化的共享指针，不会进行reference count https://blog.csdn.net/albertsh/article/details/82286999 它不会等到全部的指针都被销毁了才销毁，它会在指针销毁的时候就对对象进行析构，所以可能会有部分指针指向没有分配值的地址。 总结一下：重要！！！ 所以只是当我们需要在一个heap上声明的对象，但是希望能在作用域外自动销毁的时候我们才应该使用智能指针。 也就是智能指针分配的数据空间是在heap上的，但是存储指针自身的空间是在stack上的 先思考使用unique point 在需要不同的地方共享的时候在考虑share pointer 避免使用new delete 实际上智智能指针就是对原生指针的一个高层封装，就是类似struct ，在struct 的destructor 调用指针指向地址的delete？ 看看视频中的代码。 2.3. Part 4 Class & Struct 面向对象 This Chapter 主要就是面向对象的编程逻辑，以及类和结构体中的一些知识点 面向对象编程：这是一种非常流行的编程思想，这是一种编码的风格。 jave所有的一切都应该是一个class. 2.3.1. 类（Class） Class：和python是一样的，是一种将数据和function（method）组织在一起的一种方式。 和namespace的区别是啥：namespace没有访问控制。 一些基础用法和信息 比如玩家的属性，和玩家的一些function，就可以使用class的instance来定义多个角色，而不必重复的定义类似的方法和属性值。 类中的function就被称为method 可以使用大括号的方式来初始化赋值 struct Vector2 { float x,y; }; int main() { Vector2 a = {2,3}; } 实际上可以在类内declaration函数，然后再类外用::namespace的方式进行定义 当我们想要将Class or Struct传入Function中的时候，我们最好是使用reference &和Const，这样可以防止传入的类之类的被修改，同时也节省了memory，不需要额外的生成一个copy。 建立一个Logging类：将warning Or Error 打印在控制台上，因为控制台永远不会出问题，对于debug很有帮助。 继承所有在Entity中不是private的都会被player继承。 访问控制（Visibility） public：可以在类外访问，也就是可以在类的外部随便定义，取出，或者修改。 private：只能通过类内的操作或者类内的函数，以及friends进行修改调用。 protect：类内或者继承类 friend的定义方式：在类内的public使用前置friend 去重载这个函数或者类别即可。就能访问私有变量了。 类的构造函数和析构函数 C++ 类构造函数 & 析构函数 | 菜鸟教程 (runoob.com) constructors**：构造函数（可重载）** 用于每个instance生成时候的初始化，我们可以通过不同的传入值来重载这个函数。 名称和类名一致，不需要type。 默认是存在constructor的（但是不初始化变量），如果我们不希望用户构建实例，我们可以将constructor写在private中，那么就无法使用该类别去生成一个实例。 函数构造的初始化列表： 构造函数初始化列表以一个冒号开始，接着是以逗号分隔的数据成员列表，每个数据成员后面跟一个放在括号中的初始化式。 但是这种方式，需要我们按照成员函数的顺序去编写，他是默认这样执行的。 类构造函数初始化列表。 这样假如我们成员中有类别实体，我们可以避免该类别实体进行重复的构造，浪费了性能。（可以用cout测试） 此外这样也可以分离初始化参数和一些其他的初始化指令操作（写在大括号里）。 Destructor**：析构函数** 用于destory我们生成的object或者说instance；清除变量。 在构造函数前面加~就是定义的方式。一般不需要显式编程 活到生命周期末尾（大括号之类的）会自动调用 如果New 就需要Delete才会调用 类的继承 继承的主要作用，是让我们拒绝duplicated，拒绝代码重复。所以我们就能在父类中放置通用功能，然后在子类中重载或者编写新功能。 在子类的定义的时候：[public] 父类，父类2 如果函数的输入是（父类* a），那么所有的子类都可以输入来着 ：实际上是多态的因素，就是所有的子类都属于父类把，所以父类的指针可以代表所有的子类。 虚函数Virtual Function 在父类中编写的virtual function就能在子类中选择覆盖重载 Virtual function可以避免在特殊情况下，我们在子类覆盖定义了父类函数的情况下，还是调用了夫类中的同名函数的情况：它加入了动态分配的机制，通过存档虚函数 所代表的各种虚函数映射情况，便于我们找到正确的函数。 实际上也就是在需要重载（override）函数前面加入一个virtual的关键字（在最前面）；同时可以在覆写（override）的地方加上override关键字（在声明的最后面，大括号的前面），但是这不是必须的，但是更具备可读性。 但是需要额外的内存空间：需要表需要基类指向虚函数表的指针； 需要额外的运行速度：因为每次调用虚函数，需要额外遍历虚函数； 一般情况损耗不会太大，除非对于嵌入式设备来说。 Virtual Destructors 和virtual一样，为了我们在使用子类进行多态操作的时候，不会发生没有调用destructor导致内存泄漏的情况。所以就要再析构函数的时候virtual一下。 Interface（PureVirtualFunction）纯虚函数 纯虚函数也可以理解为接口，就是需要后续被实现的一些 实际纯虚函数就是一种在基类中没有实现的函数：在Jave和C#这类语言中就被称作接口，也就是我们需要在子类中一定要重写的函数。（有时候我们需要每个子类都定义特殊的函数，基类的定义顶多就当作模板，不包含实现方法） 在基类中对虚函数的实现（{}）改成（=0；），就是纯虚函数了，如果我们想要使得子类能够实例化对象，我们就必须在子类中override这个纯虚函数。这种时候基类也不能被实例化了。 如果父类对祖父类的override了，那么我们可以直接继承父类就不用再覆写了。BTW：纯虚函数（和成员变量）组成的也叫做抽象类 类的多态 多态 https://www.runoob.com/cplusplus/cpp-polymorphism.html 其实就是用base的指针来指向子类的一种调用方式 2.3.2. “This” 指针 this是指指向当前对象的指针，索引到当前的instance； 用来调用当前类中的函数或者变量 class Entity { public: int x,y; Entity(int x, int y) { this.x = x; } }; 2.3.3. 复制构造函数以及浅拷贝深拷贝 基本概念 通常对Class进行复制或者等号赋值操作的时候，很多情况下会发生内存拷贝，这样会使用新的内存地址去存储新的相同数据，也就是建立一个副本，但是很多情况下是不需要的。（用等号的时候都是进行的copy） 这种方式就是建立副本： Vector2 a = {2,3}; Vector2 b = a; b.x = 5; //不改变a 可以使用指针建立指针的副本，数据空间不进行拷贝； Vector2* a = new Vector2(); Vector2* b = a; b->x 浅拷贝：以自定义的String为例 c++已经实现了，这就是个范例； memcpy是内存赋值（赋值内存块）：在这里就是取代for loop 去copy value class String { private: char* m_buffer; unsigned int m_size; public: String(const char* string) { m_size = strlen(string); m_buffer = new char[m_size + 1]; //假设包含终止符的情况，但是这样就会对于非普通string的类别不太安全（它们可能不包括终止符） memcpy(m_buffer, string, m_size); } friend std::ostream& operator 基于终止符的不同情况，我们可以修改成： memcpy(m_buffer, string, m_size); m_buffer[m_size] = 0; 但是实际上这样可能会发生内存泄漏，所以实际上我们不应该忘记析构函数： ~String() { delete[] m_buffer; } 但是如果我们在这种情况下定义的string进行复制的时候，我们会发现，我们实际上是对类中所有的value进行copy，所以我们实际上拷贝了一个指针，然后再最终程序结束的时候，我们对同一个地址调用了两次析构函数，于是程序崩溃了。这就是浅拷贝。 深拷贝：以自定义的String为例 我们想要的就是有一个指向新地址的新指针，然后指向的地址有一样的value。 于是复制构造函数就被需要了，就是实际上我们是重新调用了一次前面的构造函数，去构造了一个完全相同的变量，而不是只是浅拷贝。 C++实际上已经存在了一个默认了：我们可以直接声明，但是这实际上是浅拷贝，也就是默认的方式 String(const String& other); //这个实际上是浅拷贝 == 下面的 String(const String& other) :m_buffer(other.m_buffer), m_size(other.m_size) {} or if you want to be more exciting... // 也等同于下面这个，都是浅拷贝 String(const String& other) { memcpy(this, &other, sizeof(String)); } 那么如果我们想要完全禁用浅拷贝：我们可以在声明后面加上=delete 实际上就是通过构造函数的类型隐式转换（就是那个等号的重载来实现的），所以我们要弄一个深拷贝的话： String(const String& other) :m_size(other.m_size) { m_buffer = new char[m_size+1] memcpy(m_buffer, other.m_buffer, m_size+1); } 2.3.4. 单例（singleton） 单例模式(Singleton)及其C++实现_FBeetle的博客-CSDN博客 单例是面向对象里面的一种编程模式，也就是某些类别只有一个例子：比如班主任，一个班只需要一个班主任。 或者我们只是要提供一个通用的方法库这种情况下，可以使用这种编程模式。 这就是一种编程方法，我们把东西都放到类中。然后使用类变量来调用global set of function 或者data.我们不需要进行实例化或者其他的操作、 换句话说，我们就是把class 像namespace这样来用。 Singleton 就是组织大量全局变量和static function的方法，将这些组合成一个blob。 阻止实例化 可以通过把构造函数设置为私有来阻止实例化操作，但是还是会存在缺陷，还是可以通过下面的操作来类似的实例化； Singleton instance = Singleton::Get(); 但是这样的话会把我们singleton的数据都复制一次（每执行一次复制一次），所以我们需要去除复制构造函数来防止这种操作的实现。这样的话，我们最多就是使用&来使用一个别名进行这个singleton的调用（在上面那个定义加入& ->Singleton&） public: Singleton(const Singleton&) = delete; 标准的单例调用模式存在一定的麻烦（代码块1），如果我们希望去掉Get来优化调用的过程：可以在定义的时候修改成（代码块2） Random::Get().Float(); // 先获得单例，然后调用函数 // 或者使用下面的方式 auto& random = Random::Get(); float number = random.Float(); public: static float Float() {return Get().IFloat();} private: float IFloat() {return m_RandomGenerator;} 单例通常使用::在外面定义，就像下面大图的nullptr 但是如果我们不希望这样（不希望自己在外面进行一个初始化）我们可以写成（写在get里，那么初次调用的时候会生成类static的singleton）（这也是结合了后面修正的最终版）（分析一下这些static）第一次生成，后面全是singleton引用。 class Singleton_origin { private: // 构建一个通用的单例指针 static Singleton_origin* s_Instance; public: // 通过Get返回指针所指向的singleton对象 static Singleton_origin& Get() { return *s_Instance; } // 每个Singleton_origin class 的实例中的通用Function void hello() { } }; Singleton_origin* Singleton_origin::s_Instance = nullptr; // TYPE2: 实际构建一个singleton怎么去做 // 这一步还没有对复制的情况以及调用的麻烦的情况进行优化，结合下面的最终版。 class Singleton { public: static Singleton& Get() { // 这个静态的instance只会在初次调用这个GET的时候生成，后续的话就是直接return他了。所以这样写就行 // 需要注意的是，由于我们返回的类型设定为reference（&），所以如果我们去掉的了↓的static关键词，就会出错 // 因为reference返回的是别名，也就是需要这个值一直存在，不然在后续的使用中会出现问题，而不是返回一个copy，当然也可以去掉函数中的&。 // 所以我们需要借助static 关键词，来延长这个instance的声明周期，才会被正确的使用。 // 这种只有第一次起到作用的方法，就适用于很多需要初始化的场景。 static Singleton instance; return instance; } void hello() {} }; int main() { //Singleton_origin::Get().hello(); Singleton::Get().hello(); std::cin.get(); } 最终版 集成了上面的全部优点，简化了调用，不会进行复制，不需要在类外进行单例的初始化。 public: Random(const Random&) =delete; static Random& Get() { static Random instance; return instance; } static float Float() {return Get().IFloat();} private: float IFloat() {return m_RandomGenerator;} Random(){} float m_RandomGenerator = o.5f 2.3.5. 结构体（Structure） 实际上就是一个默认是Public的Class，Class是反过来的。 什么时候该用结构体什么时候该用类？ 默认情况下：class是私有的，类或属性（需要共有的时候要public）；而Struct是默认公有的（需要私有的时候要用private）； 这是唯一的区别，但是在代码实际使用的时候还是有所不同的：存在的原因 保持和c之间的兼容性 或者我们想要全都用public的时候。 自定义使用场景（规定编程风格） Plain old data（pod）的时候喜欢更多的用struct，就是仅仅只代表一堆变量的时候。比如说定义向量，这中类似的数据体的时候 “我将永远不会对struct使用继承” 2.3.6. 公用体（Unions） 是一种类似Struct的结构，但是同时只能存在一个member（变量），无论你声明了多少个，实际上都是共享内存空间（地址）的，所以如果我们declaration的Type不同，就可能会出现Type Punning的现象，实际上也可以理解为一个变量的多个别名？ 菜鸟教程，CPP reference 但是通常匿名使用，也就是只使用其只能有一个member的特性，很多时候会放置再Struct 之类的里面。和Struct一起匿名使用是不影响我们的调用层级结构的 存储空间的大小以最大的成员作为标准好像 struct Vet2 { float x,y; }; struct Vet4 { union { struct { float x,y,z,w; }; struct { Vet2 a,b; }; }; }; // 这样的话 Vet4 v = {1.0f,2.0f,3.0f,4.0f}; //v.x 实际上和 v.a.x 是一样的,他们共享了一样的内存地址，这个和type punning有点像。 2.3.7. 结构体绑定（Structured binding） 这一部分实际上解决的是，我们使用struct的方式来实现多类回归的情况：linkto),所以参考那一部分就好了 2.4. Part 5 WorkFlow&逻辑控制 This Part 介绍一些算法的逻辑控制以及workflow控制，包括循环，条件，函数，线程之类的，控制工作流的内容。 2.4.1. Func： 基本的函数定义（Functions） 其实没啥特别好说的就是： 一般在header 中写入declaration，然后把definition写在Cpp files里面。 其实定义的全局变量在function中也是可以直接调用的，不需要重新导入之类的。这个应该都是懂的吧 :star: Always pass you object by const reference!总是使用const和reference传入我们的参数。 需要副本就复制. 2.4.2. Func： 匿名函数（Lambdas） 参考资料：lambda详细教程，捕获值理解 匿名函数实际上是用于基本上一次性的函数：我们不需要真正的（实际的）对函数进行定义。 基本定义方式：[capture](传入参数){实现内容} auto lambda = [](int value){std::cout 需要传入外部数据的时候就需要使用Capture，比如main中的值，用&or =；如果我们要使用Capture的时候，我们可能要#include Question: lambda的传入的参数默认是不能修改的，要修改的话我们需要加入mutable关键词（在传入参数和实现内容之间） 其他的lambda实例： std::vectorvalues = {1,5,4,2,3}; std::find_if(values.begin(), values.end(), [](int value){return value>3;}); // it actually is&#x1F447; 返回第一个>3的值 auto it = std::find_if(values.begin(), values.end(), [](int value){return value>3;}); std::cout 2.4.3. Func：三元运算符（Ternary Operator） 条件表达式？表达式1：表达式2 这种形式实际上和python中的如下的表达式一致 Flag = True a = 5 if Flag else 10 c++中表示为如下 s_Speed = s_Level > 5? 10: 5; s_Speed = s_Level > 5? s_Level >10? 15: 10: 5; 2.4.4. Func: 多值输出 (Multiple Return) include新报本的结构体绑定方式。 由于C++本身的Type机制，我们没办法在func中同时直接return不同类型。 而如果我们试图return同一type的多个value的话，我们实际上可以用returnvector或者array的方式实现，当然这就是一种比较蠢的操作了。 stdarray或者传统的array 好像也可以使用tuple的方式{v1,v2}同个类型的多个值 推荐：当然聪明一点的方法就是，我们定义一个struct，包含我们需要的这些所有type，然后return这个struct就好了 还有一种方法就是使用&来传递参数，就不需要return了，设置为void就可以了 也可以用指针的方式，指针方式的好处就是可以是空值 C++的默认指定方式：tuple和pair 用tuple类型的方式操作起来有点麻烦啊如下，但是也可以混杂多种类型输出。 #import static std::tuple func() { //return std::make_pair(var1,var2); //上面的type指定有时候也能省略 //或者使用下面这种方式,上面那种实际上是pair类型的返回把 return {var1,var2} } auto [name, age] = func(); // c++ 标准更新以后tuple的使用方法变得更加的好用了。 ------------------&#x1F446;NEW Version & Called Structure Binding---------------- --------------------------------&#x1F447;OLD VERSION a = func(); ////或者 //auto a = func(); ////oldversion要取出元素的时候我们还需要 //std::string& name=std::get(a); //这里可以用&防止动态的内存copy的情况 //std::get(a); ////所以这边建议使用struct //// 取出元素的第二种方法. //std::string name; //int age; //std::tie(name,age) = func(); 2.4.5. Threads：线程操作 函数编写过程中的多线程操作和线程管理，下面是一个典型的例子，我们好像也可以使用进程去建立一个线程对象。 使用线程主要的目的是为了1. 完成单线程没法完成的事情以及2. 优化一些算法的运行速度。 #include #include //支撑线程的基准库 static bool Flag = true; // 编写一个函数用于子进程的执行，通常使用函数指针的方式调用 void DoWork() { while (true) { std::cout 如果我们再调用某个进程的时候想看当前的ID，也可以再运行的函数中加入STD::this_thread::get_id()得到当前进程的ID。当然我们每次运行可能都是不一样的。 2.4.6. Threads：多线程管理 这一部分没有英文字幕，缺失了很多信息，后续使用到的时候进行补充和修正把。 线程并行；异步；等等的多线程管理。std::async 对于independent的Application和Function Part，实际上很多操作我们可以在cpu和memory上并行进行，对不相关的任务进行分布（异步），对相关的任务有所约束（同步），合理的对进程进行调度，能够使得我们对资源有更充分的利用，同时也能提升程序的运行速度。 对于不依赖于运行次序的一些操作：比如载入很多模型或者数据（num_worker） cppref参考页面；异步合同的概念； #include static std::mutex s_meshesMutex; //解决1. 针对变量定义一个互斥锁 static void LoadMesh(std::vector>& meshes, std::string filepath) { auto mesh = Mesh::Load(filepath); //解决1. 使用lock，锁住我们可能需要修改的这个变量，使得一个thread在进行修改的时候，&#x1F512;（其余不能对该变量进行操作），修改完成解锁&#x1F511;； std::Lock_guard(std::mutex) lock(s_meshexMutex) meshes.push_back(mesh); } for (const auto& file : filelist) { //1. 异步使用文件载入，但是这样会出现问题，就是当我们两个进程同时进行修改操作的时候怎么办？ m_Futures.push_back(std::async(std::lauch::async,LoadMesh,m_Meshes,filelist)); } 为了防止同时的写入操作，我们还需要Lock操作去锁住可能会修改的变量。同时好像异步程序的返回值比较特殊，所以我们需要在头文件中进行如下定义： std::vector> m_Future; 此外我们可以控制是否执行异步程序（老办法了） #define ASYNC 1 #if ASYNC ASYNCfunc() #else func() #endif 最后我们可以在debug的时候 windows ，parallel stacks找到进程表（很吊）。好像也可以在执行代码的地方跳到正在执行的某个进程 2.4.7. Benchmark：基准测试 在运行程序的时候如何监控我们该代码的性能（运行时间等等），或者测试新方法的方式。这里给出了他的方式。实际上有很多不同的方式。 _debugbreak(): 类似python中的raise exception // 简单的范例， int main() { int v = 0; { Timer timer1; func() } ...; _debugbreak(); } 我们要确信的一点是，计时器是否真正的测量了运行的时间，因为有时候编译器会直接进行中间态计算，所以实际运行的时候，就会没有计算到开销。 测量share pointer和unique pointer unique>make share >new share Visual Benchmarking （可视化） 使用chrome:://tracing 在浏览器中进行可视化,这一课作为补充资料把。暂时不需要用这种方式 2.4.8. Switch：case 分支 一个 switch 语句允许测试一个变量等于多个值时的情况。每个值称为一个 case，且被测试的变量会对每个 switch case 进行检查。 switch(expression){ case constant-expression : statement(s); break; // 可选的 case constant-expression : statement(s); break; // 可选的 // 您可以有任意数量的 case 语句 default : // 可选的 statement(s); } 2.4.9. Workflow：Conditions and Branches 条件和分支 if 指令实际上是检查值是否为0，0 == False， !0 ==True; Keyword：if; else if; else; 2.4.10. Loops: For and While 循环定义 For 循环其实就是 （声明变量；condition；迭代规则） 我们可以直接在括号里写，也可以全部写在外面 condition要声明，但是可以在外面定义 While （condition） Do { }While（condition） 即使条件为false也至少能执行一次 2.4.11. Workflow: Control Flow (contiune, break , return) 循环控制 这几个关键词的使用和含义基本是和Python一致的，但是这里的Break还用于switch 2.4.12. Workflow： Iterators迭代器 迭代器的一些基本的参数：first（key），（当然这个是以这种形式存在的是时候才有的）second（value）；it本身是以指针形式存在的？ 是一种对dataset中的数据进行迭代的方式，这就是一种迭代器，有点像是运算符重载，通常用于对数据结构进行迭代（遍历）。 //最常见的方式就不再说了，。vector.size()来循环就行了 #include // type1 内置的迭代器，也是常用的使用方式。 std::vector values = {1,2,3,4,5}; for (int value : values) std::cout::iterator it = values.begin(); it != values.end(); it++){ std::cout 那么我们如何对于无序的数集(实际上unorder_map是Hash的C++实现)来进行迭代或者遍历呢？，看下面这个例子： #include using ScoreMap = std::unordered_map; //std::unordered_map map; ScoreMap map; map[\"aiken\"] = 5; map[\"c++\"] = 2; // 由于无序图没有index，经典的就是使用这样的方式 for (ScoreMap::const_iterator it = map.begin(); it != map.end(); it++) { auto& key = it->first; auto& value = it->second; //second 应该指的是value把，但是对于多元素的hash怎么处理？ PRINT_FUC HERE. } // 仿照上面那种更方便的方式来编写迭代的话(这里的auto 实际上是pair的形式) for (auto kv : map) { auto& key = kv->first; auto& value = kv->second; PRINT_FUC HERE. } // 当然还有一种梦寐以求的方式，后续可能我们会最常用的方式 for (auto [key, value] : map) { std::cout 编写我们自己Structure中的Iterator： 假设：这是不是应该使用类似linklist之类的方式，将数据通过类似指针的方式迭代的串起来？ 这一部分太长了，还是参考视频94把，我就直接重载[]和size通过for去做迭代器了，建议加入相关的.cpp，不要集成在文档中。 这一部分其实可以帮助对一些重载，还有一些vector机制有一个更好的理解，以及一些动态的内存管理，可以自己在后续编写一下试试。 2.4.13. Workflow: Continuous Integration CI （持续集成） 每次commit后都build 以及run &test就是CI吗？ “持续集成是一种软件开发实践，即团队开发成员经常集成他们的工作，通常每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来验证，从而尽早地发现集成错误。” 使用：jenkins（第86课） 2.4.14. Workflow: Static Analysis(静态代码分析) 也就是一些分析工具，比如pylint之类的东西； Cherno推荐：PVS-Studio 2.5. Part 6 Memory 资源管理 This Section 我们从memory出发来谈及关于Stack，Heap之类的内存管理和优化方法,以及编写的safe。 粗略：Heap和Stack的区别；进阶1：C++中内存分配，堆（Heap）与栈（Stack）区别 为什么c++中要分为heap（堆）和stack（栈） 2.5.1. Stack vs Heap: C++中的内存栈与堆 char* buffer = new char[8] 定义一个8个字节的内存空间，并返回指向内存开始的地址的指针 基本概念：stack和heap都是内存（RAM）中实际存在的单元 stack存在预定义的长度: 2M左右。 heap虽然已有预设的默认值，但会随着我们的Application去更改大小 目的都是为我们的程序和全局或者局部变量提供存储空间。 不同的内存分配方式。 memset可以用来填充内存块 memcpy 内存拷贝，拷贝内存块 基本定义方式：前面一般是定义在stack上的，后半部分是定义在heap上的 int main() { int value = 5; int array[5]; ClassA vector; int* hvalue = new int; *havalue = 5; int* harray = new int[5]; ClassA* hvector = new ClassA(); } 分配方式上的区别（主要是new） stack上分配的内存空间是连续的，实际上就是栈顶的指针移动需要的距离，然后重新赋予数值。每一个在另一个上面。 所以这样分配会比较快，我们只需要在寄存器上移动指针的地址就可以了 {}实际上就是一个stack，超出这个作用域后，栈内的数据会自动销毁，也就是实际上就是将指针还原到了作用域开始的地方。Free操作实际上只是指针的移动。 heap上分配的内存空间是随机的。 实际上会call malloc，给你一个指定大小的内存块，同时也会管理一个需要free-list的内存列表（也就是已经申请了的列表）， 所以在heap上分配空间实际上是一整套任务，而在stack上实际上就是指针移动，他们两个的效率是完全不一样的。但是有各自面对的状况吧。 需要大量数据，或者说是，需要延长生存周期的话都需要用heap。 在debug model中的汇编之类的机器代码是没有经过精简的，但是release后vs会自己优化。 2.5.2. New：Keyword For Mem内存关键词 使用new实际上是一系列命令（运算符重载），包括在空闲的内存块中占用一块指定大小的内存，所以会需要时间； 基本准则：有new有Delete；无new 无delete new经常和数组一起使用来获得指定大小的heap空间 new在使用的时候也会调用constructor（构造函数），相应的delete； new ->delete; new [] -> delete []; int* b = new int [50]; Entity* e = new Entity[50]; //这种情况下同时会调用构造函数 //虽然我们可以用malloc指令定义，但是这种方式不会调用析构函数，所以千万不要用这种方法。 具体的底层原因： new底层其实是call了malloc，malloc是memory allocation的简写，从名字也可以知道它负责分配内存，delete则调用了free()。区别是new和delete不仅管理内存，还会调用constructor和destructor，另外它们都是operator，所以你可以重载它们，做一些有趣的事情。 对了，new【】和delete【】其实另两个operator，它们做的事情稍微有点不一样，你调用new【】的时候，必须要指定一个size，但调用delete【】的时候，并没有指定size，它怎么知道delete多少呢？这是因为new【】不仅分配了所需要的内存，还会多分配一个额外的空间，来存储这个size，所以以视频中的举例，它所做的是分配这样一块内存【8, 0, 0, 0, 0, 0, 0, 0, 0】，连续的，但是多一块在最前面，但是return给你的是跳过那块内存的地址，比如malloc返回的是0x1，但new【】给你返回的是0x1+2（我记得它分配的是一个word（一般是short）的大小，具体大小需要看系统），然后在delete【】的时候，它会往前推一个word，因为它知道前面一个word肯定是size，从而拿到size，进而delete所有） 什么时候通过New来定义实体（instance） 就是如果我们希望在一个Function中定义类的实体的时候，为了延长生命周期，我们需要将实体定义在heap上 或者是class 规模太大，但是stack太小了，所以我们要借助heap的存储空间 Entity* entity = new Entity(\"Cherno\"); 2.5.3. Safety：使用智能指针的情景 什么样的程序是safe的？减少崩溃和内存泄漏的情况，也就是让Code尽量不要越过需要的边界。This Part is about Smart Pointer。 实际上安全性和内存分配是分不开的。 对自己所有allocated的memory负责，所以智能指针特别屌，应该100%使用智能指针，不要仅仅使用原生指针（Raw Poniter），能不用就尽量不用。智能指针我们就不需要担心delete或者内存泄漏等等的问题了。 Raw Poniter在我们使用的小规模程序的时候偷懒，因为只要使用※，他不安全，就只是好读和简单。 所以就是，使用smart pointer，当然在确保安全的情况下，我们也可以用raw pointer自由选择反正。优点和缺点就是这样了。 2.5.4. Track Memory Allocation：内存申请跟踪 优化跟踪内存管理对于计算机的性能来说相当重要，特别是我们要知道我们在哪里分配了数据。 虽然heap的space比较大，但是在性能至上的环境中，可能heap不会是一个最佳的选择。 智能指针会在heap上分配内存，（std::string都是分配在内存中的） Easy Way： 这一部分能够简单的嵌入我们的任何project；(void* 存储的就是一个内存地址) 基本思路：重载我们的new 操作符; import void* operator new(size_t size) { std::cout 那么当然我们也可以重载delete去检测内存的释放情况 void operator delete(void* memory, size_t size) { std::cout 那么最方便的实现方法就是我们使用一个struct来统一管理我们的内存分配情况。 struct AllocationMetrics { uint32_t TotalAllocated = 0; uint32_t TotalFreed = 0; uint32_t CurrentUsage(){return TotalAllocated-TotalFreed;} } //实例化全局架构 static AllocationMetrics s_AllocationMetrics; // 然后将上面的new和delete中的print改成+= 和-= static void PrintUsage() { std::cout 2.6. Part 7 How to make C++ run Faster 算法或者进程优化的部分，这一部分在做题的时候慢慢的进行填充把，在学习的时候先选择性的看看，实际上应该是Part6的延申，逃不脱资源管理的部分、 TODO：（已知可以但是应该暂时没用） std::async （应该是用于进程优化） 79课 80 83：string优化 2.6.1. run string faster 优化string的运行速度 string会在heap上allocated，所以对于性能优先的情况下不是特别推荐的,可以通过下面的方式查看heap申请。 void PrintName(const std::string& name){} // 想要看空间的声明，重载new就对了，看Part6 static uint32_t s_AllocCount = 0; void* operator new(size_t size) { s_AllocCount ++; PrintHere; return malloc(size) } PrintName(const std::string& name){} int main() { name = \"aiken\"; Print(name); //copy一次 std::string name = \"aiken aiken\"; //copy std::string firstname = name.substr(0,3); //copy std::string lastname = name.substr(4,9); //copy } 但是加入我们只是想要一个很简单的输出“”，不希望发生再次的construct，来增加一个string的heap空间。甚至我们使用substr()来输出其中的一部分，也会copy（allocation）一次原本的string。 避免这样无意义的Copy，只是要一个指向原数据内存地址的指针，以及size把，我们可以很容易写一个这样的类，但是在C++17中官方集成了把&#x1F447; PrintName(std::string_view name){} //这样以后print（“”）也不会分配了，原本的情况，执行print甚至都会复制一份。究极不合适 std::string name = \"aiken aiken\"; //allocation only //3可以修改成5 就没有allocation了,但是5的话 6，7的c_str()要去掉 const char* name = \"aiken aiken\"; //not allocation std::string_view firstName(name.c_str(),3); std::string_view lastName(name.c_str()+4,9); print(firstName); //这种类型也不会再发生复制了 print(LastName); print(\"aiken aiken\"); ----------------------------------整理如下-------------------------------------- //只在定义的时候发生一次赋值。 printname(std::string_view name){} std::string name = \"aikenaiken\"; //allow 1(copy happen) std::string_view firstName(name.c_str(),3); std::string_view lastName(name.c_str()+4,9); print(firstName); //这种类型也不会再发生复制了 print(LastName); print(\"aiken aiken\"); 2.7. Somthing Else 无题 畅所欲言，或者等待归类。 sizeof通常用来获取数据的存储空间； strlen():获取const char*的长度 alloca(size)：再当前地址内存分配；所以我们可以类型转换指针（int*） size_t:可以存储各种类型大小的值,size type constexprhttps://www.jianshu.com/p/34a2a79ea947 \\:编写代码的时候的换行续接符号 wandbox.org ：在线编译网站 Visual Studio 表达式的编译顺序是从右到左运算 2.7.1. Argument Evaluation Order 参数输入顺序（面试？） 传入参数实际上可以是传入一个函数或者是一个表达式，我们应该规划一下这样的输入。（和++相关） i ++ : 先传递在增长 ，++i：先增长在传递把 (undefine behavior: 意味着这种方式实际上没有被定义，也就是说是一种不可控的行为，下面是一种实例)（切换这种++的位置也是） void Psum(int a, int b) { std::cout 正确答案是这种实际上是C++没有规范的，我们没法得到真实的值，但是再C++17中要求： The Postfix-Expression is Sequenced before each expression in the expression-list and any default argument。 也就是他们需要被一个接着一个的运行。这个其实没有太听清。 https://blog.csdn.net/samantha_wang/article/details/46942343 https://blog.51cto.com/8681495/1416759 也不要写像这种的 v[i] = ++i; © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 21:57:31 "},"Langs/Shell.html":{"url":"Langs/Shell.html","title":"3 Shell","keywords":"","body":"1. Shell Lang1.1. 基本命令1.2. 条件语句1.3. 特殊指令1. Shell Lang @AikenHong 2021 This Doc is About shell Script @Reference: The Art of Command Line 脚本语法的文档 1.1. 基本命令 执行script.sh的方式： # powershell # but this function will appear some error for git command bash script.sh # linux sh script.sh echo Display A Line of Text 1.2. 条件语句 按照if,else,fi来进行编写，同时[]中前后都需要有相应的空格 # enter the right dir cd _book # using conditional rules to control the update actions if [ -d \".git\" ]; then echo \"exist git files\" else git init git remote add origin .. echo \"add remote repo\" fi Linux-shell中$(( ))、$( )、``与${ }的区别 1.3. 特殊指令 g++ helloworld.cpp -o a ./a output © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-06 00:14:45 "},"Langs/Markup_readme.html":{"url":"Langs/Markup_readme.html","title":"4. Markup Langs","keywords":"","body":"1. Markup Langs1. Markup Langs @AikenHong 2021 标记语言是用于格式化文本或者数据的一种语言，用来在多种Langs中进行通用的传递。像是JSON，XML，CSV等等类型都算是标记语言。 在这里会简要的介绍一下常见的几种标记语言的一些特性和对应的在Python或者cpp中读取和存储的方式。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 19:20:33 "},"Langs/Markdown.html":{"url":"Langs/Markdown.html","title":"4.1 Markdown","keywords":"","body":"1. Markdown Material1.1. Editor：1.2. Some daily usage：1.3. 希腊字母表：1. Markdown Material written by ==Aiken==, 2020this document is about Markdown’s Tips, in order to help me writing good notes. 参考资料： 一些关于markdown语法的参考资料，但是实际上如果用Typora的话，有很多的语法是不需要记忆的，只需要稍微了解就可以了，更需要学习的其实是Latex的公式编写。 基本语法：https://www.jianshu.com/p/191d1e21f7ed 进阶语法：https://blog.csdn.net/m0_37925202/article/details/80461714 其他语法：https://blog.csdn.net/cuishizun/article/details/80311673 目录： Editor 一些常用操作 希腊字母表 1.1. Editor： Typora：Notes的主力编写工具 VsCode：主要编写README等工程文档的时候使用 Jupyter：代码笔记编写的时候 1.2. Some daily usage： 操作名称 Typora VsCode 跳转 [button] (#name)-># [button] (#name)-> \\ 复选框 - [ ] - [ ] 1.3. 希腊字母表： 序号 希腊字母 Markdoown 序号 希腊字母 Markdoown 1 α \\alpha 19 β \\beta 2 γ \\gamma 20 δ \\delta 3 Γ \\Gamma 21 Δ \\Delta 4 ε \\varepsilon 22 ϵ \\epsilon 5 ζ \\zeta 23 η \\eta 6 Θ \\Theta 24 ι \\iota 7 θ \\theta 25 κ \\kappa 8 Λ \\Lambda 26 λ \\lambda 9 μ \\mu 27 ν \\nu 10 ξ \\xi 28 ο \\omicron 11 Π \\Pi 29 ρ \\rho 12 π \\pi 30 τ \\tau 13 Σ \\Sigma 31 Φ \\Phi 14 σ \\sigma 32 ϕ \\phi 15 Υ \\Upsilon 33 Ψ \\Psi 16 υ \\upsilon 34 ψ \\psi 17 Ω \\Omega 35 ω \\omega 18 φ \\varphi 36 Ξ \\Xi © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:35 "},"Langs/Latex.html":{"url":"Langs/Latex.html","title":"4.2 LaTex","keywords":"","body":"1. LATEX 中的公式1. LATEX 中的公式 参考资料 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-18 16:57:38 "},"Langs/Markup_Langs.html":{"url":"Langs/Markup_Langs.html","title":"4.3 Json, Xml, Csv with py","keywords":"","body":"1. Read Local Data1.1. Python: Glob1.1.1. glob.glob:1.1.2. glob.iglob()1.2. Python：pickle1.3. YAML1.3.1. 写在前面1.3.2. YAML-Python1.3.3. YAML的写入1.4. JSON1.4.1. Json-Python1.5. XML1.6. CSV1.6.1. write1.6.2. read1. Read Local Data @Aiken 2021 对于各种形式的标记文档和数据集的处理进行一个整合，还有一些python中的相关模块（比比如glob，后续可能会迁移到别的文档中），主要包括：yaml，json，csv，xml，这些可拓展的标记语言. TODO： [ ] csv,xml：这一部分可以看一下吉仲师兄那边是怎么存和取文件的，继承一下代码减少我这一部分的工作量 [ ] 按照csv文件对数据集进行本地的文件夹切分。实际上很多数据集，像mini-imageNet这类的是需要我们下载下来之后按照csv文件对训练集和测试集进行切分的 [ ] 使用sklearn对完整的未切分数据进行切分。 1.1. Python: Glob 文件操作相关模块，用于简单的路径匹配的模块，用来查找路径中的相关文件，基本的正则匹配规则如下： “*”: 匹配0哥或多个字符 “?” : 匹配单个字符 “[ ]”: 匹配指定范围内的字符,如[0-9]匹配所有的数字 1.1.1. glob.glob: 返回所有匹配的路径列表,只有一个参数pathname,定一乐文件路径的匹配规则,可以是绝对路径或者是相对路径,具体的使用可以参考如下的方式: for xmlpath in glob.glob('media/all/DATAPART/' + \"*\") # xmlpath 遍历文件夹下的所有文件和文件夹 for xmlpath in glob.glob(xmlpath + \"/*/*\") # xmlpath 遍历文件夹下所有文件夹中的文件夹中的文件:按照层数自由设定 img_path = sorted(glob.glob(os.path.join(images, '*.npy'))) # 遍历文件夹下的所有npy文件,说实话感觉这个怪离谱的,晚点试一下 import glob print(glob.glob(r\"E:/imgdir/*/*.jpg\")) 1.1.2. glob.iglob() 获取一个可遍历对象使用它可以逐个获取匹配的文件路径名: glob:一次获取全部 iglob:逐个匹配路径获取. 1.2. Python：pickle palceholder 1.3. YAML YAML是一种标记语言，可以通过YAML定义超参数，然后从外部引入，所以常用来作为一些特定的config，具体的用发和用途可以这样理解： 当我们使用不同的backbone module的时候，我们可能对于超参数等等的一系列配置是不恒定的，所以使用config文件去配置的时候，当我们每次切换，我们就只需要读取不同的config文件就行了。 实际上就是argparse的一种替代 所以本文档聚焦于如何在python/cpp中读取yaml（以及cpp补充相应的数据类型） 1.3.1. 写在前面 基本的语法什么的很好搜索，随便百度一下就行了 offical site 菜鸟入门教程 实际上大部分都是使用缩进去控制的，例子&#x1F447;，很明显可以看出对应的元素关系，包括字典，boolean，float等其他的类型。 # default num_head = 2 criterions: PerformanceLoss: def_file: ./loss/SoftmaxLoss.py loss_params: {} optim_params: null weight: 1.0 last: false # apply incremental pca to remove main components apply_ipca: false num_components: 512 networks: classifier: def_file: ./models/CausalNormClassifier.py optim_params: {lr: 0.2, momentum: 0.9, weight_decay: 0.0005} scheduler_params: {coslr: true, endlr: 0.0, gamma: 0.1, step_size: 30} params: {dataset: ImageNet_LT, feat_dim: 2048, num_classes: 1000, stage1_weights: false, use_effect: true, num_head: 2, tau: 16.0, alpha: 3.0, gamma: 0.03125} shuffle: false training_opt: backbone: resnext50 batch_size: 512 dataset: ImageNet_LT display_step: 10 1.3.2. YAML-Python 下面直接给出一个例子，基本就按照这个格式去编写就没什么问题了。 import yaml import os # 通常使用这种方式去打开文件并进行读取，这里实际上涉及到Python的IO操作 with open(args.cfg) as f: config = yaml.load(f) # 为了保证编写的一致性和与argparse的一致性使用（整合同个用途或者同个类型的数据），通常会编写update函数将两种类型中的参数整合起来 config = update(config,args) # 然后用字典的方式将config（yaml）中的每一部分要素按照命名读取出来 # 原则是：让读取出来的数据的堆叠层数不要太多，尽量就是一个dict或者一个list把 def update(config,args): # 在这里可以只提取出args中感兴趣的要素，也可以递归调用args中的所有参数 for k,v in args.items(): config[k] = v # or write down some interest elements only config['element'] = 'specific value' 1.3.3. YAML的写入 1.4. JSON json是一种存储和交换文本的语法，类似XML。Link1 经纬师兄这块是按照coco的json格式去整理的文档，同时数据的存储用的是npy,npz { \"employees\": [ { \"firstName\":\"Bill\" , \"lastName\":\"Gates\" }, { \"firstName\":\"George\" , \"lastName\":\"Bush\" }, { \"firstName\":\"Thomas\" , \"lastName\":\"Carter\" }] } 可以看VsCode中的配置文件实际上也是这种格式的： \"todo-tree.highlights.customHighlight\": { \"TODO\": { \"foreground\": \"#2f3542\", \"background\": \"#f6b93b\", \"iconColour\": \"#f39c12\", \"icon\": \"issue-opened\", \"type\": \"line\" }, \"FIXME\": { \"foreground\": \"#2f3542\", \"background\": \"#e55039\", \"iconColour\": \"#e55039\", \"icon\": \"flame\", \"type\": \"line\" } }, 读取的时候实际上也是和键值对一样的读取,用dict, 1.4.1. Json-Python 首先给出一个Json和python的类型对照表 Python Json dict object list,tuple array str string int float int-&float-derived Enums number True true False false None null Python中Json的主要导入和输出的方式主要是使用dumps和loads将python对象编写成json字符串,以及对json字符串在python中编码 dumps import json data = [ { 'a' : 1, 'b' : 2, 'c' : 3, 'd' : 4, 'e' : 5 } ] data2 = json.dumps(data) print(data2) # 使用参数让json数据格式化输出 #!/usr/bin/python data2 = json.dumps({'a': 'Runoob', 'b': 7}, sort_keys=True, indent=4, separators=(',', ': ')) print(data2) loads #!/usr/bin/python import json jsonData = '{\"a\":1,\"b\":2,\"c\":3,\"d\":4,\"e\":5}'; text = json.loads(jsonData) print(text) 载入文件的示例: with open(\"../config/record.json\",'r') as load_f: load_dict = json.load(load_f) print(load_dict) load_dict['smallberg'] = [8200,{1:[['Python',81],['shirt',300]]}] print(load_dict) with open(\"../config/record.json\",\"w\") as dump_f: json.dump(load_dict,dump_f) 1.5. XML 参考一下吉仲师兄的数据处理文件,按照该文件进行数据处理和xml python 读取情景的学习。 1.6. CSV python3：csv的读写_katyusha1的博客-CSDN博客 1.6.1. write 好像直接修改文件后缀进行编写的编码方式会出现一些离奇的问题，所以最好还是调用代码来写入csv 1.6.2. read 需要注意的参数是 quotechar：说明：delimiter是分隔符，quotechar是引用符，当一段话中出现分隔符的时候，用引用符将这句话括起来，就能排除歧义。 首先按照row进行文件的读取，这应该回事比较常见的那种类型。 import csv with open('test.csv'，newline = '') as f: f_csv = f.reader(f,delimiter=default,quotechar = default) for row in f_csv: print(row) # 这种格式读取出来的数据会有一个存放对应的=label，然后剩下的就是每一行数据的每一个 # 可以按照这种方式去根据index 索引对应的数据 [‘class’,‘name’,’sex’,...] [‘1’,‘xiaoming’，‘male’,...] [‘1’,‘xiaohong’，‘male’,...] 按照上面的很容易知道，只读取指定的列就是通过即可 for row in f_csv: print(row[i]) © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 19:13:49 "},"Langs/PyTorch.html":{"url":"Langs/PyTorch.html","title":"5. PyTorch","keywords":"","body":"1. PyTorch Notebook2. Basic Part基础设定部分2.1. Tensor张量计算2.1.1. 两个tensor的数乘2.1.2. 张量命名2.1.3. 类型转换2.1.4. 维度堆叠2.2. 基本的张量函数2.2.1. 选取划窗2.3. Torch环境设置2.3.1. pytorch中的随机种子初始化2.3.2. nn.parameter()2.3.3. nn.Softmax中的dim2.4. 测试、验证模块2.4.1. 基本编写2.4.2. model.eval()和model.train()的区别2.4.3. with torch.no_grad()2.4.4. 模型的保存和读取专题3. GPU相关的设置3.1. 查看GPU状态3.1.1. 设置默认GPU设备3.1.2. 设备基本信息3.2. GPU使用率优化（注意事项）3.2.1. 缓存爆炸问题3.2.2. 运行效率优化3.2.3. 设置使用GPU的方式3.2.4. 设置相应的随机种子3.2.5. CUDA转换3.2.6. 多GPU并行4. CPU4.1. 核心和线程数设置5. 网络定义模块5.1. 数据定义模块5.1.1. 利用TorchVision读取本地数据5.1.2. torch 自定义Dataset后的使用5.1.3. Dataloader中的transformer（）：5.1.4. Dataloader中的参数5.2. 编写模型5.2.1. 模型基本单元5.2.2. 模型参数共享：5.2.3. 网络定义的方式对比分析5.2.4. Detach & detach_5.2.5. 模型调用的Tips5.2.6. Warm-up factor5.2.7. Weight decay（L2）5.2.8. Learning Rate Decay5.3. 损失函数5.3.1. CrossEntropy交叉熵5.4. 优化器设计5.5. 模型参数初始化和架构查看方法5.5.1. children、modules、parameters：5.5.2. 初始化原则：（继续调研）5.5.3. 典型的参数初始化方法5.6. 数据类型和维度5.6.1. 输入数据的通道5.6.2. 标签的形式转换one-hot6. Visualize 可视化部分6.1. Tensorboard in Pytorch6.1.1. Histogram 直方图参数统计6.1.2. Embedding Projection6.1.3. PR_CURVE6.1.4. Add_TEXT6.1.5. ADD_Figure6.2. 可视化神经网络热力图（CAM）6.2.1. 算法原理6.2.2. 代码实现：6.3. BUGs7. DEBUG7.1. 1.ImportError: cannot import name 'PILLOW_VERSION'7.2. 2.模型参数&计算量统计 and Debug输出7.3. 3.PyTorch加载预训练模型7.4. 4.some of the strides of a given numpy array are negative.7.5. 5.读取loader的时候图像的大小不一7.6. 6.bus error dataloader num_worker7.7. 7.bus error：insufficient shared memory（shm）7.8. 8.训练过程中Cache和Memory的占用逐渐升高7.9. 9.梯度爆炸问题，算法没有学习效果7.10. 10.类型转换问题汇总7.11. 11.数据维度不对应问题汇总7.12. 12.取出具体数值时候的问题7.13. 13.CPU占用99%7.14. 14. 预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等）7.15. 15.模型部分： 训练中模型准确率不上升7.16. 16. On entry to SGEMM parameter number 8 had an illegal value7.17. 17. CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call7.18. RuntimeError the derivative for target is not implemented7.19. Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment1. PyTorch Notebook Created by: Aiken H Detail: lang Finished?: No Tags: Code @AikenH 2021 Make My notebook of Pytorch in One File. [ ] torchvision.dataset.ImageFolder [ ] normalize [ ] data.clamp_ 梯度截断 [ ] detach, 取出数据的设定 2. Basic Part基础设定部分 @AikenH 2020 + 2021 this part is about pytorch basic unit, help me to code deep learning better. 2.1. Tensor张量计算 2.1.1. 两个tensor的数乘 计算两个tensor的矩阵乘法，注意其中的batch要相互对应，如果不考虑batch，就是另一个函数 # 简单的分析一下算法的逻辑 # 这是割裂出来batch的矩阵相乘形式 batch1 = torch.randn(10,3,4) batch2 = torch.randn(10,4,5) out = torch.bmm(batch1, batch2) out.size() '''output ans is torch.size([10,3,5])''' # 按位相乘 res = torch.mul(batch1,batch2) view和permute的使用实际上都是不改变原值，要用赋值的方式去做，主要是使用方式要对，一个是按照顺序去做。 2.1.2. 张量命名 NCHW = [‘N’, ‘C’, ‘H’, ‘W’] images = torch.randn(32, 3, 56, 56, names=NCHW) images.sum('C') images.select('C', index=0) 2.1.3. 类型转换 # tensor 与 nd.array进行互换 ndarray = tensor.cpu().numpy() tensor = torch.from_numpy(ndarray).float() # tensor与PIL.IMAGE进行互换 image = torchvision.transforms.functional.to_pil_image(tensor) path = r'./figure.jpg' tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # np.ndarray 与 PIL.Image的互换 image = PIL.Image.fromarray(nd.array.astype(np.uint8)) ndarray = np.asarray(PIL.Image.open(path)) 2.1.4. 维度堆叠 Stack，普通的维度堆叠的测试代码如下 测试代码如下，实际上dim=0就是基本的堆起来，dim=1就是按照行来堆，dim=2就是按照列来堆 a = torch.arange(1,10).reshape(3,3) b = torch.arange(10,100,10).reshape(3,3) c = torch.arange(100,1000,100).reshape(3,3) print('-----------------a----------------') print(a) print('-----------------b----------------') print(b) print('-----------------c----------------') print(c) print('-----------------dim =0----------------') d = torch.stack((a,b,c),dim = 0) print(d.shape) print('the value of d:- {}'.format(d[2,1,0])) print(d) # 也就是说，把单个当成整体直接从上往下堆叠 # 以x[:][:]为构成单元 print('-----------------dim =1----------------') d = torch.stack((a,b,c),dim = 1) print(d.shape) print('the value of d:- {}'.format(d[1,2,2])) print(d) # 将每个的第一个维度，按次序纳出来，同value的堆在一起 # for example：[a[i][:],b[i][:],c[i][:] ]组成新的单元块 # 不，另一种理解，以x[i][:] 为单元 print('-----------------dim =2----------------') d = torch.stack((a,b,c),dim = 2) print(d.shape) print('the value of d:- {}'.format(d[1,2,1])) print(d) # 相应的以x[i][j]为单元构成 list的情况下的维度堆叠测试代码如下 相应的测试代码如下，实际上一般是按照dim=1来进行堆叠 A = torch.randn([3,4,2]) B = [A[:,i] for i in range(A.size(1))] # 这样生成的是一个list,按照我们index的排序 print(A) print(B) C = torch.stack(B,dim=1) print('---------------------result-----------------------') print(C) Cat 实际上应该也是类似的堆叠思路 2.2. 基本的张量函数 torch.split() 划分tensor torch.randperm进行list的乱序处理 # 和shuffle区分，这是另一种乱序的操作 # cat操作 a = [] for i in range(3): a.append(torch.tensor([i,i])) all_inputs = torch.cat(a) # randperm的效果 test1 idx = torch.randperm(all_inputs.size(0)) print(idx) a1, b = all_inputs, all_inputs[idx] print(a1,b) # test2 ， print('-------------------------') # randperm 进行list的shuffle tensor_a = torch.randint(0,10,[8]) print('origin version ', tensor_a) idx = torch.randperm(tensor_a.size(0)) print('shuffle idx ', idx) tensor_b = tensor_a[idx] print('after operation ', tensor_b) .fill_()按照输入的值对张量进行填充 2.2.1. 选取划窗 nn.unfold拆解卷积中的划窗步骤 import torch inputs = torch.randn(1,3,224,224) unfold = torch.nn.Unfold(4,stride=4) output = unfold(inputs) # res output output.size() $ [1,4,3136] # 3136 = (224/4) * (224/4) 2.3. Torch环境设置 2.3.1. pytorch中的随机种子初始化 yTorch 和 Python的随机数生成器就算随机种子一样也不会产生一样的结果。 我们可以这样来设置Pytorch的随机数种子：（通常和GPU一起使用） torch.manual_seed(seed) 2.3.2. nn.parameter() Main idea：parameter的作用，主要是将参数和model绑定在一起，我们就知道这个模型中，可能需要训练的参数有哪些，可以需要进行训练的参数加进去，但是当我们想要freeze it的时候就使用detach或者直接修改require_grad来让参数不在接受训练就好了， require_grad是其中的一个属性。可以结合上面的代码分析。 tensor变量是不可训练的，只有修改成parameter才能进行训练。 自带的网络结构中的一些weight和bias应该都是parameter的变量 2.3.3. nn.Softmax中的dim 其实没那么复杂，就和数据的维度是一样的，我们需要把那一个维度的数据之后的数据全部加起来处理就用哪个维度去做。 IMAGE = N* DATA，dim=1 说明dim = 0 的Channel 是需要被排外的。也就是我们的softmax是基于data进行的。可以找寻源码进行进一步分析解释。 2.4. 测试、验证模块 2.4.1. 基本编写 2.4.2. model.eval()和model.train()的区别 通常在模型测试的时候会执行model.eval()切换模型的状态，而在训练的时候会执行model.train()，model在这两个状态下的区别主要有： 在train状态下会启用BN和Dropout，而在eval不启用这两个模块； 启用BN指的是：用到每一个Batch数据的均值和方差；不启用则指的是使用整体的均值和方差（同时停止更新mean和var） 而对于Dropout来说：启用的时候指的是会随机进行dropout，而关闭的话就会用到全部的网络链接 2.4.3. with torch.no_grad() 上下文管理器，wrap起来的部分不会track grade 主要用于停止autograd模块的工作，被with包裹起来的部分会停止梯度的更新，得到进一步的加速把，因为我们实际上在验证的时候不会执行step()等操作，所以能够节省计算模型梯度的时间。 2.4.4. 模型的保存和读取专题 @Aiken 2020 基于onenote笔记，我们知道关键在于如何自由的读取模型中的参数，并选择性的取出来。 pytorch 模型部分参数的加载_LXX516的博客-CSDN博客_pytorch 加载部分参数 # 至少基于这样的方式我们能把模型中参数的string取出来。 pretrained_dict=torch.load(model_weight) model_dict=myNet.state_dict() # 1. filter out unnecessary keys pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict} # 2. overwrite entries in the existing state dict model_dict.update(pretrained_dict) myNet.load_state_dict(model_dict) 3. GPU相关的设置 @written by Aiken, 2020 this document is about Pytorch‘s CUDA, & GPU setting. 3.1. 查看GPU状态 3.1.1. 设置默认GPU设备 一般使用GPU之前，我们需要知道系统中有多少GPU设备，因为默认的GPU设备是0，而且，大家一般都直接使用这张卡，所以我们如果只使用单卡的话，切换一下默认的GPU设备，能够避免一定的冲突。 # 查看GPU使用状态 $ nvidia-smi # or $ gpustat [--watch] 3.1.2. 设备基本信息 查看是否存在GPU，数量，类型 import torch # 查看是否存在GPU，数量，类型 torch.cuda.is_available() torch.cuda.device_count() torch.cuda.get_device_name(0) 查看指定的GPU的容量和名称 torch.cuda.get_device_capability(device) torch.cuda.get_device_name(device) 设置当前系统的默认gpu_devices，推荐使用os来设置（实际上是命令行中的操作）实际上是系统设定针对当前进程的可见GPU，其他的GPU会对当前的程序隐藏，所以默认的0 os.environ['CUDA_VISIBLE_DEVICES'] = \"id\" #推荐用法 # 可以在vscode的launch.json中设置env 注意事项：该命令需要在所有调用了CUDA的代码、子程序之前，包括import，所以很多代码的import都是在main()中的。 3.2. GPU使用率优化（注意事项） 3.2.1. 缓存爆炸问题 GPU使用途中需要注意的地方，在每次iteration之后记得清除在GPU中占用的memory，cache等，不然有时候会导致缓存和内存的递增和爆炸。 具体操作： torch.cuda.empty_cache() # after every iteration 3.2.2. 运行效率优化 cudnn.benchmark、pytorch论坛 pytorch中文网、zhihu究极分析文 基本使用思路： 在程序的开始，让cudnn花费一点额外的时间，找到适用于当前配置的最佳算法，从而优化运行效率。 注意事项： 但是如果我们的input_size在每个iteration都存在变化的话，那么每一个iteration都要执行一次搜索，反而得不偿失。 具体操作 torch.backends.cudnn.benchmark = true 3.2.3. 设置使用GPU的方式 3.2.4. 设置相应的随机种子 torch.cuda.empty_cache() # part2 设置随机种子 torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) 3.2.5. CUDA转换 使用.cuda()来对模型，数据，Loss进行赋值，或者使用to_devices()来设置到相应的GPU设备 将模型转化到cuda中要在优化器的建立之前执行，因为optimizer是对于模型建立的，对模型执行cuda后已经和原本的参数和模型都不是同一个了，所以一定要在建立优化器之前就对模型进行Cuda 的转化。 是否要对loss转换到CUDA，取决于一下的两种情况： 损失函数是Functional：这样的话只要传入的参数是CUDA的就会再CUDA上计算 损失函数是Class with params：如果类内有参数的话，也要转换到CUDA才能一起在CUDA上计算 if torch.cuda.is_available(): try: loss = loss.cuda() except AttributeError: print('the loss is not cuda-able {}'.format(type(loss))) 3.2.6. 多GPU并行 主要使用的命令nn.DataParallel() model = nn.DataParaller(model,device_ids=None) # 如果不设定id的话，应该是自动指定全部可见的GPU的 4. CPU 偶然会由于pin_memory 的设置来致使CPU的不正常运行（满载等等），并非总是这样。 4.1. 核心和线程数设置 限制或增加pytorch的线程个数！指定核数或者满核运行Pytorch！！！_lei_qi的博客-CSDN博客 import os from multiprocessing import cpu_count # 设置环境变量来控制线程多发的情况 cpu_num = cpu_count() # 核心代码 os.environ['OMP_NUM_THREADS'] = str(cpu_num) # 下面这些应该是不一定药有 os.environ ['OPENBLAS_NUM_THREADS'] = str(cpu_num) os.environ ['MKL_NUM_THREADS'] = str(cpu_num) os.environ ['VECLIB_MAXIMUM_THREADS'] = str(cpu_num) os.environ ['NUMEXPR_NUM_THREADS'] = str(cpu_num) # 从其他资料中可以感觉这条代码应该是和核心代码一样的功能，所以两个写一个应该就可以了 torch.set_num_threds(cpu_num) 5. 网络定义模块 5.1. 数据定义模块 5.1.1. 利用TorchVision读取本地数据 torchvision.datasets.imagefolder() 这个函数实际上能代替我们之前写的函数，但是由于自己写的有一部分统一规则可以使得我们的自定义程度很高，所以目前我们在绝大多数情况下不使用该方法来进行替代。 但是由于是一个重要的函数，我们在这里还是介绍一下该工具的使用方式： 5.1.2. torch 自定义Dataset后的使用 自定义dataset的继承以及后续调用需要注意的是不能忘记将其转换成dataloaer，然后进行iter命令的执行。 也可以用enumerate函数来进行调用，就是记得调用的格式是什么就好 可以参考basicunit中的对shuffle的认知对该函数进行进一步的理解。 # 定义dataset的部分 class RL_AET_Dataset(torch.utils.data.Dataset): def __init__(self): super(RL_AET_Dataset,self).__init__() pass def __len__(self): pass def __getitem(self): pass # 声明和构建部分 要记得使用dataloader train_l_dataset = RL_AET_Dataset(x_l, y_l, args) train_l_dataloader =torch.utils.data.DataLoader(train_l_dataset,batch_size=args['b_s'],shuffle=True,num_workers=args['num_workers'],drop_last=True,pin_memory=True) #调用迭代部分 labeled_loader = iter(train_l_dataloader) #all_label_info = [*next(labeled_loader)] 5.1.3. Dataloader中的transformer（）： 疑惑解答 用compose集成的所有transform，都会应用，有个to_tensor，切to_tensor会自动转换PIL中的channel和数值范围。 compose中的变换组合的顺序关系 PIL处理的图像变换（比如数据增强之类的方法） to_tensor() 处理tensor的方法：normalize 示例代码 data_transforms = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(). transforms.ToTensor(), transforms.Normalize([a,b,c],[A,B,C])]) # 然后直接加入dataset中的参数，或者是我们自定义的部分 # 在dataset中的写法如下，我们可以在自己的dataset中进行定义 if self.transformer is not None: img = self.transform(img) # 具体的源码细节表现如下 for t in self.transforms: img = t(img) return img 5.1.4. Dataloader中的参数 shuffle机制 主要解决问题： 是否每次调用的时候都进行随机的操作，还是只有在初始化的时候才进行随机 两种不同使用Dataloader的方式是否会对shuffle的方式进行区分 结论： 每次对dataloader进行重新调用（重新放到enumerate），或者重新定义iter，都会重新进行shuffle。 num_worker 参考资料1 参考资料2：pytorch中文文档&#x1F447; num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0) 用num_worker个子进程加载数据，所以能够将数据在主进程展示还没有调用到该数据之前就将后续的数据存入RAM，所以在数据读取上会比较快，但是占用的RAM和CPU资源会比较大。 samples: torch.utils.data - PyTorch 1.9.0 documentation 一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系 官方的解释是： sampler (Sampler or Iterable, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with len implemented. If specified, shuffle must not be specified. 定义从数据集（还是最开始的哪个数据集，不能是额外的数据集）中提取样本的策略：是否能通过该Method去实现Hard-Task或者像Meta-Task一样的采样过程呢？从Meta-Transfer-Learning中看来是可以的，可以学习一下它的写法。 collate_fn() collatefn的作用就是将一个batch的数据进行合并操作。默认的collatefn是将img和label分别合并成imgs和labels，所以如果你的__getitem方法只是返回 img, label,那么你可以使用默认的collate_fn方法, 但是如果你每次读取的数据有img, box, label等等，那么你就需要自定义collate_fn来将对应的数据合并成一个batch数据，这样方便后续的训练步骤。 编写collate_fn可以参考qidong的文章主要是接受数据和标签列表，将其整合成一个矩阵的形式; 如果对传参有需求,可以参考lambda的形式或者是类定义的形式去传入 dataload = DataLoader(dataset, lambda x: collate_fn(x, **params)) class collater(): def __init__(**params): self.params = ... def __call(self,datas): # make it a batch in this function, then we will instance this class ... def _helpful_fn(self): ... using collate_fn, we can augment the dataset more flexible. 5.2. 编写模型 5.2.1. 模型基本单元 nn.conv2D： kernel_size[1]应该指的是卷积核的宽（不一定都是正方形的） 5.2.2. 模型参数共享： pytorch：对比clone、detach以及copy_等张量复制操作 # 假设有modela和modelb，我们需要在进行下降的时候执行参数统一， for a_para,b_para in zip(modela.parameters(),modelb.parameters()): b_para.data.copy_(a_para.data) 5.2.3. 网络定义的方式对比分析 @Aiken 2021 主要对比的是ModuleList和Sequtial 结论：通常使用的话，这里我个人推荐使用的是sequtial结合collection中的orderdict来构建的方法，这个方法集成了内部的forward，同时通过`orderdict也能给print带来更好的可视化效果。 但是还是有一些特殊的使用场景我们会用到ModuleList 详解PyTorch中的ModuleList和Sequential 主要区别： nn.Sequential内部实现了forward函数，因此可以不用写forward函数。而nn.ModuleList则没有实现内部forward函数。 nn.Sequential可以使用OrderedDict对每层进行命名，上面已经阐述过了； nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。网络的执行顺序按照我们在forward中怎么编写来决定的 有的时候网络中有很多相似或者重复的层，我们一般会考虑用 for 循环来创建它们，而不是一行一行地写，这种时候就使用ModuleList： class net4(nn.Module): def __init__(self): super(net4, self).__init__() layers = [nn.Linear(10, 10) for i in range(5)] self.linears = nn.ModuleList(layers) def forward(self, x): for layer in self.linears: x = layer(x) return x net = net4() print(net) # net4( # (linears): ModuleList( # (0): Linear(in_features=10, out_features=10, bias=True) # (1): Linear(in_features=10, out_features=10, bias=True) # (2): Linear(in_features=10, out_features=10, bias=True) # ) # ) 基本使用： nn.sequential 可以通过list和*以及add moudle来进行迭代的定义，同时这种定义方式，会方便我们的重复注册 from collections import OrderedDict class net_seq(nn.Module): def __init__(self): super(net_seq, self).__init__() self.seq = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) def forward(self, x): return self.seq(x) net_seq = net_seq() print(net_seq) #net_seq( # (seq): Sequential( # (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1)) # (relu1): ReLU() # (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1)) # (relu2): ReLU() # ) #) nn.ModuleList:与python自带的List不同的地方在于他会自动将网络注册到Parameter中，成为网络，但是需要自己去编写forward过程 class net_modlist(nn.Module): def __init__(self): super(net_modlist, self).__init__() self.modlist = nn.ModuleList([ nn.Conv2d(1, 20, 5), nn.ReLU(), nn.Conv2d(20, 64, 5), nn.ReLU() ]) def forward(self, x): for m in self.modlist: x = m(x) return x net_modlist = net_modlist() print(net_modlist) #net_modlist( # (modlist): ModuleList( # (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1)) # (1): ReLU() # (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1)) # (3): ReLU() # ) #) for param in net_modlist.parameters(): print(type(param.data), param.size()) # torch.Size([20, 1, 5, 5]) # torch.Size([20]) # torch.Size([64, 20, 5, 5]) # torch.Size([64]) 5.2.4. Detach & detach_ 这个模块在后续进行pretrain或者transfer的时候应该会经常被用到，所以这种方法还是需要熟练掌握的 详细的分析介绍 detach是产生一组不需要下降的“Copy”：如果要修改原值的话就要进行赋值操作。 detach_则是修改本身参数的属性（require_gradetc.）执行函数就能将参数修改为不需要下降的情况，不需要执行赋值处理。 5.2.5. 模型调用的Tips 使用list进行多模型的混合调用 由于python默认的是引用赋值，也就是浅拷贝的方式？ 通过list来进行模型的批量构建，通过list来将模型整合起来，是不会使用额外的存储空间的，它们指向同一个地址。基于这样的假设，我们可以基于list来简化代码，通过LOOP来执行，相关的调用操作，比如生成器或者预测之类的，来简化代码结构。 model1 = AET_model(3,4,5,**kwargs) model2 = AET_model(3,4,5,**kwargs) model_list = [model1,model2] if id(model1)==id(model2): print('the address of those model is same, so donot need extra space') # 具体可以简化什么类型的操作： optimizer_list = [] for _, models_t in enumerate(model_list): optimizer_list.append(optim.SGD( models_t.parameters(), lr,mom，wd)) optimizer1 = _[0] optimizer2 = _[1] # like this 5.2.6. Warm-up factor 对于这一部分的概念我还是有些不了解，是否和冷启动和热启动的概念是相关的，如果不是的话，顺便就学习一下冷启动和热启动的概念。 具体解析： neural network - What does \"learning rate warm-up\" mean? - Stack Overflow 关于warmup学习率云中寻雾的博客-CSDN博客 pytorch学习率调整方法（warm up） ，label smooth、apex混合精度训练、梯度累加_xys430381_1的专栏-CSDN博客 神经网络中 warmup 策略为什么有效；有什么理论解释么？ 5.2.7. Weight decay（L2） 实际上就是对权重进行L2正则化，让权重衰减到更小的值，在一定程度上减少模型的过拟合问题，所以权重衰减实际上也叫L2正则化。 具体的数学推导后续将集成到GoodNote笔记上，将正则化单独作为一个模块去整理。 权重衰减（L2正则化）的作用 作用： 权重衰减（L2正则化）可以避免模型过拟合问题。 思考： L2正则化项有让w变小的效果，但是为什么w变小可以防止过拟合呢？ 原理： （1）从模型的复杂度上解释：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合更好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。（2）从数学方面的解释：过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。 image-20201205175236273 内容来自： 正则化方法：L1和L2 regularization、数据集扩增、dropout 5.2.8. Learning Rate Decay 当我们选择了一个合适的lr，但是损失训练到一定程度以后就不再下降了，就在一个区间中来回动荡，可能是出现了一下的问题： image-20201205175605729 对这种问题的解决就是通过学习率衰减来实现的：将学习率随着训练的进行来进行衰减，这个方法就比较直观了。具体的方法描述可以在 ../project_note/训练参数调整策略.md中找到。 也可以参考如下连接：详细理解pytorch的六种学习率pytorch 5.3. 损失函数 nn中自带的Loss Function比如说MSE之类的，计算出来的值本身就已经对batch取了平均值，同时我们进行交叉熵的计算的时候，我们不需要实现对他进行softmax，因为再CE中已经集成了softmax的操作。 5.3.1. CrossEntropy交叉熵 这里会介绍一下Pytorch中的CE损失的具体实现的方法，这里给出三种方式的对比。 import torch # initial data and calculate method input_x = torch.randn((4,5)) label = torch.tensor((1,2,3,4)) cri = torch.nn.CrossEntropyLoss() nll_f = torch.nn.NLLLoss() # output softmax and logsoftmax and pred softamx_x = torch.softmax(input_x,dim=1) logsoftmax_x = torch.log(softamx_x) print(\"softamx_x \\n\", softamx_x) print(\"pre_res \\n\", softamx_x.argmax(axis=1)) print(\"log_softamx_x \\n\", logsoftmax_x) # calculate official ce and NLL print(\"torch ce \\n\",cri(input_x,label)) print(\"nll_cal \\n\", nll_f(logsoftmax_x,label)) # calculate the manual ce loss we cal res = [-logsoftmax_x[i][label[i]] for i in range(len(label))] print(\"manual cal \\n\",sum(res)/len(label)) 可以发现三种方式计算出来的损失是一样的，这就说明了我们在计算的时候要记住，ce中是自己集成了softmax的操作，同时在Nll中是存在了取negative的操作的。按照这个操作手册去实现自己相应的损失函数设计 5.4. 优化器设计 这一部分主要添加一些常见的优化器参数的设置包括SGD和Adam的对应设置，主要介绍一下设置Adam 实际上Adam的设置对于学习率来说没有那么敏感，但是我们还是要了解参数的意思才知道怎么去设置该优化器 5.5. 模型参数初始化和架构查看方法 实际上对参数初始化也就是需要对整体的架构进行遍历，所以这两个会归为一个子课题 参数的初始化方法只要使用如下的方式，无论我们采取那种定义的方式，，都能遍历到其中所包含的所有网络层 # 如果直接在网络定义的时候直接进行初始化 for m in self.modules(): if isinstance(m,nn.Conv2d): nn.init.kaiming_normal_(m.weight,mode='fan_out') if isinstance(m,nn.BatchNorm2d): nn.init.constant_(m.weight,1) nn.init.constant_(m.bias,1) # 如果是在模型定义的外部的话 for layer in model.modules(): if isinstance(layer, torch.nn.Conv2d): torch.nn.init.kaiming_normal_(layer.weight,mode='fan_out', nonlinearity='relu') if layer.bias isnotNone: torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.BatchNorm2d): torch.nn.init.constant_(layer.weight, val=1.0) torch.nn.init.constant_(layer.bias, val=0.0) elif isinstance(layer, torch.nn.Linear): torch.nn.init.xavier_normal_(layer.weight) if layer.bias isnotNone: torch.nn.init.constant_(layer.bias, val=0.0) layer.weight = torch.nn.Parameter(tensor) # 也可以使用其他的方法比如parameters，children 5.5.1. children、modules、parameters： model.modules会遍历model中所有的子层，而children只会遍历当前层，也就是最外层的情况，所以如果要进行参数的初始化的话，最好是用类内或者类外的两种方法来实现初始化 parameter返回的是模型的所有参数，所以初始化最好使用的是`modules，而parameter一般用来初始化参数 用numel与parameters计算参数的个数 #可以简洁的写成下面的形式 #numel()函数本身的作用是返回数组中元素的个数 def count_parameters(model): return sum(P.numel() for P in model.parameters() if P.requires_grad) #帮助理解的结构形式可以表达如下： def count_parameters(model): for p in model.parameters(): if p.requires_grad: ans += p.numel() 5.5.2. 初始化原则：（继续调研） pytorch中的参数初始化方法总结_ys1305的博客-CSDN博客_pytorch 参数初始化 Batch-Normalization：Batch Normalization详解 - shine-lee - 博客园 (cnblogs.com) conv：kaming_normal_ fc：constan_,xvaier bn：normal_\\constant| 5.5.3. 典型的参数初始化方法 EnAET中可以看到参考的源码如下，需要注意的是，BN中只有两个参数，所以不需要进行参数的初始化，或者直接置0、1即可. for m in self.modules(): if isinstance(m,nn.Conv2d): # 计算参数 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0,math.sqrt(2. / n)) elif isinstance(m,nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): nn.init.xavier_normal_(m.weight.data) # what's this method m.bias.data.zero_() 5.6. 数据类型和维度 在算法编写的过程中，数据的类型和维度的对齐和channel是很重要的，在这里也很容易出现很多的bug，在这里做一个信息的汇总 5.6.1. 输入数据的通道 结论：pytorch网络输入图片的shape要求通道是channel_first（通道在前）的，所以如果我们的图片不是这样的话，我们就需要执行相应的变化。 TODO：整理各种数据读取方式读入的channel first 或是 last : skimage,PIL,numpy 整理相应的各种数据类型进行transpose（numpy）的方式 # 也可以使用view函数，但是相应的，view需要计算出各个维度相应的数值 # view（）直接使用的时候不改变原值的大小，permute也不改变，使用的方法不同而已 if img.shape[-1] == 3: img = img.permute(0,3,1,2) 5.6.2. 标签的形式转换one-hot 进行训练之前要将数据转化为onehot的形式，才能输入训练，而且一般因为是batch_size的形式，所以我们需要转化为矩阵形式的onehot，不能用单个label的转化方法。 def make_onehot_single(num,index): '''根据类别数量和index生成single，onehot''' # BTW：scatter方法也能生成one-hot onehot = torch.zeros(num) onehot[index] = 1.0 return onehot # 主要是下面这种方法需要掌握， def make_onehot_array(width,target): '''根据label生成onehot矩阵。 width：类别数 target：具体的labeldata''' try: length = len(target.view(-1,1)) except ValueError: print('the type of target is {} '.format(type(target))) print(target) raise Exception('break down') onehot = torch.zeros(length, width).scatter_(1,target.view(-1,1),1) return onehot 6. Visualize 可视化部分 6.1. Tensorboard in Pytorch @Aiken H 2021 review 之前这一部分的projection和model都没有成功显示，这次在新框架中展示一下。 Visualizing Models, Data, and Training with TensorBoard - PyTorch Tutorials 1.8.1+cu102 documentation 详解PyTorch项目使用TensorboardX进行训练可视化_浅度寺-CSDN博客_tensorboardx 使用 TensorBoard 可视化模型，数据和训练 (apachecn.org) 在pytorch教程中的Projection可以结合后续输出的Feature使用来分析相应的聚类和分类可靠性 可以尝试使用，教程写的很简单易懂。 6.1.1. Histogram 直方图参数统计 一般来说用来统计模型中间的一些参数的分布情况，具体的使用在训练的epoch之间，和val是一个比较类似的机制，具体的代码样例如下： # visualize those parameter as historgram # we can add other model here if i % 10 == 0: for name,param in self.main_model.named_parameters(): self.writer.add_histogram('main_model'+name,param.clone().cpu().data.numpy(),i) pass 6.1.2. Embedding Projection @Aiken H 2021 这一部分可能才是神经网络的特征分布的可视化图。 下面这个是Google的Embedding Projection，需要上传.tsv保存的数据，但是实际上就是Tensorboard上也有集成的功能 Embedding projector - visualization of high-dimensional data Visualizing Data using the Embedding Projector in TensorBoard 6.1.3. PR_CURVE 这里会贴上pr_curve中需要的参数和我们这边编写的示例代码 6.1.4. Add_TEXT 换行失效问题, 这是因为在Tensorboard中这一部分使用的是Markdown的格式, 所以在这里我们在换行符\\n之前, 需要保留两个空格才能实现真正的换行 6.1.5. ADD_Figure 有时候我们会发现我们编写的figure在step中没有全部现实出来, 这是我们可以通过启动命令来展示所有的图片 --samples_per_plugin images=9999 # 999 > the num you want to displ 6.2. 可视化神经网络热力图（CAM） @Aiken2020 为了便于查看神经网络的输出，对于图像的哪一部分更加的侧重，也就是指导网络进行分类的主要是图像的哪些区域，（相应的也可以按照类似的方法查看Attention Network的效果把），就想着可视化一下CAM。看指导分类的高响应区域是否落在核心区域。 参考链接： CAM Pytorch 6.2.1. 算法原理 其计算方法如下图所示。对于一个CNN模型，对其最后一个featuremap做全局平均池化（GAP）计算各通道均值，然后通过FC层等映射到class score，找出argmax，计算最大的那一类的输出相对于最后一个featuremap的梯度（实际上就是最后一个map中哪些对于分类的变化其更大的作用，也就是类似权重的机制），再把这个梯度可视化到原图上即可。直观来说，就是看一下网络抽取到的高层特征的哪部分对最终的classifier影响更大。 Quote: 找到了一篇基于Keras的CAM实现，感谢： https://blog.csdn.net/Einstellung/article/details/82858974 但是我还是习惯用Pytorch一点，所以参考着改了一版Pytorch的实现。其中，有一个地方困扰了一下，因为Pytorch的自动求导机制，一般只会保存函数值对输入的导数值，而中间变量的导数值都没有保留，而此处我们需要计算输出层相对于最后一个feature map梯度，所以参考https://blog.csdn.net/qq_27061325/article/details/84728539解决了该问题。 6.2.2. 代码实现： import os from PIL import Image import torch import numpy as np import cv2 import matplotlib.pyplot as plt def draw_CAM(model, img_path, save_path, transform=None, visual_heatmap=False): ''' 绘制 Class Activation Map :param model: 加载好权重的Pytorch model :param img_path: 测试图片路径 :param save_path: CAM结果保存路径 :param transform: 输入图像预处理方法 :param visual_heatmap: 是否可视化原始heatmap（调用matplotlib） :return: ''' # 图像加载&预处理 img = Image.open(img_path).convert('RGB') if transform: img = transform(img) img = img.unsqueeze(0) # 获取模型输出的feature/score model.eval() features = model.features(img) output = model.classifier(features) # 为了能读取到中间梯度定义的辅助函数 def extract(g): global features_grad features_grad = g # 预测得分最高的那一类对应的输出score pred = torch.argmax(output).item() pred_class = output[:, pred] features.register_hook(extract) pred_class.backward() # 计算梯度 grads = features_grad # 获取梯度 pooled_grads = torch.nn.functional.adaptive_avg_pool2d(grads, (1, 1)) # 此处batch size默认为1，所以去掉了第0维（batch size维） pooled_grads = pooled_grads[0] features = features[0] # 512是最后一层feature的通道数 for i in range(512): features[i, ...] *= pooled_grads[i, ...] # 以下部分同Keras版实现 heatmap = features.detach().numpy() heatmap = np.mean(heatmap, axis=0) heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) # 可视化原始热力图 if visual_heatmap: plt.matshow(heatmap) plt.show() img = cv2.imread(img_path) # 用cv2加载原始图像 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) # 将热力图的大小调整为与原始图像相同 heatmap = np.uint8(255 * heatmap) # 将热力图转换为RGB格式 heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # 将热力图应用于原始图像 superimposed_img = heatmap * 0.4 + img # 这里的0.4是热力图强度因子 cv2.imwrite(save_path, superimposed_img) # 将图像保存到硬盘 6.3. BUGs 如果想要展示出所有step的图片, 我们可以在命令行里执行tensoroard的时候执行下列命令. tensorboard --logdir log/cifar100_resnet18 --samples_per_plugin images=999999 7. DEBUG 7.1. 1.ImportError: cannot import name 'PILLOW_VERSION' PIL版本过高，换低就可以，他不配是一个棘手的问题 pip install Pillow==6.2.2 --user 7.2. 2.模型参数&计算量统计 and Debug输出 用来计算模型构建中网络的参数，空间大小，MAdd，FLOPs等指标，count_params很好写，然后剩下的计算我们交给两个第三方的库来实现：torchstat,thop from torchstat import stat stat(model,(3,224,224)) #that‘s all using it in the eval stage 也可以使用torchsummary来查看各层输出的数据的维度数目 from torchsummary import summary summary(model.cuda(),input_size=(3,224,224),batch_size=1) 相应的Debug还可以使用torchsnooper进行：变量的类型和维度追踪这个模块通过@xxxx修饰器的方法调用在指定的method前面，能够在训练过程中输出一些参数值的类型和数值变化的较为详细的信息。个人理解的最佳使用环境是，用于调试或者监控类型之间的错误。 # 这个package如果没记错的话好像是使用装饰器的方法去进行测试 @... method() 7.3. 3.PyTorch加载预训练模型 具体错误：在于模型Dict中的Key和预训练model中的Key不对应，无法匹配。 Unexpected key(s) in state_dict: \"module.features. ...\".，Expected \".features....\" 问题分析： situation1：可以看到前面多了module这个str，这一般是由于其中一方使用了多GPU训练后直接保存的，也就是DataParallel模式下导致的不匹配问题。 solution1： 参考资料 load模型后去掉多余的参数在事先的时候发现这个方法还是存在问题的，并不是简单的dict封装的结构，所以没法这样简单的进行赋值处理:x: 用空白代替module，暂时还没尝试，但是我觉得会遇到和第一个一样的问题:x: :zap:最简单的方法：加载模型后将模型进行DataParallel，再进行数据的转化，将数据进行并行化。具体的操作如下 model.cuda() # 将ids设置成拥有的GPU即可，但是不知道单GPU的情况可不可以实现这种情况 model = nn.DataParallel(model, device_ids=None) Situation2： 保存模型格式为.pth.tar，无法载入训练好的模型 Solution2： 原因是因为被保存的模型是在高版本的pytorch下实现的，但是再低版本中读取的模型是.pth格式的，就会出现版本冲突。 解决方法如下&#x1F447;： # 在高版本的环境中load model，然后再重新保存，保存的时候添加参数，使得保存成旧版本即可 torch.save(checkpoint,save_path,_use_new_zipfile_serialization=False) # DONE xxx is a zip archive(did you mean to use torch.jit.load()?) 使用低版本的Torch去Load高版本（>1.6）保存的模型（.pth.tar）遇到的问题, 这种错误，主要是模型的版本冲突。 解决办法：在高版本的环境中，重新load模型，然后直接save，在保存的时候添加参数 torch.save(model.state_dict(),model_path,_use_new_zipfile_serialization=False) 就可以保存成.pth的模型，也能在低版本的torch环境中使用了 7.4. 4.some of the strides of a given numpy array are negative. ver：torch1.2 这个问题可能会在后续的版本中被优化。 Situation： https://www.cnblogs.com/devilmaycry812839668/p/13761613.html 问题出现在flat操作中，反向切片[::-1]会导致数据存储在内存上不连续，在旧版本中无法实现，对这样数据进行存储。 Solution1: 所以在执倒排切片的时候执行，img2 = np.ascontiguousarray(img) 使得数据在内存空间上连续。 Solution2: 或者执行倒排切片的时候，直接return img.copy() 7.5. 5.读取loader的时候图像的大小不一 使用Crop对图像进行处理的时候，不注意的话就是会出现这样的问题，图像的size随机改变，导致的输出不统一。也可能是Crop函数写的有问题。 bug info如下 $ RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 182 and 360 in dimension 2 Solution： resize，spp，padding，adaptiveMaxPooling（自适应的pooling，pooling到指定的size（channel除外）） 7.6. 6.bus error dataloader num_worker 原因暂时还不是太清楚，但是我们可以把num_worker设置为0 来解决这个问题.jpg 7.7. 7.bus error：insufficient shared memory（shm） 这种原因通常会在docker环境中出现，由于未指定shm容量大小，比如ipc=host之类的命令，就会导致docker的shm只有64m，于是在运行的时候就会出问题。这种情况下只能重新run docker（目前只找到了这个方法）。 如果要妥协的话，就只能试着减小batch_size。但是随着模型的设计上，这其实不是一个可以逃避的问题，也会增加莫须有的其他成本，所以。 7.8. 8.训练过程中Cache和Memory的占用逐渐升高 主要的体现是：逐渐升高这一点，而不是稳定的情况； 有点玄学，但是在这种情况下，我们在每个iteration结束的时候使用清楚显存的函数，竟然就能进行控制了，虽然我不知道为啥清楚显存的函数会顺便连内存中的cache也一起清除了，但是就是，学。 torch.cuda.empty_cache() 7.9. 9.梯度爆炸问题，算法没有学习效果 梯度爆炸问题，分析可能出现存在的问题： 某一部分的学习参数可能的lr过高，权重过高，导致误差快速传播。 问题的复杂度过高，算法overpower了把。 针对于第一点的话，我们参考工程笔记中的学习率调整策略即可。 如果是问题的复杂度过高，那么可能是问题对于我们的模型来说已经overpower的，我们可能需要去加深网络的层数，或者对网络进行进一步的设计和对数据的分析问题。 7.10. 10.类型转换问题汇总 比如scatter_需要将数据从int32的格式转换成int64，我们要掌握一下在Pytorch中进行数据类型转换的技巧。 Expected object of scalar type Float but got scalar type Double for argument #2 'target' 数据类型不匹配，一个是np.float32,另一个是64 参考解决方案：重要 Expected object of scalar type Long but got scalar type Float for argument 希望得到的是Long型标量数据，但是得到了Float型的数据（实际上可能是我们进行测试的时候使用了小数带来的，但是我们也能将其转化就是了） Longtensor() type(torch.longtensor) RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.DoubleTensor) should be the same RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same问题实际上都是和权重的数据类型不匹配，需要将字节型或者是FLoat型向Weight的数据类型转换，但是可能这里的问题实际上出现在就是我们导入的数据类型是不正确的。还是使用type()命令来进行数据类型的转换，但是关键还是：检查输入数据的类型以及数值范围，同时看看在进行dataloader的时候有没有指定to_tensor的变换等等 参考资料链接 进行数据转换的几种方式 使用函数tensor1.type_as(tensor2)将1的数据类型转换成2的数据类型。 tensor_1 = torch.FloatTensor(5) tensor_2 = torch.IntTensor([10, 20]) tensor_1 = tensor_1.type_as(tensor_2) tensor.type(torch.IntTensor) tensor.long(),tensor.char(),tensor.int(),tensor.byte(),tensor.double() tenosr.to(torch.long) 7.11. 11.数据维度不对应问题汇总 multi-target not supported at问题实际上可以翻译成：维度上和交叉熵损失函数的需求不对应。在使用交叉熵损失函数的时候，target的形状应该是和label的形状一致或者是只有batchsize这一个维度的。如果target是这样的【batchszie，1】就会出现上述的错误 使用squeeze（）函数降低维度 7.12. 12.取出具体数值时候的问题 RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy()对于输出的结果要转换成具体的数值的时候，如果我们后续还需要这个数值的梯度，就不能转换到cpu后再转换到numpy,就好比说，我们要取出Loss的时候，我们可以直接使用item()取出具体的数值，而不需要转到CPU上 7.13. 13.CPU占用99% 问题描述：使用torch自带的dataset中的cifar10的时候，在每个epoch结束的时候，CPU占用率高达99%，并不随着num_workder而改变，问题可能由于pytorch开辟了太多的线程 windows10下pytorch的GPU利用率低，占用率低_stay_zezo的博客-CSDN博客 可能是由于GPU运算太快了，启用了多线程进行加载数据，这种时候启用pin_memory=true 能起到一定的作用把，加快一点数据读取。 最终解决方案 ：pin-memory=false 反正原因很神奇，但是最终就是因为这个解决的，可能是因为memory超了，所以每次都需要重新empty_cache 重新装进页，所以反而加重了CPU的负担 7.14. 14. 预测值全为0，模型收敛到奇怪的地方，损失保持一致（全均等） 这种情况通常是由于模型设计中存在一点问题： 比如这次是由于模型中fc后面添加了relu，这样导致输出的负值全被抑制了，导致学习出现了严重的错误后果。 7.15. 15.模型部分： 训练中模型准确率不上升 由于框架已经验证过是可以进行正常训练的，在这种情况下出现模型的准确率不上升可能是由于模型本身设计（内部代码编写）上的问题。 7.16. 16. On entry to SGEMM parameter number 8 had an illegal value Tracing failed sanity checks! Graphs differed across invocations! fc的问题，输入fc和对应的网络输入层不一致，检查阶段数目和feature输出的特征维度 7.17. 17. CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call 这个问题的出现的根本原因在于： 维度不匹配：标签的dimension 超出了全连接层最后输出的dimension，这一部分错误的触发，和Loss的计算，Acc的计算，有着强烈的相关关系。 为了解决这个问题，我们在训练相关的验证和训练环节，需要保持训练数据集和验证数据集在类别数目上的一致性，而在我们需要对数据集外的数据进行测试的时候，我们避免进行Loss的计算，在对Acc进行计算的时候，也尽量避免Torch中的自有库，避免产生该类的问题/ 7.18. RuntimeError the derivative for target is not implemented 问题通常出现在损失计算的过程中，这个错误是由于我们在损失中的第二项 targets不应该有梯度，但是在这个地方却存在梯度导致的. 在这里我们可以通过仅仅取出 tensor的data或者使用detachandcopy来进行数值的传递 7.19. Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment 该错误是由deepcopy和require_grad, require_fn同时构成, 如果我们对一个需要计算梯度的非叶子节点进行deepcopy就会触发这个错误。 如果我们需要对这个数据进行存储的话，我们可以执行 save = copy.deepcopy(feature.data.cpu().numpy()) © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-15 16:00:54 "},"Papers/notes.html":{"url":"Papers/notes.html","title":"Chapter3 Papers And Algos","keywords":"","body":"1. Paper Reading1. Paper Reading @AikenHong 2021 阅读论文和算法的时候，对于重要的论文，最好实在记录笔记的同时添加对应的编码实现，而粗浅的论文解读，可以主要提取关键思路，通过阅读解析来进行整理和归纳。 此外，这里会整理一些关于论文阅读中的重要参考资料, 或者是对应的一些解读的网站: Code： CV论文解读 -> 对应的代码复现 | SSL Lightly 实现 | PT-img-model Blogs： Lils'Blog | 刘建平s' Blog -> Code | Videos： 李沐B站 | 李沐笔记 | 李宏毅 Algorithm: TheAlgorithms | © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-01 00:57:21 "},"Papers/Component.html":{"url":"Papers/Component.html","title":"0 Component","keywords":"","body":"1. Component1. Component @AikenHong 2021 In this part we will introduce the Component of the Neural Network. 介绍关于神经网络构建过程中的重要组成部分以及一些基本概念。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-30 00:28:22 "},"Papers/Involution.html":{"url":"Papers/Involution.html","title":"0.1 Involution","keywords":"","body":"1. Involution:Inverting the Inherence of Convolution for Visual Recognition1.1. Intro 引子1.2. 基本思想1.3. 要点分析1.3.1. 生成FeatureMap对应Size的Kernel1.3.2. 与Self-Attention的联系1.3.3. Mathematical1. Involution:Inverting the Inherence of Convolution for Visual Recognition @Aiken 2021-4-8 Ariticle ；Paper；:star:Code； ZHIHU 1.1. Intro 引子 提出了一种新的神经网络算子（operator或op）称为involution，它比convolution更轻量更高效，形式上比self-attention更加简洁，可以用在各种视觉任务的模型上取得精度和效率的双重提升。 通过involution的结构设计，我们能够以统一的视角来理解经典的卷积操作和近来流行的自注意力操作。 1.2. 基本思想 将传统Convolution Kernel 的两个基本特性： 空间不变性：在同个通道的HW上共享3*3的卷积系数，参数共享； 通道特异性：在每个通道上有特异的卷积核，最终使用1*1 like的方式来进行通道间的整合 反对称的修改成： 空间特异性： 对每个Feature有对应size $H·W·K·K·G | G G表示Involution操作的分组数，如果需要下采样，就需要接步长为2的平均池化层，最终可以得到，实际上是一个分组卷积的方式，也就是说，我们K个一组的共享一个Kernel。用G去切分C，最终组合起来 通道不变性：对每个通道之间共享这样的kernel，然后做简单的线性整合，对每个不同的channel有相同的处理方式。 传统的卷积基于邻域相关性的思想，同时旨在同一个channel中用单一的角度去分析特征，所以有空间不变性核通道特异性的这两个特征。 而Involution实际上更像是Self-Attention这种思路，通过Whole-Size的Kernel，执行一个特异性处理？ 1.3. 要点分析 这一部分主要介绍一些实现上的技术/理论要点： 1.3.1. 生成FeatureMap对应Size的Kernel 通用的公式如下，我们可以自定义对应的Kernel生成Function，这是算法的开放性和潜力所在。 \\mathbf{H}_{i,j} = \\phi(\\mathbf{X}_{\\Psi_{i,j}}) \\\\ \\Psi_{i,j} 是邻域的一个index集合，\\mathbf{X}_{\\Psi_{i,j}}是包含i,j的邻域的一个patch 其中可能会包含一些线性变换和通道缩减之类的变换，一种简单的实例化可以由下图来理解。 对某个index，首先转化生成对应的$K^2$，对应的Value，然后通过加权整合得到最终的OutputValue， Channel 数的放射就又我们的对应生成的Kernel数去控制。 有点NIN那味了，反正就是嵌套，架构，用MLP得到Kernel，用Kernel进行降维和信息交互。 The Author Says:&#x1F447; 无论是convolution，self-attention还是新的involution都是message passing和feature aggregation的组合形式，尽管外表各异，本质上没有必要割裂开来看。 1.3.2. 与Self-Attention的联系 将Self-Attention的QKV展开成WX的形式，可以发现实际上Involution是Self-Attention的一个General的表达形式， self-attention中不同的head对应到involution中不同的group（在channel维度split） self-attention中每个pixel的attention map $QK^T$对应到involution中每个pixel的kernel 同时两者在操作后都会加一个线性变换和残差链接，这和Involution中的对应BottleNet也存在一致 Position encoding self-attention中的计算是loacation-agnostic的所以需要进行position-encoding，但是involution，生成的元素本身就是按照location排列的，所以不需要进行位置编码。 此外，Involution保留了CNN中locally的优先特性。： 因此，我们重新思考self-attention在backbone网络结构中有效的本质可能就是捕捉long-range and self-adaptive interactions，通俗点说是使用一个large and dynamic kernel，而这个kernel用query-key relation来构建则并不是必要的。另一方面，因为我们的involution kernel是单个pixel生成的，这个kernel不太适合扩展到全图来应用，但在一个相对较大的邻域内应用还是可行的），这同时也说明了CNN设计中的locallity依然是宝藏，因为即使用global self-attention，网络的浅层也很难真的利用到复杂的全局信息。 所以我们所采用的involution去除了self-attention中很多复杂的东西，比如我们仅使用单个pixel的特征向量生成involution kernel（而不是依靠pixel-to-pixel correspondence生成attention map），在生成kernel时隐式地编码了pixel的位置信息（丢弃了显式的position encoding），从而构建了一个非常干净高效的op。 1.3.3. Mathematical 画一下计算图来看看实际上是怎么运行的，这里最后的size变换还没弄清楚计算的规则 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 16:56:38 "},"Papers/Pooling.html":{"url":"Papers/Pooling.html","title":"0.2 Pooling","keywords":"","body":"1. Pooling2. DownSampling：Pooling的全面调研2.1. 池化的根本目的（Motivation）2.2. 主流的池化方法2.2.1. Average Pooling 平均池化2.2.2. Max Pooling 最大值池化2.2.3. Mixed pooling2.2.4. L_p pooling2.2.5. Stochastic Pooling2.2.6. Spatial Pyramid Pooling （SPP）2.2.7. YOLO v3 变体2.2.8. SPP有效的原因分析2.2.9. Region of Interest Pooling （ROI Pooling）2.2.10. ROI Align的改进2.3. 新颖特殊的池化方法2.3.1. 中值池化2.3.2. 组合池化2.3.3. Multi-scale order-less Pooling MOP池化2.3.4. NetVLAD Pooling2.3.5. 双线性池化2.3.6. UnPooling上采样操作2.3.7. 光谱池化2.3.8. 基于排名的均值池化2.3.9. 基于权重的池化2.3.10. Edge-aware Pyramid Pooling1. Pooling Detail: survey Finished?: Yes Tags: Paper 2. DownSampling：Pooling的全面调研 @Aiken 2021 笔记摘录： 深度神经网络中的池化方法：全面调研（1989-2020） - 知乎 ；相同论文的简单中文Version 16页综述，共计67篇参考文献。网络千奇百怪，但基础元素却大致相同！本文全面调研了1989至2020年一些著名且有用的池化方法，并主要对20种池化方法进行了详细介绍（这些方法，你都知道么？） 注1：文末附【计算机视… 来自 https://zhuanlan.zhihu.com/p/341820742 原文：《Pooling Methods in Deep Neural Networks, a Review》 整合2 2.1. 池化的根本目的（Motivation） 卷积神经网络是DNN的一种特殊类型，它由几个卷积层组成，每个卷积层后都有一个激活函数和一个池化层。 池化层是重要的层，它对来自上一层的特征图执行下采样，并生成具有简化分辨率的新feature maps 。该层极大地减小了输入的空间尺寸。 它有两个主要目的。 首先是减少参数或权重的数量，从而减少计算成本。 第二是控制网络的过拟合。 池化可以增加网络对于平移（旋转，伸缩）的不变性，提升网络的泛化能力。 增大感受野； 降低优化难度和参数数目， 理想的池化方法应仅提取有用的信息，并丢弃无关的细节。 特征不变性、特征降维、在一定程度上防止过拟合，更方便优化 2.2. 主流的池化方法 2.2.1. Average Pooling 平均池化 没啥好说的，就是每个block取一个均值。如下图所示：更关注全局特征 2.2.2. Max Pooling 最大值池化 更关注重要的局部特征 image-20210219153154458 2.2.3. Mixed pooling 在max、average pooling中进行随机选择，来组合pooling 2.2.4. L_p pooling 作者声称这个泛化能力比Max Pooling要好，输入的平均值权重（也就是和分之一）来进行，算是推广的公式。 s_j = (\\\\frac{1}{|R_j|}\\\\sum_{i \\\\in R_j}{a_i^p})^{1/p} 2.2.5. Stochastic Pooling feature_map中的元素按照其概率值大小随机选择，元素被选中的概率与数值大小正相关，这就是正则化操作了。 image-20210219160011182 2.2.6. Spatial Pyramid Pooling （SPP） SPPNet在RCNN之后提出的，用于解决重复卷积计算和固定输出的问题，具体的方法是：在Feature_Map中通过Selective Search获得窗口然后输入CNN中。 这个池化方法实际上就是多个空间池化的组合，对不同的输出尺度采用不同的划窗大小和步长来确保输出的尺度相同，同时能够融合多种尺度特征，提供更丰富的语意信息，常用于： 多尺度训练 目标检测中的RPN 实际上也就是（全图pooling一次，全图分成22块Pooling，全图分成44块以后 做Pooling，然后就是固定尺寸的了，前面的输出是256-d 然后就是（4+16+1）* 256 最后的特征 image-20210219160238878 2.2.7. YOLO v3 变体 在YOLO v3中，有一个网络结构中的yolo-v3-spp比原本的准确率更高，具体的cfg如下： ### SPP ### [maxpool] stride=1 size=5 [route] layers=-2 [maxpool] stride=1 size=9 [route] layers=-4 [maxpool] stride=1 size=13 [route] layers=-1,-3,-5,-6 ### End SPP ### 这里的SPP是原本的SPPNet的变体，通过多个Kernel Size的maxpool 将最终得到的feature map进行concate，得到新的特征组合： 2.2.8. SPP有效的原因分析 从感受野角度来讲，之前计算感受野的时候可以明显发现，maxpool的操作对感受野的影响非常大，其中主要取决于kernel size大小。在SPP中，使用了kernel size非常大的maxpool会极大提高模型的感受野，笔者没有详细计算过darknet53这个backbone的感受野，在COCO上有效很可能是因为backbone的感受野还不够大。 第二个角度是从Attention的角度考虑，这一点启发自CSDN@小楞（链接在参考文献中），他在文章中这样讲： 出现检测效果提升的原因：通过spp模块实现局部特征和全局特征（所以空间金字塔池化结构的最大的池化核要尽可能的接近等于需要池化的featherMap的大小）的featherMap级别的融合，丰富最终特征图的表达能力，从而提高MAP。 Attention机制很多都是为了解决远距离依赖问题，通过使用kernel size接近特征图的size可以以比较小的计算代价解决这个问题。另外就是如果使用了SPP模块，就没有必要在SPP后继续使用其他空间注意力模块比如SK block，因为他们作用相似，可能会有一定冗余。 2.2.9. Region of Interest Pooling （ROI Pooling） 参考链接：原理以及代码实现；Some Detail 以及Align的改进；Best One 对于ROI pooling 的讲解首先要从目标检测的框架出发，帮助理解， 目标检测分为两步： region proposal：输入image，找到所有可能的object的位置（bounding box），也就是ROI，在这过程中可能用到滑窗和selective search。 final classification：确定上阶段的每个region proposal是否是目标类别，或者背景 这样的框架存在问题： 大量的ROI要进行计算，就很难实时监测，也无法做到E2E 使用ROI Pooling进行简化，输入和作用如下： 从多个具有卷积核池化的深度网络中获得固定大小的Feature-Map； 对不同尺寸的ROI进行处理，能得到统一的尺寸。 一个表示所有ROI的N*5的尺寸，N是数目，5维度分别是Index，左上角坐标，右下角坐标 具体实现的操作： 根据输入image，将ROI映射到feature map对应位置； 将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）； 对每个sections进行max pooling操作； 这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。ROI pooling 最大的好处就在于极大地提高了处理速度。 下图大黑框是对应的ROI，输出最后的要求是2*2，基于下面的划分再进行maxpooling即可。 2.2.10. ROI Align的改进 ROI pooling在映射的时候出现小数，这是第一次量化，在每个roi中选取多少个采样点进行max pooling也会出现小数。这样的处理可能会丢失数据，降低了模型的精度 ROI Align并不需要对两步量化中产生的浮点数坐标的像素值都进行计算，而是设计了一套优雅的流程。如图2，其中虚线代表的是一个feature map，实线代表的是一个roi(在这个例子中，一个roi是分成了2*2个bins)，实心点代表的是采样点，每个bin中有4个采样点。我们通过双线性插值的方法根据采样点周围的四个点计算每一个采样点的值，然后对着四个采样点执行最大池化操作得到当前bin的像素值。 RoI Align做法：假定采样点数为4，即表示，对于每个2.97 x 2.97的bin，平分四份小矩形，每一份取其中心点位置，而中心点位置的像素，采用双线性插值法进行计算，这样就会得到四个小数坐标点的像素值。 实际上就是用双线性插值来取代了ROI Pooling的量化过程。 2.3. 新颖特殊的池化方法 这一部分的池化方法存在着一些特殊的特性，在实际需要的时候再进行仔细的研究，但是可以将大体的特征简单的描述一下，方便后续寻找。 2.3.1. 中值池化 与中值滤波特别类似，但是用的特别少，中值池化也具有学习边缘和纹理结构的特性，抗噪声能力比较强。 2.3.2. 组合池化 就是将max 和 average concate或者add起来。 2.3.3. Multi-scale order-less Pooling MOP池化 基于多尺度的池化方式，提升了卷积网络的不变性同时没有破坏卷积网络的可鉴别性，分布从全局与局部池化中提取特征，图示与说明如下： 2.3.4. NetVLAD Pooling NetVLAD是论文《NetVLAD: CNN Architecture for Weakly Supervised Place Recognition》提出的一个局部特征聚合的方法。 2.3.5. 双线性池化 Bilinear Pooling是在《Bilinear CNN Models for Fine-grained Visual Recognition》被提出的，主要用在细粒度分类网络中。双线性池化主要用于特征融合，对于同一个样本提取得到的特征x和特征y, 通过双线性池化来融合两个特征(外积)，进而提高模型分类的能力。 2.3.6. UnPooling上采样操作 1.在Pooling（一般是Max Pooling）时，保存最大值的位置。 2.中间经历若干网络层的运算。 3.上采样阶段，利用第1步保存的Max Location，重建下一层的feature map。 UnPooling不完全是Pooling的逆运算，Pooling之后的feature map，要经过若干运算，才会进行UnPooling操作；对于非Max Location的地方以零填充。然而这样并不能完全还原信息。 2.3.7. 光谱池化 图像池化不光发生在空间域，还可以通过DFT变换，在频域空间实现池化，一个使用光谱池化最大池化的例子如下： 2.3.8. 基于排名的均值池化 Rank-based Average Pooling 这种池化方式的好处事可以克服最大池化与均值池化方式的不足 S_j = \\\\frac{1}{t}\\\\sum_{i\\\\in R_{j,r_i 2.3.9. 基于权重的池化 2.3.10. Edge-aware Pyramid Pooling Survey_NIPS 中国预讲会.md © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:36 "},"Algorithms/HardTask.html":{"url":"Algorithms/HardTask.html","title":"0.3 HardTask","keywords":"","body":"1. Trick：Hard Task1.1. 基本思路1.1.1. 对比Adaboost1.1.2. 具体实现1. Trick：Hard Task @Aiken 2020 思路来源于Meta-Tranfer-Learning，基本思路是在Meta-Learning的每一次Meta-Test的时候，会从预训练错误率比较高的Task中再次采样，增加那些task的训练次数。也就是难题多做的策略。 1.1. 基本思路 1.1.1. 对比Adaboost 这样的思路其实和AdaBoost的想法是有一定的异曲同工之妙的，或者说其实就是AdaBoost的思路： Adaboost 参考笔记，从该笔记中我们可以看到，AdaBoost的基本思路如下： Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器. 和Meta-Transfer-Learning对比一下，我们可以发现，这个方法实际上就是讲Transfer Learning的与训练网络当成弱学习器1，然后通过弱学习器1的训练样本权重，来增大Hard-Task的配比（也就是增加任务的权重）完全一致。 1.1.2. 具体实现 实现上主要是，样本sample的过程，就是如何在进行参数选择后和原本的Dataloader，结合起来。在这里我们主要参考MTL中的方法，进行网络的构建处理。 第一部分：sampler构建，为了后续Dataloader中进行数据的采样，需要构建一个这样的sampler，关键在于index的对应关系，以及最后输出的是index的集合。 import torch import numpy as np # 注意的点，我们需要确定我们batch数目，cls数目和每次每个cls选出多少个数据per # 紧接着定义一个sample，sample输出的是对应原dataset中的数据的index， class CatagoriesSampler(): def __init__(self, label, n_batch, n_cls, n_per): self.n_batch = n_batch self.n_cls = n_cls self.n_per = n_per label = np.array(label) # 根据不同的label输入情况，我们可可能需要找到每个label对应的样本的index，将其整合在一起。如下（option） self.m_idx = [] for i in range(max(label)+1): idx = np.argwhere(label==i).reshape(-1) idx = torch.from_numpy(idx) self.m_idx.append(idx) def __len__(self): # 要注意一下这里数据的长度是根据我们要输出的batch数目决定的 return self.n_batch def __iter__(self): # 直接定义每次采样的时候的batch输出 for i_batch in range(self.n_batch): batch = [] classes = torch.randperm(len(self.m_idx))[:self.n_cls] for c in classes: # 随机选择出的类标签 l = self.m_idx[c] # 随机选择样本 random_pos = torch.randperm(len(l))[:self.n_per] batch.append(l[random_pos]) # stack t and reshape的作用&#x1F447; # stack 变成n_cls * n_per , t转置，reshape（-1）变成行向量 batch = torch.stack(batch).t().reshape(-1) yield batch 第二部分：直接调用部分 其实就是很简单的Dataloader中就有这个参数设置，只需要定义好sampler就没什么太大的问题了。 self.trainset = Dataset('train', self.args) self.train_sampler = CategoriesSampler( self.trainset.label,self.args.num_batch, self.args.way, self.args.shot+self.args.train_query) self.train_loader = DataLoader( dataset=self.trainset,batch_sampler=self.train_sampler, num_workers=8, pin_memory=True) # 关键的地方在于最后一样的batch_sampler，这个在pytorch的dataload文档中分析过，就是每次会按这个规则在这里采样数据出来，一起训练。 第三部分：Hard-Task的选取 以什么形式或者标准来对Hard-Task进行选择，以及构建这个label list，因为我们知道，很多时候dataloader是不输出index的。 本文作者tmd直接偷懒，直接用数据集的label，也就是根本就不是Hard-Task的处理 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-06 23:41:31 "},"Papers/Attention.html":{"url":"Papers/Attention.html","title":"0.4 Attention","keywords":"","body":"1. ATTENTION MECHANISM1.1. What’s Attention In Deep Learning1.2. What’s Wrong with Seq2Seq Model1.3. Born for Translation1.4. A Family of Attention Mechanism1.4.1. Summary1.4.2. Self-Attention1.4.3. Soft vs Hard Attention1.4.4. Global vs Loacal Attention1.5. Neural Turing Machines1.5.1. Attention Mechanisms1.6. Pointer Network1.7. Transformer Introduction1.7.1. Key,Value and Query1.8. Encoder-Decoder and Taxonomy of Attention1.9. Transformer and Self-Attention1.10. Reference1. ATTENTION MECHANISM @Aiken 2020.9.16 对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。 基本索引，根据论文和工程需求实现： 1.TODO-List： 根据Lil’Log的Attention？Attention！进行初步的整理 各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。 其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。 1.1. What’s Attention In Deep Learning 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。 可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。 简而言之，深度学习中的注意力就是针对不同问题下的重要性权重的向量，比如我们根据关联性，给上面的每个单词赋予一个相关性的向量权重，然后基于注意力加权后的总和作为目标的近似值。 1.2. What’s Wrong with Seq2Seq Model seq2swq旨在将再encoder-decoder的架构下，将输入序列转换为新序列，对两个序列的长度没有要求。 Encoder：将输入序列转换成固定长度的context vector，用来概括整个源序列的信息 Decoder：使用context vector初始化，来进行解码（转换） 这样的Encoder和Decoder架构都是Recurrent Neural Networks，就像LSTM和GRU架构。 缺陷：固定长度的context vector可能会导致序列信息的缺失，同时可能会无法记住长句子，同时也会丢失时序的对齐信息。所以Attention就诞生了。 :question: 1.3. Born for Translation 这几个部分的研究都是基于NMT自然机器翻译，来进行分析的（文本非图像） 原本的E-D是通过Encoder的最后一个hidden states构建单个Context Vector，而Attention 做的就是在Context Vec和Source之间建立一个Shortcut（简单的Feed Forword Network），而源和目标的对齐由上下文向量来学习和控制。而上下文中应该consumes（包含）几个维度的信息 Encoder的hidden States； Decoder的hidden States； Source 和 Target之间的对齐信息； Encoder Decoder实际上都是RNN的架构，S实际上就是H，隐层状态，Decoder对EOS输出了一个初始的预测Y以后，推进Decoder的进程。 双向的recurrent network（bidirectional RNN）使得Encoder隐态同时考虑前后单词。而在Decoder中 st = f(s{t-1},y_{t-1},c_t) 其中上下文向量c_t是输入序列的隐态之和，通过alignment score来进行加权。 $a_{t,i}$将作为输入i对输出t的隐态加权（相关性），在网络中共同训练，通过tanh来进行激活处理。Score则使用下面的策略： \r score(st,hi)=v_a^Ttanh(Wa[st;hi])\r 其中V, W都是对齐模型中学到的参数 最终对齐完成以后就会是： 1.4. A Family of Attention Mechanism 1.4.1. Summary 由于这个良好的理论基础和实现效果，以及对序列的良好兼容性，这样的算法就被很快的拓展到了计算机视觉的领域中。（学 科 交 叉） 这些不同的score方法实际上就是对每个position上的input和当前位置（通常用上一个时刻的state）来进行相关性建模，针对这些相关性建模，来确定输入对于下一个状态的影响因子，也就是得到一个context向量。这实际上就是注意力机制的核心理念把。 Referred to as “concat” in Luong, et al, 2015 and as “additive attention” in Vaswani, et al, 2017.(^) It adds a scaling factor 1/n−−√1/n, motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning. 下面对一些更广泛类型的注意力机制做一个摘要性的总结 1.4.2. Self-Attention 这个已经在我的Onenote中整理过了，大概看看就好。这里只讲了一些应用上的点， 自我注意，内部注意力，也就是自我内部关联的注意力关系获取； 1.4.3. Soft vs Hard Attention Image Caption《show, attend and tell》，CNN获取特征，LSTM解码器利用卷积功能来逐一生成单词。通过Attention 来学习权重，可视化如下： 根据注意力是访问整个图像还是访问图像的一个patch来定义hard和soft attention。 Soft Attention：就是普通的全图Attention，将权重放在图像的所有色块上，基本没啥区别 Pro: 模型平滑容易微分 Con：expensive when the source image is large Hard Attention：一次只处理一小块图像，其实就相当于用0/1去对图像区域进行硬编码，只对一部分的图像进行处理，但是这种方式的技术实现我还是没什么概念，后面可以专门研究一下。 Pro：推理计算需求比较小 Con：不可微分，需要用更复杂的技术来进行训练（例如variance reduction or reinforcement learning） 1.4.4. Global vs Loacal Attention Global实际上就和soft是类似的，而局部注意力机制更像是hard和soft之间的一个融合，对改进hard使其可以微分； the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector. 后面提到了一些神经图灵机的内容就是一些基本的计算机制，实际上可能是启发LSTM设计擦除算法设计的根源。 1.5. Neural Turing Machines 1.5.1. Attention Mechanisms 可以将权重的看作一个神经图灵机的寻址权重：（基于位置或者基于内容的两种方式） Content-based addressing: 基于内容寻址的权重设置从输入行和存储行提取的键值向量之间的相似度来创建关注向量：权重的具体计算通过余弦相似度后进行softmax归一化来进行权重的分配。 另外通过β系数来放大或者减弱分布的焦点。 Interpolation 通过interpolation gate scalar $g_t$ 将生成的上下文向量和上一步生成的注意力权重进行混合 \r w^g_t = g_tw^c_t + (1-g_t)w_{t-1}\r Location-baesd addressing 基于位置的寻址，对注意力向量中不同位置的值进行求和，通过对允许的整数偏移位置中的权重来进行参数加权。这相当于是一个一维卷积来测试偏移量。 然后对注意力的分布进行锐化处理，使用γ参数 完整的注意力workflow为： 1.6. Pointer Network 解决的是输入输出都是顺序数据的问题。 1.7. Transformer Introduction Attention is all u need ：提出的重要的Transformer的这种架构，使得算法能够完全基于注意力机制，而不需要序列对齐的recurrent Network。 1.7.1. Key,Value and Query 这一块还是详细解读一下，这里说的太粗略了，没讲清楚。 Transformer的主要重要的架构在于multi-head和self-attention，Transformer将输入堪称一个key-value pairs，均为输入长度n。 \r Attention(Q,K,V)=softmax(\\frac{Q K^⊤} {\\sqrt{n}})V\r Key Value Query 到底都是些什么东西。 1.8. Encoder-Decoder and Taxonomy of Attention 基本的encoder-decoder是基于对序列处理的问题提出的，通常情况下针对的是输入输出都是序列的计算模型。下图a展示了典型的E-D机制，以及加入了self-attention的情况b。（文中提到了一些E-D机制的问题） Seq2Seq的RNN序列模型Encoder需要把之前的输入最终转化成单一的ht（定长），可能会造成序列信息的丢失，同时他只能平权的考虑输入，不能有所重点。同时对于时序对齐信息也会丢失，这对结构化特别重要 注意力机制，在实现上就是通过在体系架构中，嵌入一个额外的前馈神经网络，来进行学习额外的参数（作为Decoder的补充输入，作为序列和之前信息的补充），而且通常，这个前馈的神经网络是需要和encoder-decoder协同训练的，也就是需要和整体网络共同训练。 训练层面是否存在一些特殊的情况，对于一些特殊的Attention 模型，是否需要一些特殊的训练机制。这点如果看到的话，建议需要整理一下 在Survey中，对注意力机制基于多种标准进行了分类，具体的分类情况可以依下图所示，还有一些具体方法的实现。 对于几种分类方式，可以参考几个译文的解读，但是我觉得说的并不清楚，后续就分别针对各种方法进行分析吧。 BTW：在RNN等循环结构不能并行化处理的条件下，提出的类似AM的Transformer（Attention is all u need）结构，他的encoder和decoder由两个子层堆叠而来：Feed Forward Network 和 Multi-head self attention。 Position-wise FFN：获取输入的顺序，在encoder阶段，对于每一个token既生成content embedding也生成position encoding。 Multi-Head Self-Attention：在每个子层中使用self - attention来关联标记及其在相同输入序列中的位置，几个注意力层并行堆叠，对于相同的输入有不同的线性转换。这使得模型能够捕获输入的不同方面，提高表达能力。 1.9. Transformer and Self-Attention 参考论文以及参考资料1 AND 《Attention is all you need》 Self-Attention ： 下图这一段整的明明白白，把整个框架说的比较明白，对输入的embedding（分别做3次卷积，图像输入的情况），分成Query，Key，Value，然后如图进行后续的操作 图2就表示了在CV领域，为什么需要将输入做完卷积以后再成进去，而其中的scale由于图片和序列是不一致的，他们的size本来就是统一的（基本规范的数据集中），那么就可以省略掉这一步，从而直接进行softmax，相当于在预处理的时候已经进行了这样的归一操作。 self-attention 是 Transformer的基础，通过多个Self-Attention组成的Multi-head Attention 就是Transformer模型的基本结构。 Multi-head Attention 1.10. Reference 参考资料 Attention机制详解2：self-attention & Transformer origin document of ↑ Attention的数学原理 Survey: 《An Attentive Survey of Attention Models》 mendeley 论文翻译和解读1 :zap: 论文解读和翻译2 论文解读和翻译3 Transformer: 《Attention is all you need》Mendeley &OneNote © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-30 22:13:23 "},"Papers/EfficientNet.html":{"url":"Papers/EfficientNet.html","title":"0.5 EfficientNet","keywords":"","body":"1. Efficient Net2. Efficient Net V12.1. Motivation and Method2.2. Experience Detail2.2.1. 实现中的问题：1. Efficient Net Desc: Backbone Finished?: Yes Tags: Paper URL1: https://arxiv.org/pdf/1905.11946.pdf URL2: https://arxiv.org/pdf/2104.00298.pdf 提出了一种模型缩放策略，如何更高效的平衡网络的深度、宽度、和图片分辨率 **1. Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks EfficientNetV2: Smaller Models and Faster Training** @Aiken H 2021 find detail to code his 2. Efficient Net V1 除了提出了缩放策略以外，还使用神经架构搜索还建立了一个新的baseline network，得到了一系列模型。 平衡网络宽度、深度、分辨率至关重要，这种平衡可以通过简单的恒定比率缩放维度来实现，于是我们提出了一种简单有效的复合缩放方法。 复合缩放的物理意义：输入图像更大的话就需要更多层来增加感受野和更多通道，从而能在更大的图像上捕获更多细粒度的图案，而宽度和深度（对于表达能力来说很重要）之间也存在着一定的关系，“我们”是第一个对此进行了建模的。 从各个维度单独的进行缩放能发现都存在着增益瓶颈，如何去得到这么一个合适的等比缩放增益 2.1. Motivation and Method 一些直观上的motivation，以及假想 不同的缩放维度不是独立的 直观上，对于更高分辨率的图像我们应该增加网络深度。 这样更大的感受野可以帮助捕捉更大图像中包含的更多像素的相似特征 相应的，更高分辨率的图像也应该增加网路的深度以便再高分辨率图像中捕获具有更多像素的更细粒度的图案。 基于实验最终得到了这样的结果： depth: d = \\alpha^\\phi width: w = \\beta^\\phi resolution: r = \\gamma^\\phi s.t. \\alpha · \\beta^2 · \\gamma^2 \\approx 2 \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 求解方法： 固定φ，然后通过网格搜索得到最基本的模型 Efficient Net-B0 固定α、β、γ的值，使用不同的φ，得到相应的B1 -B7 2.2. Experience Detail github.surf github.surf EfficientNet网络结构图_LYS_1129的博客-CSDN博客_efficientnet网络结构 图解EfficientNet模型的完整细节 EfficientNet网络解析_bblingbbling的博客-CSDN博客_efficientnet网络结构 EfficientNet B0-B7 网络参数_繁华落尽，寻一世真情的博客-CSDN博客 从第三个连接中，我们可以整理出那些需要input的相关参数，然后输入网络中去建立该Model。 2.2.1. 实现中的问题： blockN stride和padding在各个重复组合层中间的变化（只有DW卷积改变Imageview）（堆叠的层不改变相应的HW），也是由第一个层去进行处理 channel，在各个组合层之间的变化（堆叠的层刚好不改变channel数目） © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 18:25:37 "},"Papers/Transformer.html":{"url":"Papers/Transformer.html","title":"0.6 Transformer","keywords":"","body":"1. Transformer Survey(VIT version)1.1. Intro导言1.1.1. 基本问题1.1.2. CNN的异同分析1.2. Attention Is All You Need 李沐1. Transformer Survey(VIT version) @aikenhong 2021 References For Transformer: NLP The Transformer Family (lilianweng.github.io) VIT Transformer眼中世界 VS CNN眼中世界 李沐 NLP Transformer论文精读 Suveys cver1， cver2，cver3 This blog will divided into several part : lil's blog, the survey for ViT, we using those article to help us understand the transformer. 综述我们以最新的一篇为准进行阅读，其他的可能后续进行查缺补漏把，如无必要，勿增烦恼。 1.1. Intro导言 主要参考文章2来进行我们简单的导入 1.1.1. 基本问题 Transformer原本是NLP中的重要模型, 作为LSTM的后继者, 用于处理Seq2Seq的数据类型和情景, 若是要将Transformer运用到Vision的领域中, 首要的问题就是如何: 将Image作为序列化的Token输入Transform中 , 而达成这个目的主要有三种典型的方法: 像素点作为token, 使用VAE离散化图片作为token再输入 ViT: 将图片切为一个个Patch在经过线性的projector之后组成一个embedding表示进行交互 1.1.2. CNN的异同分析 差异分析和计算主要靠CKA向量相似度计算来计算模型和表征之间的差异，这里的理论分析暂且不赘述，后续有需求的话可参考论文Similarity of neural network representations revisited或当前文章. ViT的深浅特征高度相似, 而CNN则是层次化的存在表征区别. 我想这和网络的结构之间有很大的关系, 序列化的结构和层次化的结构之间存在的差别. ViT最后输出使用的是CLS token, 而CNN最终的Global Pooling导致Vi T的顶层特征是独特的, 与CNN的深浅都不匹配 ViT 模型，在底层就已经是局部和全局信息都混在一起了，而上层则基本都是全局信息。和 ResNet 差别在于，因为 CNN 本身特性，底层只利用局部信息。 此外，当用少量数据训练 ViT 的时候，发现底层的头是学习不到局部信息的。 而这也导致了模型性能不是很好，所以视觉模型需要表现好，底层一般都需要学习到局部信息，这也是符合解剖学里面人类视觉神经结构的。 最后一层的空间信息的学习和Pooling有关, 导致了ViT中有更多空间信息. 1.2. Attention Is All You Need 李沐 实际上《Attention is All You Need》就是NLP的Transformer的祖宗，这一篇论文已经反反复复的度过很多次了, 所以这一部分主要用来做查缺补漏。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-11 09:18:38 "},"Papers/MIM-V-MAE.html":{"url":"Papers/MIM-V-MAE.html","title":"0.7 MIM-MAE ","keywords":"","body":"1. Masked Autoencoders Are Scalable Vision Learners1.1. Conclusion1.2. experiment1. Masked Autoencoders Are Scalable Vision Learners @Author：Facebook AI Research-Kaiming He @Read：AikenHong Kaiming-MAE 1.1. Conclusion 总而言之这是一种大模型的训练方法, 通过在少量数据的基础上实现大模型的训练. 整体的架构上是参考了NLP中的AutoEncoder机制，将原图切分patch，用mask掩盖原图，通过少量可见的Patch进行Encoder后和Mask融合，再通过非对称的Decoder进行pixel的还原。 这种设计的有点在于mask的scala是可变的，同时这种mask能减少我们训练过程中对显存和计算复杂度的损耗，同时问题本身是一个比较复杂的问题，得以训练复杂的大模型，这种方式最终呈现的效果就是训练的效率高且效益好。 体现了自监督学习在这方面的优越性，同时这种方法得以实现也是由于ViT模型对于CNN模型的取代，才使得这种序列化切块的方式容易实现和验证。 这种方式在最终体现了自监督学习对于有监督与训练的优越性，使用这种方式能够更好的得到一个模型的通用表征。 在这里论文中也说明了和NLP的不同点以及这样的模型对于decoder的要求实际上是比NLP更高的 1.2. experiment Masking：对于输入的图像进行均匀的切分并均匀的随机采样 MAE encoder: 简单的ViT模型，对输入图像进行编码后和Mask进行混合得到一个完整的令牌集合，从而确保Decode能够得到对应的位置信息。 MAE decoder：轻量级的架构，可以独立于编码器进行设计，我们使用更窄更浅的网络，计算量比编码器10%更小，这样能够更快的进行训练。解码器的最后一层是先行投影，输出的数量==补丁中像素值的数量，最后会resize层原图的维度。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-30 00:09:53 "},"Papers/MIM-V-iBOT.html":{"url":"Papers/MIM-V-iBOT.html","title":"0.8 MIM-iBoT ","keywords":"","body":"1. Image Best Pre-Training With Online Tokenizer1.1. 基本思想1. Image Best Pre-Training With Online Tokenizer @Read: AikenHong 2021 @Author: https://arxiv.org/abs/2111.07832 @解读：Machine Heart 1.1. 基本思想 基于NLP中的MLM(Masked Language Model)的核心训练目标: 也就是遮住文本的一部分, 然后通过模型去预测和补全, 这一过程是模型学到泛化的特征, 使用这种方法来进行大规模的与训练范式. 在基本的思想上和MAE采用的是一样的设计, 但是本文中坐着认为visual tokenizer的设计才是其中的关键. 不同于 NLP 中 tokenization 通过离线的词频分析即可将语料编码为含高语义的分词，图像 patch 是连续分布的且存在大量冗余的底层细节信息。而作者认为一个能够提取图像 patch 中高层语义的 tokenizer 可帮助模型避免学习到冗余的这些细节信息。作者认为视觉的 tokenizer 应该具备两个属性：（a）具备完整表征连续图像内容的能力；(b) 像 NLP 中的 tokenizer 一样具备高层语义。 文中对tokenizer的设计为一个知识蒸馏的过程: 文中使用这种在线tokenizer同时来监督这样的MIM过程, 也就是两部分协同学习, 能够较好的保证语义的同时并将图像内容转化为连续的特征分布, 具体的, tokenizer和目标网络狗狗想网络结构, 有移动平均来得到实际的tokenizer. 该形式近期在 DINO [3]中以自蒸馏被提出，并被用以针对同一张图片的两个不同视野在 [CLS] 标签上的优化： L_{CLS} = - P_{\\theta^`}^{[CLS]}(v)^T logP_{\\theta}^{[CLS]}(\\mu) 在该损失函数的基础上, MIM同样也是用这种自蒸馏的方式去优化, 其中在线tokenizer的参数为目标网络历史参数的平均. L_{MIM} = - \\sum_{i=1}^Nm_i *P_{\\theta^`}^{patch}(\\mu_i)^TlogP_{\\theta}^{patch}(\\hat{\\mu}_i) 基于上述的这些训练目标，提出了一种自监督预训练框架iBOT， 同时优化两种损失函数。 其中，在 [CLS] 标签上的自蒸馏保证了在线 tokenizer 学习到高语义特征，并将该语义迁移到 MIM 的优化过程中；而在 patch 标签上的自蒸馏则将在线 tokenizer 表征的 patch 连续分布作为目标监督 masked patch 的复原。该方法在保证模型学习到高语义特征的同时，通过 MIM 显式建模了图片的内部结构。同时，在线 tokenizer 与 MIM 目标可以一起端到端地学习，无需额外的 tokenizer 训练阶段。 预训练时采用孪生网络结构，其中在线 tokenizer 可以看作教师分支的一部分。教师、学生两分支包括结构相同的 backbone 网络和 projection 网络。作者广泛验证了 iBOT 方法搭配不同的 Transformers 作为 backbone，如 Vision Transformers（ViT-S/16, ViT-B/16, ViT-L/16）及 Swin Transformers（Swin-T/7, Swin-T/14）。作者发现共享 [CLS] 标签与 patch 标签的 projection 网络能够有效提升模型在下游任务上的迁移性能。作者还采用了随机 MIM 的训练机制，对每张图片而言，以 0.5 的概率不进行 mask，以 0.5 的概率从 [0.1, 0.5] 区间随机选取一个比例进行 mask。实验表明随机 MIM 的机制对于使用了 multi-crop 数据增强的 iBOT 非常关键。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 16:40:40 "},"Papers/MIM-V-simMIM.html":{"url":"Papers/MIM-V-simMIM.html","title":"0.9 MIM-simMIM","keywords":"","body":"1. SimMIM：A Simple Framework for Masked Image Modeling1.1. Intro & Simple Conclusion1.1.1. Conclusion1.1.2. Motivation1.2. Theoretical Design1.3. Structure Design1.3.1. Masking1.3.2. Projector Head1.3.3. Projector Targets1. SimMIM：A Simple Framework for Masked Image Modeling @Author： MSRA Zhenda Xie @Source：Arxiv， Code TBP，Blog_CVer @Read：AikenHong 2021.11.22 “What I cannot create, I do not understand.” — Richard Feynman 1.1. Intro & Simple Conclusion 1.1.1. Conclusion 继MAE和iBoT之后，MSRA也提出了一个图像掩码建模的新框架，SimMIM，该方法简化了最近这些提出的方法，不需要特殊设计，作者也验证了不需要那些特殊设计就已经能让模型展现出优秀的学习能力 采用中等大小的掩码块（32），对输入图像进行随机掩码，能使其成为强大的代理任务（pretext task） 直接回归预测原始像素的RGB值的效果并不比复杂设计的Patch分类方法差 Projector Head可以是轻量的Linear Layer，效果并不一定比MLP（多层）的差 1.1.2. Motivation 通过这种MIM方法可以实现在大量无标注的数据上得到一个表征能力up的通用特征模型，这种方式的backbone可以广泛的应用到图像上的各种子任务中（按照NLP）的经验来说，而为了类似的方式在图像上的大放异彩，我们首先需要分析Vision和Language的不同 图像有更强的局部关系：相互靠近的像素是高度相关和近似的，我们可以通过简单的copy padding复制一部分缺失 视觉信号是原始，低层次的，而文本分词是高级概念：对低层次信号的预测是否对高层次的视觉识别任务有用呢？ 视觉信号是连续的，而文本的分词是离散的： 如何基于分类的掩码语言建模方法来处理连续的视觉信号 1.2. Theoretical Design 掩码选择：同样的掩码的策略还是基于Patch进行的，对于掩码的设计来说，太大的掩码快或者太密集的掩码快，可能会导致找不到附近的像素来预测，实验证明32是一个具有竞争力的size，和文本任务的信息冗余程度不同也带来了覆盖比的选择，NLP通常是0.15，而在V中，32size可以支持0.1-0.7的覆盖率。 任务选择：使用原始像素的回归任务，因为回归任务和具有有序性的视觉信号的连续性很好的吻合。 预测头选择：使用轻量的预测头如（linear），迁移性能与繁琐的预测头相似或者略好，同时训练上更加的块。虽然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务。 1.3. Structure Design SimMIM方法就是掩码表示学习，实际上就是掩码图像然后预测原始信号，主要的组成部分、 Masking Strategy，选择图像掩码掩码区域，并实现掩码，将掩码后的图像作为图像的模型输入 Encoder Architecture， 提取特征表示，用来预测原始信号，主要采用vanilla VIT和Swin Transformer Prediction Head，用于预测潜在的特征表示，表示掩码区域中的原始信号 Prediction target，定义了要预测的原始信号的形式，可以是原始像素值也有可以是元素像素变换。同时定义了损失：分类ce，回归l1，l2 1.3.1. Masking 使用可学习的mask token vector代替每个掩码区域，这个token向量的维度和其他的可见patch，经过patch embedding后的维数相同，主要测试了以下的几种策略： 1.3.2. Projector Head 形式和大小任意，只要输入和编码器的输入是一致的，其输出达到预期目标即可，只是本文的作者证明了预测头可以做成轻量的单层线性层。 也测试过2layers-MLP，inverse Swin-T/B 1.3.3. Projector Targets 原始像素之回归，一般情况下视觉框架生成下采样分辨率的特征图，ViT为16*其他架构为32* 为了预测输入图像全分辨率下的所有像素值， 将feature map中的每个特征向量映射回原始分辨率，并让该向量负责相应的原始像素的预测 例如，对于Swin Transformer编码器生成的32×下采样的feature map，作者使用输出维数为3072 = 32×32×3的1×1卷积(线性)层来表示32×32像素的RGB值。对原始图像分别进行{32×， 16×， 8×， 4×， 2×}下采样，考虑分辨率较低的目标。 在掩码像素上使用L1-Loss， 可以使用其他的预测目标： Color clustering. 在iGPT中，利用大量自然图像，通过k-means将RGB值分成512个簇。然后每个像素被分配到最近的簇中心。这种方法需要一个额外的聚类步骤来生成9位调色板。在实验中，作者使用了在iGPT中学习到的512簇中心。 Vision tokenization. 在BEiT中，采用离散VAE (dVAE)网络将图像patch转换为dVAE tokens。token可用作为分类目标。在这种方法中，需要预训练一个额外的dVAE网络。 Channel-wise bin color discretization. 将R、G、B通道分别进行分类，每个通道离散为相同的bins，例如实验中使用的8和256 bins。在· © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-24 16:58:17 "},"Papers/Loss-WhyZero.html":{"url":"Papers/Loss-WhyZero.html","title":"0.10 Loss-Not0","keywords":"","body":"1. Loss :Why Zero Loss？1. Loss :Why Zero Loss？ @Comments: ICML2020 《Do We Need Zero Training Loss After Achieving Zero Training Error》 @Noteby：AikenHong2021 如何解决训练损失下降，但是验证损失上升的问题（过拟合like）的问题，该文章实际上可以作为我们损失设计中的一个trick，只需要简单的一行代码，来提升代码的泛化能力； 这张图体现了本文的灵魂（思路），主要体现在我们在算法趋于稳定后继续训练可能验证损失会反而上升； 所以本文提出了一种flooding方法，当我们training loss 大于阈值的时候我们使其正常下降，当低于阈值的时候，flooding的设计会反过来使得梯度上升，让训练损失保持在flooding附近，让模型持续进行random walk，希望模型最终能优化到一个平坦的损失区域，这样发现test loss进一步的进行下降。 理解： 当我们的训练损失低到一定的程度，然后随着lr的下降，模型会很难跳出当前的极小值，这种情况下我们的泛化能力也会被限制住，采用这种方法在牺牲测试精度的同时能提升算法的泛化能力。 损失公式表示如下 \\widetilde{J}(\\theta) = |J(\\theta) - b| +b 具体的代码表示只需要添加一层： b = the flood num new_loss = (loss - b).abs() + b optimizer.zero_grad() new_loss.backward() optimizer.step() 损失中怎么设置b值 摘自知乎回答，我觉得这种方式说好也好，说不好也不好，算是一种治标不治本的trick把，通过这种方式可以勉强缓解那种代码陷入极小值无法调整的情况，但是实际上算法原理并不是一个很solid的 看了下评论，不少人关心b值应该如何设置，首先论文给出说法是b做为超参数需要在一定范围内遍历选优，对于b得取值范围文中也仅有一个限定是：b值要小于测试损失，这个范围显然太宽泛了。也有人说应该在Validation Error 开始上升的时候，设置b值在此附近，进行flooding，因为此处说明已经开始过拟合，避免在错误方向上渐行渐远。个人觉得有道理，但是在自己的本地任务上尝试下来发现，通常来说b值需要设置成比\"Validation Error 开始上升\"的值更小，1/2处甚至更小，结果更优；想下来原因应该是：Validation Error开始上升的原因不仅仅使说明过拟合情况的发生，还有可能是验证机和训练集不满足独立同分布（这种情况更见），当原因是后者时，往往需要沿着梯度下降方向继续学习，也是解释通了实际使用种为何b值要设置的更小。 和参数正则化之间的差异在哪里。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-10 16:19:11 "},"Papers/Loss-SmoothSharpen.html":{"url":"Papers/Loss-SmoothSharpen.html","title":"0.11 Loss-Smooth","keywords":"","body":"1. Loss: Smooth the label1.1. What's the smooth label1.1.1. Pros and Cons1.2. How it work1.2.1. Feature Norm1.3. With the Sharpen Label1.4. reference1. Loss: Smooth the label @AikenHong 2021 @topic smooth label (inception v2) when does label smoothing help (nips 2019) sharpen in semi-supervised in the future offical code github 不是一个通用的方法，在很多的任务上反而会导致掉点的现象，可以简单分析一下，汲取一下思想和Sharpen做对比，在这篇文章中，我们可以结合之前的人脸对比损失来进行分析。 [ ] Check The idea for Incremental Learning with Smooth Labels and Sharpen Label. 1.1. What's the smooth label 首先介绍在图像分类任务中对logits和Hard label做ce得到我们的损失，可以表现为如下的形式： Loss = -\\sum^{K}_{i=1}p_i \\log(q_i) 由于我们的标签是一个hard label，实际上可以转化成一个one-hot，即 \\begin{equation} p_i = \\left\\{ \\begin{array}{c1} 1 & i==gt \\\\ 0 & i!=gt \\\\ \\end{array} \\right. \\end{equation} 而soft label实际上做的是将 1的位置变为$1-\\alpha$，其他位置设置为$\\alpha/(K-1)$，然后再去求CE， Hinton论文中给出该损失对特征分布的作用测试图： 1.1.1. Pros and Cons why does label smoothing help 校准性：模型预测的分数能不能同时表征其置信度 LS可以提高模型的泛化性，同时还能提高模型的校准性（model calibration） 在模型蒸馏中，如果我们的teacher model是由LS训练的，TM的效果更好，但是Student的性能会变差，这是因为LS的作用是将相同类别的example聚类到更加紧促的cluster中，但是这也导致了，不同样本之间的相似性信息的损失，从而影响了蒸馏的效果 此外模型的校准性能，通常可以使用T系数来进行优化，Temprature scaling（TS）可以有效的降低ECE（expected calibration error） （TS就是在计算cross entropy之前把模型的输出除以超参T，然后再参与cross entropy的计算，比较典型的应用就是在蒸馏中对teacher模型 soft label 处理） 1.2. How it work 从特征输出的信息来看，启用了LS（loss smooth）的特征的Feature Norm比没有启用小得多，特征空间减小的话，实际上就是降低softmax中的s值（长度，还有另一个指标是角度） ==较低的s值==会有这样的几个作用： softmax prob的最大值降低，这样我们就可以永远在线性优化区，几乎不存在平滑区域，这样样本向中心的聚拢程度会更高 s过小的话，对于人脸匹配（往往设置较大的s），为了有更宽广的判别面，使得精度更高，对应于Hard Sample（Task）也是一样的到理道理，就会起到反作用。 Label Smoothing起到的作用实际上是抑制了feature norm，此时softmax prob永远无法达到设定的$1-\\alpha/k-1$ ，loss曲面上不再存在平缓区域，处处都有较大的梯度指向各个类中心，所以特征会更加聚拢。而之所以人脸上不work，是因为我们通常会使用固定的s，此时Label Smoothing无法控制feature norm，只能控制角度，就会起到反向优化的作用 1.2.1. Feature Norm BTW：对比损失可以分为alignment和uniformity部分 第一部分和正样例有关，第二部分仅和负样例有关，作用是远近。 1.3. With the Sharpen Label Sharpen实际上是和Smooth相反的过程, Sharpen使用的场景可能相对较少, 比如我们希望能最小化熵损失(Like Semi-Supervised), 让输出模型的置信度更高, 或者让分界面更加分明的情况. Sharpen(p, T)_i= \\frac {p_i^{1/T}} {\\sum_{j=1}^L p_i^{1/T}} 当T->0的时候,标签将趋向于ont-hot(Dirac) 而Smooth实际上模拟的是真实的标签分布场景, 目的是为了让Label本身携带更多的类别信息, 而所以Smooth Label会更适应Incremental或者知识蒸馏这样的任务, 但是他是不适应于细粒度分类这样的任务的. 用预测的分布标签去做蒸馏, 实际上就有点Smooth的味道了. 而当我们去做FIL任务的时候, 我们拥有的标签实际上应该是One-Hot的, 所以我们需要分析是否需要对其去做smooth, 还是说我们结合SCL的特性, 用One-Hot这种Sharpen的标签去学一个更好的分界面. 1.4. reference ⁉️理解的是错的，从NCE角度 参考个人对NCE的理解[[Papers/Loss-NCE]] 可以结合里面的人脸对比损失的softmax basic | translate | better translate © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-10 21:55:29 "},"Papers/Loss-NCE.html":{"url":"Papers/Loss-NCE.html","title":"0.12 Loss-NCE","keywords":"","body":"1. NCE-Loss 对比学习损失1.1. What's NCE Loss1.2. How it Works1.3. With Self-Supervised Learning1.3.1. 避免退化解形成1.3.2. 实验中的设置问题1.4. with supervised learning1.5. with ArcFace1.6. reference1. NCE-Loss 对比学习损失 @AikenHong 2021 Noise Contrastive Estimation Loss = NCE Loss 噪声对比估计损失，这里的Noise实际上就是Negative Samples. 该损失被广泛的用于对比学习的任务，而对比学习广泛的作为自监督学习的无监督子任务用来训练一个良好的特征提取器，于是对于对比学习的目标和效用的理解十分关键。 1.1. What's NCE Loss 在介绍NCE之前我们可以将其和CE进行一个简单的对比，虽然名称上不是同一个CE，但是在数学表达上却有很相近的地方（softmax-kind of loss） 首先softmax，他保证所有的值加起来为一，结合onehot的ce，实际上j==gt的情况下外层+log也就是ceLoss，也就是 $logSoftmax$ S_j = \\frac{e^{a_j}}{\\sum_{k=1}^N e^{a_k}} 然后看infoNCE，基础的对比学习损失可以写成： L_{contrast} = \\mathbb{E}[-\\log\\frac{e^{f_x^T f_y/T}}{e^{f_x^T f_y/T} + \\sum_i e^{f_x^T f_{y_-^i}/T}}] 其中 $f_x^T f_y^T$ 为 $sim(x,y)$ 时即转化为带$T$的NCE，即InforNCE. 分子是正例对的相似度，分母是正例对+所有负例对的相似度，最小化infoNCE loss，就是去最大化分子的同时最小化分母，也就是最大化正例对的相似度，最小化负例对的相似度。 从该形式上看，和soft的CE形式上是统一的，当我们把分母看作概率和自身以及和其他的相似性，这样和NCE在形式上和简化后的CE实现了统一。 但是我不认为这和label smooth 后的CE有相关性，而是和原始的CE经由One-hot简化后结构上有相似性。 1.2. How it Works NCE的思想是拉近相似的样本，推开不相近的样本，从而学习到一个好的语义表示空间，这一点上实际上和度量学习的思想是一样的，只是对比学习通常作用在无监督或者自监督的语境中，度量学习这是有监督的。 考虑之前人脸匹配的研究，使用 \"Alignment and Uniformity on the Hypersphere\"中的Alignment and Uniformity，就是一个更好理解他的角度 \\begin{gathered} L_{\\text {contrast }}=\\mathbb{E}\\left[-\\log \\frac{e^{f_{x}^{T} f_{y} / \\tau}}{e^{f_{x}^{T} f_{y} / \\tau}+\\sum_{i} e^{T_{x}^{T} f_{y_{i}}^{-} / \\tau}}\\right] \\\\ =\\mathbb{E}\\left[-f_{x}^{T} f_{y} / \\tau\\right]+\\mathbb{E}\\left[\\log \\left(e^{f_{x}^{T} f_{y} / \\tau}+\\sum_{i} e^{f_{x}^{T} f_{y_{i}^{-} / \\tau}}\\right)\\right] \\\\ \\mathbb{P}\\left[\\left(f_{x}=f_{y}\\right)\\right]=1 \\underbrace{\\mathbb{E}\\left[-f_{x}^{T} f_{y} / \\tau\\right]}_{\\text {positive alignment }}+\\underbrace{\\mathbb{E}\\left[\\log \\left(e^{1 / \\tau}+\\sum_{i} e^{f_{x}^{T} f_{y_{i}}-/ \\tau}\\right)\\right]}_{\\text {uniformity }} \\end{gathered} 公式经过上面的推导就可以看成下的两个部分，其中alignment只与positive pair有关，相反Uniformity只与negative pair相关，希望所有的点都能尽可能的分布在uni hypersphere上。 这样均匀的分布有利于聚类并且线性可分，且经过实验证实无监督对比学习确实能得到强判别力的特征。 Alignment：指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也就是在超球面上距离比较近； Uniformity：指的是系统应该倾向于在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀的分布在球面上，分布的越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。 参考Label Smooth中Soft Label的定义，当我们将特征拉到超球面上均匀分布的时候，特征之间相对的距离关系，远近是否应该保留真实分布中的相似性和度量分布？NCE Loss是否能保留这种关系呢？ 这种额外的Info可能能够对于后续的蒸馏学习有一个比较大的影响 1.3. With Self-Supervised Learning 自监督学习最重要的就是下游任务的设计，一般分成两种： 生成式模型：Encode-Decode架构，让输入输出尽可能的相似，或者是后续进化的MIM架构，挖空并还原空中的内容，并在Transformer架构中取代判别式模型方法称为主流。 判别式模型：通过Encoder编码，通过对比学习分析相似性来建立对比损失，自从MoCo出来后判别式模型在一定时间内成为主流。 1.3.1. 避免退化解形成 InfoNCE的两部分在理论上是缺一不可的，如果没有Alignment，就无法聚类，如果没有Uniformly，容易使得所有的输入输出又相同的表示，也就是形成退化解。 参考 Article 对几种自监督的方法解决退化解的方式进行了简要的分析。 1.3.2. 实验中的设置问题 zhihu 对比学习中一般选择一个batch中的其他样本作为负例，如果负例中又很相似的样本怎么办？ 在无监督无标注的情况下，这样的伪负例，其实是不可避免的，首先可以想到的方式是去扩大语料库，去加大batch size，以降低batch训练中采样到伪负例的概率，减少它的影响。 另外，神经网络是有一定容错能力的，像伪标签方法就是一个很好的印证，但前提是错误标签数据或伪负例占较小的比例。 也可以考虑使用监督的对比学习方法 对比学习的infoNCE loss 中的温度系数t的作用是什么？[1] 温度系数的作用是调节对困难样本的关注程度：越小的温度系数越关注于将本样本和最相似的困难样本分开，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小 考虑两个极端情况，温度系数趋向于0时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。 也可以用另一个角度理解： 可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。 对比损失中，越近的负例受到的斥力越大，具体的表现就是对应的负梯度值越大[4]。这种性质更有利于形成在超球面均匀分布的特征。 对照公式去理解： L_{i}=-\\log \\left(e^{S\\left(z_{i}, z_{i}^{+}\\right) / \\tau} / \\sum_{j=0}^{K} e^{S\\left(z_{i}, _{j}\\right) / \\tau}\\right) 当温度系数很小时，越相似也即越困难的负例，对应的坟墓就会越大，在分母叠加项中所占的比重就会越大，对整体loss的影响就会越大，具体的表现就是对应的负梯度值越大 当然，这仅仅是提供了一种定性的认识，定量的认识和推导可以参见博客zhihu 1.4. with supervised learning ZHIHU 借鉴了contrastive的设计在监督信息的基础上对其进行改造，设计一个用于监督学习的对比损失。这一点也可以解决我们问题设置中的第一个问题，但是为此也只能在监督的情况下使用。 \\mathcal{L}_{i}^{s u p}=\\frac{-1}{2 N_{\\tilde{y}_{i}}-1} \\sum_{j=1}^{2 N} 1_{i \\neq j} \\cdot 1_{\\bar{y}_{i}=\\bar{y}_{j}} \\cdot \\log \\frac{\\exp \\left(z_{i} \\cdot z_{j} / \\tau\\right)}{\\sum_{k=1}^{2 N} 1_{i \\neq k} \\cdot \\exp \\left(z_{i} \\cdot z_{k} / \\tau\\right)} 其实也就是当标签相同的时候都当作正例，其他时候都是负例，也就是修改了原本状态下positive的情况。 在训练的过程中，该方法和two-stage会使用同样的策略，也就是在第一阶段使用SCL训练Backbone，在第二阶段固定representation的参数，并只对clf的参数进行训练。 Code Part understand the code；Offical Code； 如果我们需要理解这串代码如何使用，我们需要阅读官方源码中的数据使用模式，我们需要使用图像的两组不同增强，计算对应的特征，然后整合到n_views维度，再将其传入该损失。 后续我们可以基于NXTent Loss函数来简化和改写该损失，目前我们只需要对其加入Normalization就可以暂时进行使用了，第一步我们使用大的batchsize来代替2Augs，如果效果不好的话可以测试2Augs是否会有更好的增益 import torch import torch.nn as nn class SupConLoss(nn.Module): \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf. It also supports the unsupervised contrastive loss in SimCLR\"\"\" def __init__(self, temperature=0.07, contrast_mode='all', base_temperature=0.07): super(SupConLoss, self).__init__() self.temperature = temperature self.contrast_mode = contrast_mode self.base_temperature = base_temperature def forward(self, features, labels=None, mask=None): \"\"\"Compute loss for model. If both `labels` and `mask` are None, it degenerates to SimCLR unsupervised loss: https://arxiv.org/pdf/2002.05709.pdf Args: features: hidden vector of shape [bsz, n_views, ...]. labels: ground truth of shape [bsz]. mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j has the same class as sample i. Can be asymmetric. Returns: A loss scalar. \"\"\" device = (torch.device('cuda') if features.is_cuda else torch.device('cpu')) if len(features.shape) 3: features = features.view(features.shape[0], features.shape[1], -1) batch_size = features.shape[0] if labels is not None and mask is not None: raise ValueError('Cannot define both `labels` and `mask`') elif labels is None and mask is None: mask = torch.eye(batch_size, dtype=torch.float32).to(device) elif labels is not None: labels = labels.contiguous().view(-1, 1) if labels.shape[0] != batch_size: raise ValueError('Num of labels does not match num of features') mask = torch.eq(labels, labels.T).float().to(device) else: mask = mask.float().to(device) contrast_count = features.shape[1] contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0) if self.contrast_mode == 'one': anchor_feature = features[:, 0] anchor_count = 1 elif self.contrast_mode == 'all': anchor_feature = contrast_feature anchor_count = contrast_count else: raise ValueError('Unknown mode: {}'.format(self.contrast_mode)) # compute logits anchor_dot_contrast = torch.div( torch.matmul(anchor_feature, contrast_feature.T), self.temperature) # for numerical stability logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True) logits = anchor_dot_contrast - logits_max.detach() # tile mask mask = mask.repeat(anchor_count, contrast_count) # mask-out self-contrast cases logits_mask = torch.scatter( torch.ones_like(mask), 1, torch.arange(batch_size * anchor_count).view(-1, 1).to(device), 0 ) mask = mask * logits_mask # compute log_prob exp_logits = torch.exp(logits) * logits_mask log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True)) # compute mean of log-likelihood over positive mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1) # loss loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos loss = loss.view(anchor_count, batch_size).mean() return loss 1.5. with ArcFace 对比学习损失我们知道其目的是为了，拉近相似样本之间的距离，并尽量的将不同的类别之间的样本区分，而这和进行人脸识别中的$ArcFace Loss$系列的Softmax Loss有着相同的目的。 那么这两种方法之间是否能够相互借鉴，或者说是否NCE本身在自监督学习任务上就更优于ArcFace?（是否会过度关注细节，无法关注到相应的整体架构） 或者说在后续的分类器训练过程中，这样是否能够帮助我们使用聚类的方式进行分类？（结合epoch-control的那种方法）进行fine-tuning等等 1.6. reference “Understanding the Behaviour of Contrastive Loss” CVPR2021 Analysis The InfoNCE-Loss © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-19 20:20:42 "},"Papers/OWL-survey.html":{"url":"Papers/OWL-survey.html","title":"1 Open World Learning","keywords":"","body":"1. Open World Learning1.1. Reference1.2. Conclusion1.3. Papers1.3.1. :zap: Large-Scale Long-Tailed Recognition in an Open World1.3.2. :zap:Open-world Learning and Application to Product Classification1.3.3. :zap:TOWOO1.3.4. Open World Compositional Zero-Shot Learning1.3.5. Open World Semi-Supervised Learning1.3.6. Self-Supervised Features Improve Open World Learning1.3.7. Unseen Class Discovery in Open-World Classification1. Open World Learning @AikenHong2021 OWL 分析现有的OWL特点，和当前自己的研究做一个区分，也汲取一下别人的研究的要点， 1.1. Reference arxiv @ self-supervised feature improve open-world learning arxiv @ open-world semi-supervised learning arxiv @ open-world learning without labels arxiv @ unseen class discovery in open-world classification arxiv @ Open-World Active Learning with Stacking Ensemble for Self-Driving Cars www @ open-world learning and application to product classification cvpr @ open world composition zero-shot learning cvpr @ Towards Open World Object Detection cvpr](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.pdf)) @ Large-Scale Long-Tailed Recognition in an Open World 1.2. Conclusion 1.3. Papers Mulit Open world Learning Definition 拒绝未见过的类的实例，逐步学习新的类扩展现有模型 1.3.1. :zap: Large-Scale Long-Tailed Recognition in an Open World Large-Scale Long-Tailed Recognition in an Open World (liuziwei7.github.io) Translation 这篇文章很可能作为后续我们比较的baseline，通过对这篇文章的数据和代码复用和同等环境下的处理，来进行算法优劣的比较。但是实际上该论文的定位和我们也并非完全相同，因为改论文将开放世界的类别识别为未知类，主要的问题是如何避免将未知类别分类到少样本类别中。 我们希望将开放类作为新类别数据处理（Few-Shot），增加了Incremental的部分，这是他们论文中缺少的一部分，同时这篇论文的思路面向的任务是识别而不是分类，他们在识别上的信息实际上是更完善的，但是对于分类任务来说，如果不基于相应的标签先验，实际上容易带来问题。 使用动态元嵌入的策略来结合两个部分处理尾部识别的鲁棒性： 从输入图像计算得到直接特征（这一方面上FS的类别缺乏足够的监督） 视觉记忆相关的诱导特征（来源于基于memory的meta-learning），这种特征（visual-memory）具有直接的判别中心，学习一种方法从直接的特征中学到相关的记忆摘要，通过这种meta-embedding来处理尾部的类别。 我们的测试过程是否也可以看作管理一个Memory，通过对Memory的定时定量的Dynamic-Evaluation来做后续的Incremental Learning，通过这种策略来将整个框架整合起来，从最初的模型到后续的模型增量就能更好的结合在一起。 这个方法主要是在进行分类的同时维护一个嵌入图，通过对应的距离关系来计算类别的质心，来做作为另一个角度的特征，然后讲特征整合后作为最终的特征依据 1.3.2. :zap:Open-world Learning and Application to Product Classification 重点：该模型维护一组动态可见类，允许添加或删除新类，而无需重新训练模型。每个类由以小组训练示例表示，在测试中元分类器仅使用已维护的可见类，进行分类和拒绝。 基于metric进行判别和分类 实际上是一种prototype的方法，通过维护类别原型，使用metric的方法进行是否是已知类别的判断。 Ranker的作用是在每个已知类中抽取与一个测试样例的最近邻的k个已知类样例，然后将这些已知类的k个样例存入Meta-Classifier的Memory中。Meta-Classifier将测试样例与Memory中，经过Matching layer与Aggregation layer输出测试样例属于相应已知类的概率得分。 本文最大的新颖之处在于，在解决开集识别问题时，采用meta-learning的思想，训练集、测试集、验证机中的类别完全不相交。这样做的好处是模型具备增量学习的能力，当源源不断的unknown样例进行测试时，完全不必重新训练模型，提供了open-set classification一种新的模式。 1.3.3. :zap:TOWOO 整体思路类似，聚类方法，标记出感兴趣的类别，然后加入数据库 1）将未识别的对象，识别为unknown 2）在逐步接受相应的标签的时候逐步学习这些未知类别，而不会忘记旧的类别。 使用contrastive cluster和energe-base的方法来对新类进行分类，主要的方法是通过将不确定的类别识别为未知类别。 未知类别识别方法（energe-base） ... 1.3.4. Open World Compositional Zero-Shot Learning 假设搜索空间是先验已知的，也就是存在几种类别是已知的，但是我们训练集中是没有未知类别的，共享特征空间，通过类似A-softmax的方式做匹配分析，通过在已知类别中落入的位置来判断是我们认定的已知类别还是未知类别。 1.3.5. Open World Semi-Supervised Learning 开放世界半监督学习，使用一种同时分类和聚类数据的方法 ORCA： 为了解决这个问题，本文提出了ORCA，一种学习同时分类和聚类数据的方法。ORCA将未标记数据集中的例子分类到以前见过的类中，或者通过将相似的例子组合在一起形成一个新的类。ORCA的关键思想是引入基于不确定性的自适应margin，有效地规避由可见类和新类/簇之间的方差不平衡引起的偏差。本文使用图像分类领域的三个常用数据集（CIFAR-10, CIFAR-100，ImageNet） 进行实验验证，结果表明，ORCA在已知类上的性能优于半监督方法，在新类上也优于新类发现方法。 Method 实际上就是使用半监督SimClr的backbone然后通过设定好的位置类别数目的分类器去做训练，但是这里的损失防止对已知类的偏向性。可以参考文章中的损失 基于对比学习方法SimCLR进行与训练 已知类的分类头用于将未标记的例子分配给已知类，而激活附加的分类头允许ORCA发现新类别。我们假设新类的数量是已知的，并将其作为算法的输入，这是聚类和新类发现方法的典型假设。如果不知道新类的数量，这在现实环境中是经常发生的情况，可以从数据中估计出来。在这种情况下，如果头的数量太多，那么ORCA将不会分配任何例子给一些头，所以这些头将永远不会激活，因此ORCA将自动修剪类的数量。我们在实验中进一步解决了这个问题。 related 了解决这种开放世界的问题，目前有2种思路：(1) OOD检测：能够识别已知类的数据，并且能够将所有未知类的数据检测出来，标为\"unknown\"。这种方法很好的保证了系统鲁棒性，但是无法充分利用未知类数据进行业务扩展；(2) novel class discovery(零样本，领域自适应问题): 利用源域标记数据来学习更丰富的语义表示，然后将学到的知识迁移到目标域（包含新类别），对目标域数据进行聚类。这种方法不能准确识别出已知类，只是对目标域做了聚类。 1.3.6. Self-Supervised Features Improve Open World Learning 特征提取：使用自监督学习来做特征提取器的训练 将新类发现作为特征空间中的位置标签，我们根据检测到的样本属于哪一个空间来做检测 和我的想法还是蛮贴近的，总之也是把新类的标签放置到存储区中，赋予伪标签的过程，然后微调特征提取器，基于后续的数据添加分类器的权重 1.3.7. Unseen Class Discovery in Open-World Classification 通过对已知类别的学习，分析已知类别之间的距离差异； 本文的模型提出了一个contrasive模型，对实例属于同一类还是不同类进行分类，该子模型也可以作为聚类的距离函数. © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-05 00:25:52 "},"Papers/OW-openmix.html":{"url":"Papers/OW-openmix.html","title":"1.1 OpenMix","keywords":"","body":"1. OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World1.1. Intro1.2. Detail1.3. 异同点分析1. OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World @Aiken 2021 究极万恶的撞车论文 1.1. Intro Motivation ：Tackle the problem of 发现无标注数据中与给定（已知）类别不相交的新类。 Related Research： 现有的方法通常1. 使用标记数据对模型进行预训练； 2. 无监督聚类在未标记的数据中识别新的类 作者认为label带来的essential knowledge在第二步中没有被充分学习利用到，这样模型就只能从第一步的现成知识中获益，而不能利用标记数据和未标记数据之间的潜在关系 Hypothesis： 有标记的类别和无标记的类别之间没有Overlap，这样导致在两个类别之间很难建立学习关系，（为啥我感觉这个说的都是屁话） Solution： Openmix：将标注的数据和未标注的数据同时混合起来得到一个联合标签的分布中，用两种方式来动态合成示例： 我们混合标记和未标记数据作为Training Img，混合了已知类别的先验生成的伪标签会比无监督情况下生成的伪标签跟家的可靠？防止在错误的伪标签前提下发生过拟合 在第一步的时候我们鼓励具有高类别置信度的无标记example作为可考虑的类别，然后我们将这些samples作为anchor，并将它们进一步的和无标注的samples整合，这使得我们能够对无标注数据产生更多的组合，并发现更精细的新类关系。 1.2. Detail 果然在混合的方式上和MixUp的策略进行比对了，就是diss了Mixup使用伪标签的情景可能会进一步的引入不确定性，导致算法的效果反向优化，就是再label和unlabeled数据上混用mixup，而不是单纯的对unlabel数据集进行混合。 首先将没有overlap的标签表现为联合标签分布再进行混合，也就是加长onehot，这样的标签的优越性在？对于unlabelled data引入了确定性，防止标签容易过拟合。也就是给伪标签加入了一个锚定，让他能够变化的更平滑 这尼玛这张图看了不久完事了，bibi一大堆啥的呢。主要分析一下三个损失函数代表的是什么意思。 对其中的$L_{ppl}$进行特殊的说明： 由于输入的是pair，所以添加的一个损失也就是分类是否属于同一类，二分类ce 使用的是cos similarity，通过threshold 来判断是否是同一类， 实际上应该也是一个预训练的模块，在实际进行的过程中由于是对无标注数据进行处理，讲道理是无法计算损失的，也没有开源代码。 1.3. 异同点分析 初步分析结果： 不使用无监督聚类的方法对新类进行发现，而是使用其他的策略 好像没有使用增量学习的方法进行class-incremental的增量处理，主要的motivation好像是Discovering，并没有Incremental的部分 新数据的组合方式是怎么样的这点好像值得研究一下 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-09 20:18:15 "},"Papers/OW-OD.html":{"url":"Papers/OW-OD.html","title":"1.2 OpenWorld Object Detector","keywords":"","body":"1. Open World Object Detector1.1. 思路分析1.1.1. Motivation1.1.2. ENERGY BASED1.1.3. Alleviating Forgetting1.1.4. Workflow1. Open World Object Detector @Aiken 2021 框架撞车系列，主要看看这一篇论文中怎么解决如下的问题&#x1F447;，并从中借鉴和优化的我框架设计 [ ] 置信度的模块构成，相应的pool的构建 [ ] 新类的划分和新类原型的建立 [ ] Incremental Learning的思路和实现方式 1.1. 思路分析 1.1.1. Motivation 模型实现的主要的两个TASK： Open Set Learning ： 在没有明确监督的时候，将尚未引入的目标类别识别为未知 Incremental Learning：类别增量学习 实现这两个问题的主要思路： 自动标注：借鉴RPN的class-agnostic，以及检测和分类的显著性指标的差异，找到并自动标注NewClass 对比聚类：使用prototype feature来进行聚类，同时计算Distance损失 it seems like contain a unknown prototype. energy based：亥姆霍兹自由能公式？ 1.1.2. ENERGY BASED Feature：$F$, Label: $L$ , Energy:$E(F,l)$ 能量函数倾向于将已知的类别分类到低熵的分布上，然后我们可以根据特征在能量空间上的划分来区分新类和旧类。然后我们可以根据logits表达的softmax形式，找到输出和Gibbs distribution的相关性： p(l \\mid \\boldsymbol{f})=\\frac{\\exp \\left(\\frac{g_{l}(\\boldsymbol{f})}{T}\\right)}{\\sum_{i=1}^{\\mathrm{C}} \\exp \\left(\\frac{g_{i}(\\boldsymbol{f})}{T}\\right)}=\\frac{\\exp \\left(-\\frac{E(\\boldsymbol{f}, l)}{T}\\right)}{\\exp \\left(-\\frac{E(\\boldsymbol{f})}{T}\\right)} 通过这个相关性，我们对自由能进行一个定义，以logits的形式表达 E(f:g) = -T log\\sum_{i=1}^{C}exp(\\frac{g_i(f)}{T}) g实际上表示特征最后输出的logits，通过能量的映射函数，我们将聚类转移到能量域上做，置信度较高的类别和未知的新类实际上有一个比较明显区分的分界线。 实际上我觉得就是类似熵的形式，在本文中将softmax的形式和gibis自由能做了一个对比，然后相当于对logits映射到了能量的维度去做特征的对比聚类，同时也能看出，在能量这个隐层空间中，在能级上能对已知类别和未知类别之间有一个明显的区分，所以在能级上进行划分是一个比较合理的空间映射形式。 1.1.3. Alleviating Forgetting 参数正则化方法：exemplar replay：动态扩张网络；元学习 增量学习中提到的一些贪婪的参数选择策略好像对SOTA的方法都有很大的优势；后续又有人发现存储少量的示例和replay的有效性在相关的Few-Shot Detect中式有效的。 本文采用相对简单的ORE方法来减缓灾难性遗忘的问题，也就是说存放了一组平衡的范例，在每个增量步骤后对模型进行微调，确保每个类最少有$N_{ex}$个示例出现在例子集中。 实际上说的就是每次增量学习之后都会进行数据集的混合然后朝着原本的方向进行一定的微调；好像也没有什么特别的把，具体的实现可能要参见代码。 1.1.4. Workflow 我认为是通过RPN和class-aware之间的插值直接直接标注的一个未知的类别，然后在后续直接让人类区标注感兴趣的样本，可能是从少到多的，并没有一个特定的POOL，原本的模型可能有预留的Unknown Class或者说是相应的预留输出节点，然后在获得新的数据标注之后，进行更新模型的训练，然后使用避免灾难性遗忘的策略去做，从而使得模型对新的类别存在认知，也不会忘记旧的类别的知识。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-18 00:26:51 "},"Papers/KnowledgeEvolution.html":{"url":"Papers/KnowledgeEvolution.html","title":"1.3 KnowledgeEvolution","keywords":"","body":"1. Knowledge Evolution in Neural Networks1.1. Intro引子1.2. 理论与实现细节1.2.1. The Knowledge Evolution Training Approach1.2.2. Split-Networks1.2.3. Knowledge Evolution Intuition1.3. Code细节&使用情景1. Knowledge Evolution in Neural Networks @Aiken 2021.4.7 Article：只能当成OverView，技术细节写的很差；Mendeley； Code_PyTorch 1.1. Intro引子 Problem：如何在较小的数据集上训练神经网络，这到底是个小样本的方法还是个类别增量的方法？ Motivation： 考虑生物“基因”进化的方式，有一部分是“祖传”，另一部分是“适应”，通过对“祖传”的假设的不断学习进化，得到一个新的模型。 基因编码了从祖先到后代的遗传信息（知识），而基因传递将遗传信息从父母传递至其后代。虽然祖先并不一定具有更好的知识，但是遗传信息（知识）在几代人之间的发展将会促进后代更好的学习曲线。 Hypothesis： 拟合假设$H^{origin}$： 重置假设：$H^{later}$ TOBEUPDATE：将神经网络拆分成两个假设(子网络)：通过重新训练多代网络来进化$H^{origin}$ 中的知识，每一代都会扰乱$H^{later}$的内部权重来鼓励$H^{origin}$ 学习独立的表达形式。 将深度神经网络的知识封装在一个名为拟合假设的子网络H中，将拟合假设的知识从父母网络传递至其后代，即下一代神经网络。并反复重复此过程，在后代网络中证明了其性能的显著提升： Contribution： 提出了KELS（内核级卷积感知拆分），为CNN量身定做。虽然增加了训练时间，但是大大降低了推理成本，也减轻了较小数据集中的过拟合问题。 提出了KE，提升网络在较小数据集上的性能 KELS，训练时自动学习slim网络，支持CNN，降低推理成本 Related Work 与两种不同的训练方法作比较 DSD：在网络结构上与这种dense-sparse-dense 1.2. 理论与实现细节 上图展示的是 普通Fliter：3in 4out 修正后在ResNet中的Fliter：拆分成两部分假设，深蓝色的是拟合假设，浅灰色的是重置假设。 1.2.1. The Knowledge Evolution Training Approach L：layers_num；N：network；F：Fliter（Convolution Kernal） Z：Batch Norm；W（FC）：weight；B（FC）：bias；M：0-1 mask（binary） 首先从概念上将网络划分成两个子网的部分，$H^f$、$H^r$，对网络进行随机初始化，然后再e个epoch之后得到generation 1的Network（N1），也就能提取出对应的H. :star:Iteration（迭代到下一代） 基因的贡献直到对下一代网络进行初始化，后续的操作就是“适者生存的部分了” #LOOP 使用$H^f$重新初始化N：使用$H^f$中的F和W去初始化N2，剩下的部分（$H^r$）中的参数进行随机的初始化，初始化的形式可以表达成如下的公式，（随机的部分使用指定好的分布去随机） 重新e个epochs训练进化成N2。 \r F_l = M_lF_l + (1-M_l)F_l^r\r #END LOOP 1.2.2. Split-Networks 这个框架在实现的时候涉及到Fliter的拆分，所以这部分实际上是文章的核心技术难点。 使用两种分裂技术来支持KE： 这种玩意你不看代码谁知道在写什么 weight-level splitting：按照split-rate，使用0-1mask对每一层的参数进行随机的split。 kernel-level convolutional-aware splitting：代替了对每个单独的权重进行mask，我们直接对kernels做mask，如下图所示 1.2.3. Knowledge Evolution Intuition KE的结构和ResNet和Dropout进行对比，之间的异同，一些直观或者直觉上的理解。 1.3. Code细节&使用情景 这个方法实际上是针对的小样本？相对少样本？的使用情景，通过不断的部分继承和迭代，用DNA的方式传播到后续的网络结构中，感觉这个的使用场景还挺blur的，TOBECONTIUNE. 这种评估和消融实验的测试方式的选择！ © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:35 "},"Papers/LT-Collection.html":{"url":"Papers/LT-Collection.html","title":"1.4 Long-Tailed Collation","keywords":"","body":"1. LT-Collections1.1. Introduction1.2. Rebalance1.2.1. reweighting1.2.2. rebalance1.3. two-stage1.3.1. motivation1.3.2. BBN structure1.3.3. Decoupling1.3.4. better-calibration1.3.5. DisAlign1.4. Caucal Analysis1.5. Contrastive1.5.1. hybird contrastive1.5.2. The value of labels1.6. MixUp in LT1.7. Conclusion1.7.1. 实验结果汇总1.8. Reference1. LT-Collections @AikenHong 2021 Code of must of those methods We will analysis those tricks on LT situation, and Analysis why it works. 在进行LT矫正的任务中，有几种常见的trick在各种模型中被使用，我们会对这几种不同的trick进行介绍和分析。 其实在数据量少这一方面LT和Few-Shot是有一定的OverLap的,可以参考以下那边的思路perhaps To Be Read [ ] Balanced-Meta Softmax [ ] 1.1. Introduction 通常情况下这种严重的类别不平衡问题会使得模型严重过拟合于头部，而在尾部欠拟合 首先介绍 bag of tricks 这篇论文中总结了一些常用的Trick，并组合出了最佳的一套trick 经过该文实验总结，Trick组合应该是[1]`： 在前几个epoch应用input mixup数据增强，然后后面fine-tuning; (基于CAM的)重采样来重新训练分类器; 实际上就是MixUp + Two-Stage的策略，后续对Mix-up这个策略带来的作用要进行补充了解一下 1.2. Rebalance 对于ReBalance的方法，实际上就是从 data和 update两个角度来缓解Unbalance本身，通过从数据量上达到重新均衡，或者基于Loss使得bp过程中赋予Tail更高的权重来达到优化过程的平衡。 前者称为rebalance，后者则为reweight. 1.2.1. reweighting 这一部分在实际设计上的体现主要是通过对Loss的重新构造而成，通过对Loss的构造来实现区分的BP权重. 代价敏感softmax交叉熵损失CS_CE: 在ce前乘最小训练图像数目与每个类别图像数目的比值，相当于更注重少类样本 \\mathcal{L}_{CS\\_CE}(\\mathbf{z}, c)=-\\frac{n_{\\min }}{n_{c}} \\log \\left(\\frac{\\exp \\left(z_{c}\\right)}{\\sum_{i=1}^{C} \\exp \\left(z_{i}\\right)}\\right) Focal Loss：设置$\\alpha$ 和 $\\beta$来控制少数类和难分类别对损失的贡献： \\mathcal{L}_{\\text {Focal }}(\\mathbf{z}, c)=-\\sum_{i=1}^{C}\\left(1-p_{i}^{t}\\right)^{\\gamma} \\log \\left(p_{i}^{t}\\right) 类别平衡损失：就是在基本的损失（CE，FOCAL）前加入一个衡量权重，其中$\\beta$是一个超参数，来衡量有效的信息 \\mathcal{L}_{CB\\_Focal}(\\mathbf{z}, c)=-\\frac{1-\\beta}{1-\\beta^{n_{c}}} \\sum_{i=1}^{C}\\left(1-p_{i}^{t}\\right)^{\\gamma} \\log \\left(p_{i}^{t}\\right) Logit Abjustment[3]: \\ell(y, f(x))=\\alpha_{y} \\cdot \\log \\left[1+\\sum_{y^{\\prime} \\neq y} e^{\\Delta_{y y^{\\prime}}} \\cdot e^{\\left(f_{y^{\\prime}}(x)-f_{y}(x)\\right)}\\right] 1.2.2. rebalance 实际上就是对少类或者多类的数据重新做均衡，方法的本质差别一般都不是特别大 随机过采样：随即重复少数类别的样本来使得样本均衡 随机降采样：随机删除多数类别的样本使得样本均衡 样本平衡采样：应该值得就是1-2 IB-Sampling 类别平均采样: 对类别进行统一采样，每个类别被采样的概率都一样(Q=0)，然后从每个类别中有放回的随机采样实例，从而构建平衡的数据集 p_j = \\frac{n_j^q}{\\sum_{i=1}^C n_i^q} 平方根采样(Q=0.5) 逐步平衡采样：先对多个epoch进行实例平衡采样（上式q=1，也就是没有任何平衡操作的采样），然后再剩下的epoches中进行类别平衡采样。这种采样方式需要设置一个超参数来调整从哪一个epoch开始变换采样方式。也可以使用更软的阈值，即随着epoch的增加来逐渐调整实例平衡采样（IB）和类别平衡采样所占的比例，如下面公式所示。 P_j^{PB} (t) = (1-\\frac{t}{T})P_j^{IB} + \\frac{t}{T}P_j^{CB} 1.3. two-stage 在Unbalanced的Data上Pretrain一个特征提取器，然后再rebalance（IB，CB）的数据集上对Classifier进行重新训练（调整），（and | or）对齐，校准（disalign，causal）来提升LT的性能的方法 1.3.1. motivation 但是这些rebalance的方法通常会带来以下两个问题[2]： rebalance之后分类器会倾向于分类正确的尾部样本，导致对于头部有一定的bad influence（欠拟合），对尾部过拟合 rebalance方法会显著的促进分类器的学习，但是会损害深度特征的表示能力，如上图所示，分类器学到的分界面更好，但是特征的表示却更加的松散了 我认为rebalance的策略确实会使得Clf学的更好的分界面，减少偏向性，但是不至于在尾部过拟合，这一部分分析最重要的应该是rebalance对于特征空间的Bad Influence，这可能就是Two Stage的来源。 于是作者为其设计了一些消融实验：CE指的是长尾，RW，RS指的是使用的rebalance的数据。 可以发现在Backbone上使用Unbalance的数据而在Clf上使用Resampler的数据效果是最好的，这种two-stage的解耦两阶段的训练策略展现了一个有希望的结果。 这种两阶段的方式，我认为在第二阶段的时候也要对特征进行微调来适应当前的分布，不过很多的方法都是直接只对分类器进行调整，我们可以对两种方式进行测试 下图显示了fix-two-stage和baseline对比[5] 下图展示了理想的two-stage结果与显示方法之间的距离[5] cls-bound是再fix特征后，用完全均衡的数据集训练分类器得到的结果，由此带入第二张图的绿色的线，可以知道，现有长尾方法的性能瓶颈（未使用two-stage），仍然在特征空间中的有偏差的决策边界。 基于这些分析,我们认为，在得到一个强有力的特征表示后，我们可以将问题归化到分类器上，基于这点假设，我们可以结合我们的自监督模块来对该方法进行归化。 1.3.2. BBN structure Share Weight of Backbone，Using diff dataset to get diff feature. Then we using $\\alpha$ to Combine the logits and calculate the loss. z = \\alpha * W_c^T * f_c + (1-\\alpha) * W_\\gamma^T * f_\\gamma L = \\alpha * l(softmax(z),y_c) + (1-\\alpha) *l(softmax(z),y_\\gamma) 在上述的流程图中W代表的是两个不一样的数据优化器，基于这样的设置最终就能区分两部分的优化。 但是这个方法为我们带来的最大的启发还是在于区分两阶段中学习的重点，backbone需要在一个unblance的条件下学习一个更为通用的表征，而Cls需要矫正偏差。不平衡的情况下可能能学到一个很好的通用表征，这一点就是我们使用自监督的一个重要原因。 1.3.3. Decoupling ==Train BB and Fixed then Train CLF== 此外坐着发现全连接的weight和norm和对应类别的样本数正相关，所以在第二部最后将分类器改为归一化的分类器，文中的两种设计是： $\\overline{W_i}=\\frac{w_i}{\\lVert W_i \\rVert^T}$ $\\overline{W_i}=\\frac{w_i}{f_i}$ 其中2利用fixed第一步分类权重$w_i$,对每个类学习了一个加权参数$f_i$ 1.3.4. better-calibration 但是这种两阶段的方式也不是没有代价的，他会带来比较严重的校准错误(Calibration)，也就是我们预测的概率和实际的相似度之间的一致性。 （BTW评估校准错误的指标 $ECE=\\sum_{b=1}^B\\frac{|S_b|}{N} |acc(S_b) - conf(S_b)|$，将数据分为b组，S_b是落入b区间的样本集合) 本文主要测试了MixUP在两阶段训练中的作用，以及提出了： 标签感知平滑损失，实际上就是cb_ce的半泛化形式： \\begin{gathered} l(\\boldsymbol{q}, \\boldsymbol{p})=-\\sum_{i=1}^{K} \\boldsymbol{q}_{i} \\log \\boldsymbol{p}_{i} \\\\ \\boldsymbol{q}_{i}= \\begin{cases}1-\\epsilon_{y}=1-f\\left(N_{y}\\right), & i=y \\\\ \\frac{\\epsilon_{y}}{K-1}=\\frac{f\\left(N_{y}\\right)}{K-1}, & \\text { Otherwise }\\end{cases} \\end{gathered} $\\epsilon_y$是y(gt)的一个小平滑因子,数目与类别的样本数有关，并提出了几种函数形式，来优化这个损失 BN的移位学习，由于两阶段的数据集不一致，所以normalize的参数是需要学习变化的（均值和方差） 具体的数学分析和推导，后续根据论文理解了再来补充 1.3.5. DisAlign 基于上述对于方法的分析，该文章着重于对于分类器进行校准，具体的思路是基于利于平衡预测的类别分布来对分类器的输出进行匹配，矫正；简单的说利用类别先验和输入数据学习类别的决策边界。 具体由两部分构成(重构预测的概率输出，建立理想分布，使用KL散度计算损失) 自适应配准函数 \\begin{gathered} s_{j}=\\alpha_{j} \\cdot z_{j}^{o}+\\beta_{j}, \\quad \\forall j \\in \\mathcal{C} \\\\ \\hat{z}_{j}=\\sigma(\\mathbf{x}) \\cdot s_{j}+(1-\\sigma(\\mathbf{x})) \\cdot z_{j}^{o} \\\\ =\\left(1+\\sigma(\\mathbf{x}) \\alpha_{j}\\right) \\cdot z_{j}^{o}+\\sigma(\\mathbf{x}) \\cdot \\beta_{j} \\\\ p_{m}(y=j \\mid \\mathbf{x})=\\frac{\\exp \\left(\\hat{z}_{j}\\right)}{\\sum_{k=1}^{C} \\exp \\left(\\hat{z}_{k}\\right)} \\end{gathered} 广义重加权校准 理想的分布的计算方法如下，定义说的不是很好，最好还是参考一下代码 \\begin{gathered} p_{r}\\left(y=c \\mid \\mathbf{x}_{i}\\right)=w_{c} \\cdot \\delta_{c}\\left(y_{i}\\right), \\quad \\forall c \\in \\mathcal{C} \\\\ w_{c}=\\frac{\\left(1 / r_{c}\\right)^{\\rho}}{\\sum_{k=1}^{K}\\left(1 / r_{k}\\right)^{\\rho}}, \\quad \\forall c \\in \\mathcal{C} \\end{gathered} 最终的损失计算方程如下： \\begin{aligned} \\mathcal{L} &=\\mathbb{E}_{\\mathcal{D}_{t r}}\\left[\\mathcal{K} \\mathcal{L}\\left(p_{r}(y \\mid \\mathbf{x}) \\| p_{m}(y \\mid \\mathbf{x})\\right)\\right] \\\\ & \\approx-\\frac{1}{N} \\sum_{i=1}^{N}\\left[\\sum_{y \\in \\mathcal{C}} p_{r}\\left(y \\mid \\mathbf{x}_{i}\\right) \\log \\left(p_{m}\\left(y \\mid \\mathbf{x}_{i}\\right)\\right)\\right]+C \\end{aligned} ==训练的具体策略== 1）在第一阶段，在不平衡数据集上使用实例平衡 ( instance-balanced ) 采样策略实现特征提取器和原始分类头的联合学习。此时由于不平衡的数据分布，学习到的原始分类头是严重有偏的。 2）在第二阶段，我们在特征提取器参数固定不变的情况下关注分类头以调整决策边界，引入了自适应配准函数 ( adaptive calibration function ) 和广义重加权 ( generalized re-weight ) 策略来配准各类概率。 1.4. Caucal Analysis 基于two-stage的这种现象，然后分析机器和人学习的区别，认为带来偏差的元凶在于Optim优化算法，为此，该文章构建因果图，从而去除在模型更新过程中由动量带来的偏差效应。 \"keep good and remove bad momentum\"[7] $vt = \\mu · v{t-1} + gt$, $\\theta = \\theta{t-1} - lr · v_t$ 要调用这个方法的话，我们就需要 将训练的CLF修改成Multi-Head并Normlize，参考Decouple. 训练过程中统计移动平局特征$\\overline{x}$，将其单位方向看成头部倾向. 测试的过程中修正logits即可 具体公式参考对应的解析和代码实现; 和自监督结合的话，只需要在微调的阶段进行统计和修正即可，毕竟是一个一阶段的方式。 1.5. Contrastive 这一部分考虑一些和对比学习，或者说自监督学习耦合的方法来进行分析。 为何将这两者放到同一个章节中？ 因为这两者企图从表征的层面，为LT任务，带来增益，得到一个可分的特征空间基于良好的特征表达，进而解耦的来训练一个更好的CLF。 如果我们假设我们能得到一个高维线性可分的特征空间，对于长尾的样本带来的训练偏差（决策面偏差）是否可以通过对于特定类别的Margin-Like的Loss设置，达到一个类似Balance的效果，这一点上实际上可能和Align和校正的思想有点相似。但是我们是为了让分类器空间中的分界面在小样本的束缚下变得更加的合理。 从分界面的角度看LT的情况：（上面是普通CE，下面是Contrastive Learning） 在数据量出现较大的差异的情况下，由于蓝色的数目更多更杂，所以实际上分界面可能会沿着蓝色数据的边界做切分（overfit），在这种Class-Level的过拟合下，就会导致对于少数类别的分类结果很差。 而下方的对比学习就是一样的解决方案，他试图将同一类的数据聚拢在一起，将不同类的距离尽可能的拉远，这样会使得在空间中的决策面更加的鲁棒也已于区分，虽然可能会一定程度上减少蓝色的表现，但是红色的表现会因此大大的提升。 1.5.1. hybird contrastive 该文章[9]的基本架构上实际上参考的就是BBN的epoch-params在two-stage中集成 supervised contrasive Loss，具体框架可以看下面这图： 他的设计思想很容易从这张图中领会，损失函数的表达显然如下 L_{hybird} = \\alpha · L_{SCL}(B_{SC}) + (1-\\alpha) · L_{ce}(B_{CE}) 在这里要注意SC和自监督中使用的区别在于，自监督学习的过程中没有标签，所以只能将自己作为Positive，而在SC的时候，同类的样本之间应该都作为Positive \\mathcal{L}_{S C L}\\left(\\mathbf{z}_{i}\\right)=\\frac{-1}{\\left|\\left\\{\\mathbf{z}_{i}^{+}\\right\\}\\right|} \\sum_{\\mathbf{z}_{j} \\in\\left\\{\\mathbf{z}_{i}^{+}\\right\\}} \\log \\frac{\\exp \\left(\\mathbf{z}_{i} \\cdot \\mathbf{z}_{j} / \\tau\\right)}{\\sum_{\\mathbf{z}_{k}, k \\neq i} \\exp \\left(\\mathbf{z}_{i} \\cdot \\mathbf{z}_{k} / \\tau\\right)} 鉴于SC的计算复杂度要和整个Epoch的数据进行对比，需要大量的显存空间，在这方面作者将其改进为PSC，其实也就是将每个class计算一个prototype，然后基于原型去计算这个相似性损失 \\mathcal{L}_{P S C}\\left(\\mathbf{z}_{i}\\right)=-\\log \\frac{\\exp \\left(\\mathbf{z}_{i} \\cdot \\mathbf{p}_{y_{i}} / \\tau\\right)}{\\sum_{j=1, j \\neq y_{i}}^{C} \\exp \\left(\\mathbf{z}_{i} \\cdot \\mathbf{p}_{j} / \\tau\\right)} 在这里这个Prototypical需要正则化到单位元中，这样能快速计算相似性损失，也不会需要大量的现存。 可以参考的点主要就在于损失的设计和框架上的这种分epoch机制了，但是基于自监督的方式的话，可能不是很用的上这一点，但是我们可以考虑怎么结合这个loss去做对应的分类器。 1.5.2. The value of labels 这一篇文章是将自监督学习和半监督学习应用到长尾分布的问题上，文章对应的仓库中可以get预训练模型和很多对应的数据，同时验证了下面两种策略都可以大大提升模型的效果，包括和之前的各种策略进行耦合。 半监督：利用更多的无标注数据 自监督：不利用任何其他数据，使用长尾分布的数据进行自监督训练 后续的实验过程中我可能也会遵循该设计，或者使用的是全数据的自监督预训练。 考虑尾部标签本身的意义，想要利用尾部的标签信息，又不受偏差的影响，实际上就是使用自监督进行预训练，然后后面使用各种方法兼容的一个策略。 1.6. MixUp in LT 将MixUP应用在LT中，试图\"以使其具有更高的泛化性，以及降低模型本身的置信度\"[4], 经过实验表明，仅在Stage1使用MixUP，在Stage2的第二阶段使用几个epoch的Mixup的效果可能会更好。 在这里可能也要考虑一下CutMix方法 1.7. Conclusion 1.7.1. 实验结果汇总 基于BackBone对这些方法的实验结果($Top1 Acc$)进行汇总，作为我们后续研究的参照：在进行实验的时候，我们需要首先调整好BenchMark，基于Benchmark做的改进才能和对应的方法进行对比。 整理原则： 对应的论文则由该论文本身为主，后续和LT的仓库进行对比分析； 最主要需要对比的应该是ce情况下的指标，这是我们最重要的，当这个指标对齐后，我们就可以和这些方法同台竞技了。 Dataset -> LT-Cifar-100 -> LT-CIfar10 Backbone Factor(Exp) 100 50 10 100 50 10 ResNet32 RESULT - - - - - - CE 38.32 43.85 55.71 70.36 74.81 86.39 - Focal Loss 38.4 44.3 56.8 70.4 76.7 86.7 - MixUp 39.5 45.0 58.0 73.1 77.8 87.1 - CB Loss 39.6 45.2 58.0 74.6 79.3 87.1 - BAGS-After 47.83 51.69 - 73.59 79.03 - - SSL-Uniform 40.40 45.04 57.07 73.50 78.20 87.72 SSL-Balanced 43.06 47.09 58.06 76.53 80.4 87.72 LDAM 42.0 46.6 58.7 77.0 81.0 88.2 - BBN 42.56 47.07 59.12 79.82 82.18 88.32 - Causal 44.1 50.3 59.6 80.6 83.6 88.5 - 1.8. Reference 阅读过程中还看到一些什么BAGS，进行数据分组的方法，这个方法肯定不会在我们的框架中使用，但是我们可以分析一下这种分组训练为什么会对长尾的场景存在差异。 ⭐\"Bag of Tricks for LT Visual Recognition with Deep Convolutional Neural Network\" ZHIHU ⭐\"BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition\" ZHIHU CVPR20 ❓\"Long-Tail Learning via Logit Abjustment\" ICLR 20 zhihu1 | zhihu2 \"Improving Calibration for Long-Tailed Recognition\" CVPR21 zhihu ⭐\"Distribution Alignment: A Unified Framework for Long-tail Visual Recognition\" CVPR21 zhihu | zhihu2 \"Decoupling Representation and Classifier for Long-Tailed Recognition\" ICLR20 \"Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect\" NIPS20 | zhihu \"Rethinking the Value of Labels for Improving Class-Imbalanced Learning\" NIPS20| zhihu \"Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification\" CVPR21 zhihu 总结性串讲： LT-Classification © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-14 14:51:07 "},"Experiment/Using_Judgment_Instead_of_Annotation.html":{"url":"Experiment/Using_Judgment_Instead_of_Annotation.html","title":"2 Weak Supervised","keywords":"","body":"1. Using Judgment Instead of Annotation1.1. Overall1.2. Think it again1.2.1. 假如使用模板匹配1.3. Localization1.4. Classification1.5. Analyze the effectiveness of algorithms1.6. Some Problem1.7. Experiment Detail v1：GCN FSL version1.7.1. 算法的一些修改细节1.7.2. :zap: Something we should pay attention to1.7.3. ®DEBUG和TEST 安排1.7.4. 实验结果1.7.5. Summary1. Using Judgment Instead of Annotation 为了解决数据集的标注过程中工作量过大，在实际情境下难以完成需要的数据集构建，或者成本过高的问题，参考了AL，FSL的一些思路，主要的思路分析如下。 在证实可行性之前，先简要分析一下主要的思路即可。 1.1. Overall 基于reinforcement learning(人类参与)和self-learning(机器自学习)为主体框架，以各种FSL算法作为backbone，然后再不断的迭代中，提升辨别和分类能力。 人类参与的形式：把Annotation的过程，转化成Judge的形式：分类的化好说，定位的话，judge可以不只是一个True or False。 尽量做成一个项数有限的多选的离散情况。Full correct-smaller-biger-need to pan -completely loss 之类的，同时也可以将2-3歌epoch作为一次iteration的周期，然后对迭代次数本身进行考察 把这样的Judge作为RL中的reward，或者说是作为一种对prior knowledge的补充，帮助算法学习到最佳的H。 对Judge的利用上也可以参考weakly-supervised的方法，当成一个弱标签 给出Judge的同时，在扩充数据集，完成后续的增量自学习的同时，是否能做一个BP，对现有网络进行一个初步的修正方向指示。 算法的核心仍然在于FSL的算法部分： 但是算法的meta-learner的边际增益在下降的话，有没有办法使得这个增益曲线，后移，让我们在可以容忍的地方，已经获取了足够的数据标注 for example： 一开始的few-shot 得到了50%的正确率，我们通过Judge能快速的建立50%的数据集，然后再兵分两路feed 正确和错误的数据进入网络，进行类似Label propagation的方法，然后就能从方法增益上获取更多的标注收益，然后在效益进一步下降之前，已经构建了一个足够大的数据集？ solve problem：边际增益递减 根据不同FSL算法的P特性，结合Full supervised的ML算法的特性，在不同的data situation下，采用不同的整合算法，从而取得我们需要的辅助数据标记效果？针对不同阶段适应不同网络。 3. 1.2. Think it again 1.2.1. 假如使用模板匹配 基本思路 训练一个分类上较好的embedding，然后通过聚类算法，（同时要参考slide windows）算法，减少需要匹配的模板（snips），然后基于Network（Embeding）的输出去匹配：（平移 伸缩，切分组合的相似性应该保持），匹配完输出类别结果，让人为的去判断。 Few-shot的情况，扩充完以后针对性的对各类的模板进行更新（META？）（增量学习？）好像有点Learning with External Memory的味道 将人为判断的模型，以强化学习的方式：reward，进行网络的输入，对网络进行改造：对embedding进行改造，得到一个具有更好性质（泛化能力，准确率）的embedding。 仍需思考 在视频分割层面怎么去做匹配之类的操作什么的 1.3. Localization localization的部分离不开聚类的理论，比如oic loss，就是从判别度之类的来切分出最具识别度（类内和类间的间距较大的embedding function）的片段？ “类内间距小，类间间最大”，对snip进行初次的切片，然后在对这些进行分类 聚类或模板匹配，在定位这样的层面，模板匹配是不是会表现得更好。 1.4. Classification 1.5. Analyze the effectiveness of algorithms 通过判断的方法，是否能以低于纯标注的工作量，有效的在指定的轮次（RL+Self Learning一次为一轮），也就是有限的人工干预次数后，达到令人满意的效果。 总结：bias 是否能够通过这样Judge(RL)+FSL的方法，After Limited Epochs，能够快速构建数据集。 实际上如果只是针对Classification本身，那么这样的方法应该改成：FSL +Self-learning的效益提升改进。用Meta-Learning的思路来看的话：关键就在于效益曲线，如果进一步提升效益需要的数据量随之增长的速度超过一定的Threshold，那么就没有必要做下去了。 如果是Video维度的时间定位~~图像维度的图像分割，那么在这个时候，把Label行为本身，转换成一个Judge的操作，才在Workload减少本身，有一定的意义。 1.6. Some Problem 倘若我们使用FSL的时候，N-way K-shot中N若大了，问题的复杂程度会上升，那么是不是算法的效果，会不够好，此时是不是应该从N比较小的时候，通过N-K扩充数据量，然后再逐渐的上升N，最后转化为full supervised 当我们的Judge为error的时候，如何对negative的pseudo-label在网络中进行利用 如何更好的结合localization和classification，可能要参考一下two-stream之类的，但是他们都是在用weight比较生硬的对Loss进行结合，那么有没有更有机的结合方式？ 1.7. Experiment Detail v1：GCN FSL version 1.7.1. 算法的一些修改细节 def ✅modify_eval(*parameterlist): '''在验证过程中增长正确识别的dict，或者同时构建识别错误的dict''' '''在不同的epoch训练的时候需要重新读取数据，但是对同一批数据集，我们不能进行简单的单次训练 要么通过实验效果的边际效益/算法稳定来进行选择，要么通过指定hyperparameter确定固定训练伦次''' # 应该是一开始进行普通的FSL，等算法效果稳定以后再执行人工干预操作 or 边际效益降低到一定程度的时候来做这个操作 pass # 通过对类的实例中的dict进行修改在进行调用 # 定义方法修改dict def ✅early_stop(*parameterlist): # 通过early stop来对dict进行reload和改进，可以在argument设置一个参数来控制early_stop的执行模式 # 添加一个状态值，然后通过状态值在eval中执行操作，例如如下的两种方式 MaxNum = len(singleclass) Judgement = len(dict1[class1])/MaxNum pass def ✅modify_dataloader(*parameterlist): '''尽量使用自己的数据模型，这样在后续进行开发的时候能够更方便的拓展到其他的算法 1. dataloader需要进行重构：因为需要阶段性的生成datalist 2. 可能需要直接对datalist进行操作，可能需要直接对该值进行操作，后面看看怎么做''' pass # ✅ 或者此中关键在于load_tr_batch()，但是问题在于，这个函数对数据的读取是随机进行的，有没有办法让函数读取到的是固定的list？ '''我觉得应该是可以的，现在是随机选择图片，所以我只需要让每个类别中的初始图片是固定的index就可以执行这样的操作，在获取数据的时候进行shuffle，然后再后续选择训练的时候不在随机选择即可，然后给每个class一个list包含他们所拥有的shot，然后基于这样的shot来设置训练中获取的shot参数''' # ❌是不是应该换个New项目来做 '''暂时先对之前的想法进行一个实现试试''' A = self_dataloader(*args) 1.7.2. :zap: Something we should pay attention to [x] 在改变训练模式后怎么做checkpoint？ 有没有办法保存已有的list的情况：可能没有必要，我们只需要保证模型继承，然后根据迭代的情况快速的对数据集进行更替就可以了。 [x] 训练指标：(不能使用本数据集的pretrain，会破坏实验的验证性) 所以需要删除原本使用预训练的模型，禁用预训练module，以及freeze_cnn [x] 首先我们需要使用修改过后的数据集来进行一次测试，看算法的效果是否能够保持在一个较高的水平，如果算法的效果切实可行的话，在执行加入了数据填充部分的代码测试 [x] positive class是指定每个batch中的正类把，然后将其他的样本作为该episode中的Negative，也就是负例，通过这样的设定去模拟一个完整的类别样本。这样需要重构之前编写的部分 1.7.3. ®DEBUG和TEST 安排 记录train和test两个阶段的实验结果 [x] 首先记录原训练结果:colab 上的元模型 记得删除预训练和freeze_cnn模块 记录几种情况下的效果 [x] 接着 修改成data_v2的数据读取，检测数据算法是否编写正确 算法的准确率与原本是否有区别 改写数据模块的正确性与否 这一部分&#x1F446;的第一次实验，在data_v2的编写上存在一定的问题，进行修改并重新实验查看结果。 [x] 最后将整体算法嵌入进去，看看效果 先debug，将算法调试到可以正常运行的情况下 然后根据表现调整具体算法的逻辑错误 比较算法表现 1.7.4. 实验结果 phase\\indicator(no pretrain)(no freeze cnn) Nway Nshot iter ACC Tr ACCTe origin GNN Few-shot learning(k80) 5 5 50000 58.69% 57% GNN Few-shot learning v2（modify the data loading）❌ 5 5 18000 48.52% 22.96-23.52% V2 改（没有意义，从loss和acc的表现来看，算法由于训练数据过少，后续的训练过拟合了，算法的效果根据数据的浮动很大） 5 5 16000 20%-62% ❌ V3：eval中的positive-shot随机 5 5 5 5 Mydemo 5 5 应该补充个指标，就是最终的数据情况 phase\\indicator(no pretrain)(no freeze cnn) Tr T/F Te T/F loss Tr loss Te origin GNN Few-shot learning 2871/4992 285/500 1.132 :negative_squared_cross_mark: GNN Few-shot learning v2（modify the data loading） 574/2500 2.692 :negative_squared_cross_mark: ❌ ❌ ❌ ❌ Mydemo 1064/4662 1.714 1.7.5. Summary 不该使用第三方重构的代码，去理解，破坏了原作者的逻辑 对fewshotlearning的理解和positive class的理解需要进一步分析 对meta-learning 的问题设置有初步的了解 如何去编写数据 positive class，就是只识别positive class 其他的都当成负类，所以每个episode都只输出一个预测结果 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 23:58:44 "},"Papers/FSL前期调研.html":{"url":"Papers/FSL前期调研.html","title":"2.0 FSL Survey","keywords":"","body":"1. 确定突破口需要的调研1.1. 主要是limited labels & Few Samples & Data programing1.1.1. PART1 Limited Labels （base on LiFeiFei‘s reference）1.1.2. PART2 key words searching（such as few samples etc.）1.2. - end1.2.1. PART3 few-shot learning etc.（including one-shot learning)1. 确定突破口需要的调研 1.1. 主要是limited labels & Few Samples & Data programing Weakly supervised learningsemi-supervised in video fieldif we can recoding this work?多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注元学习、浅层神经网络的概念等等 semi-supervised 1.1.1. PART1 Limited Labels （base on LiFeiFei‘s reference） in this part we may list the paper which is useful for my recoding. 还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。distant supervision(it's kind of early) can be another baseline for our method, we need to understand how this method work for that situation distant supervisor到底是什么机制可以去CSDN什么的看一下 Transfer Learning\\label propagation算法也是这一块重要的baselineBaseline：scene graph prediction with limited labels reference： ×Induction of decision treesif want download, try google it Pattern Learning for Relation Extraction with a Hierarchical Topic Modelmaybe we'll need this paper,when we try to recoding.nope.当我们写论文需要理论基础的时候可能需要， √Data Programming: Creating Large Training Sets, Quicklyit's important to see if this article have same idea with me? it's kind of learning paradigm,是一种构建数据集中，标注数据的范式，通过这样的method可以对多种labeling function进行整合，同时减少标注的误差和overlap情况的解决。后续我们实现方法的时候可以参考一下这个的数学理论，帮助在实际中进行应用。（本文中对这里的noise-aware的损失函数进行了应用，使其适应概率标签从而抑制噪声。）严重怀疑这是snorkel算法中的引文，直接引用过来 ×Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relationsfigure out how can knowledge-based benefit weakly supervised learning ?nope. NIP method. 运用语言结构，类似于发的东西，来对文本进行补全。如果我们需要了解基础知识怎么使用，可以尝试参考。 ※Realistic Evaluation of DeepSemi-Supervised Learning AlgorithmsNIPS 2018深度半监督学习的现实性评估：公布了统一的重新实现和评估的平台（方式？），（针对于算法在生产现实之中的适用性发布的一个标准。）based on analysis of image algorithm：半监督的方法通过对未标注的数据中的结构范式进行学习，从而降低了对标注数据的需求，也就是说输入的数据是大部分未标注和少量标注数据，就可以逼近完全标记数据集的表现效果。[32,50,39]，这几个针对于图像情况的方法。分析后发现，他们评估算法的方式并不适用于将算法推广到实际的生产领域中，于是我们对于评估ssl算法的方式提出了一些新的看法。说的就是以前这些ssl在算法的效果上可能作弊的一些方面，如果使用这样统一的标准对算法进行评估的话，才能使得算法得到一个好的效果，此外他也提出了一些ssl在训练过程中的一些棉铃的问题：比如说假如我们把其他类别的数据，混入其中的话，那么所有这些ssl的算法的效果都会受到极大的影响。 ×Learning from labeled and unlabeled data with label propagation这是一个很重要的方法，但是没想到这个竟然这么早？（math or algorithm？）（2002）（最近读的另一篇论文好像就借鉴的这个思路）将数据从标记的高密度区域向未标记的数据区域进行传播，这一篇论文的话，主要存在一些数学推导，我建议从19的那一篇新的标签传播开始阅读，通过这篇来补全需要的数学基础，如果另一边已经讲述的很完备了就不需要这篇的内容了 1.1.2. PART2 key words searching（such as few samples etc.） limited labels on github： ※microsoft's work:github|paperi think it’ll be important one，we‘ll need to think carefully about this.seems like they have already make it great.思路上可以给我们启发：提出了一个针对few samples的通用框架（通过度度量传播来进行的label propagation方法），来解决无论是transfer、semi-supervised、few-shot这样解决的问题，并有了一个巨大的提升。将少量标记的label propagation到大量的未标注数据上，从而创建训练数据。主要贡献：用于传播的相似性度量从其他相关域转移时，这样的标签传播方法非常有效。这个算法框架可以细读一下，后续关照一下具体的思路和实现 ※The Limited Multi-Label Projection Layer:github|paperCVPR19LML projection Layer 是一种几何的K类预测，用来再多类别少样本的情况下取代softmax的一个映射函数，这一篇主要是数学理论，在最后实现的话，要进行一定的参考和学习。 ※Learning Classifiers for Target Domain with Limited or No LabelspaperICML2019从所有的训练数据中学习一个混杂（mixture）的“原型”，然后将原型撕裂成一个个part/part-type（用attention机制来实现）、 然后通过多个part的概率组合来表示一个new instance。（use MACNN）。（即将变成低维的概率向量组成的编码=低维视觉属性LDVA）到时候找个图把这些方法全都对比一下。md花样太多了，玩晕了。 Hand and Face Segmentation with Deep Convolutional Networks using Limited Labelled Data(论文还没出)github 一些奇奇怪怪的github项目github1 limited labels on google scholar： √Large Scale Sentiment Learning with Limited LabelsSIGKDD2017他是通过对tweet的表情数据进行标注，建立的数据集，使用了self-learning和co-training两种WSL的方法，来对未标注的数据进行标注两种方法的具体注解我已经放在pdf上了 ×Large-Scale Video Understanding with Limited Training Labels这尼玛是本综述的书，吓老子一跳 few samples on google scholar： ×Learning Convolutional nerual networks from few samples2013this paper use the method of pre-trained (transfer learning instead nowadays) to get a satisfatory result.这篇文章太早了，需要的话再重新说吧。先不看 ※※Few-Shot Learning with Graph Neural Networks（two-version）（2017）（2018ICLR）using graph network to implement semi-supervised. this research prove that the graph method perform well on 'relathinal' tasks.定义了一种图模型的网络框架来实现few-shot等few samples的任务，表明这样的图网络架构能够很好的实现关系这样的处理，也很容易在这样的情境下进行拓展，这也是一个框架设计的任务。但是我们能够从中学习一下图模型如何针对关系网络进行学习和训练的，以及探讨一下图网络的优势。这一篇文章也探讨了度量学习和元学习的一些东西，这一篇可以给一个高的阅读优先级。 data programing: 1.2. - end Label propagation: ※Active Frame Selection for Label Propagation in VideosECCV2012decide how many frames we'll need to mark by human for the best result .文章通过动态规划来选定视频中的k个frame，作为key frame，通过这几个frame的人工标记，能够最大的降低算法在label propagation中的标记误差，（其中num of k和误差的权衡还不是特别清楚）取代了以往这个key frame选择的随机性，带来更好的性能。此外这个方法还关注于帧数选择的动态性，由于视频的独立性，所以固定帧数的选择不一定是合适的，应该根据视频本身的特性来选择才是更好的。（但是不知道时空复杂度怎么说）值得一提的是，文中还提到了一些辅助人工标注的算法，这些算法有时间的话可以通过CSDN去调研一下。（防撞车） √Dynamic Label Propagation for Semi-Supervised Multi-class Multi-label ClassificationICCV2013是一个基于图的方法，和eccv2012一致的地方在于，都认为视频任务的标注任务中，动态规划的part是需要的，上一篇用动态规划来实现keyframe的选择，这篇文章这是完全的semi-supervised的任务，他用dynamic的办法，动态的对多标签和多类信心进行拟合，从而动态的去更新相似性的度量，使用KNN来保留数据的固有结构。 ※※Label Propagation via Teaching-to-Learn and Learning-to-Teach2016TNNLS一个迭代的label propagation方法，结合了一定self-learning 的机制，从dataset中迭代的选出易于分类的部分，然后通过不断的对这种易于标注的数据中去self-learning，从而提高分类器的性能，然后逐步的去针对模糊边界进行propagate。感觉是一个好方法intro中简要的对比介绍了这之前的一些label propagation方法，包括DLP。 based on the sota LPmethod，所以之前的一些可能可以不用看了， ※※※Learning to Propagate Labels: Transductive Propagation Network for Few-shot LearningICLR2019结合了meta-learning/label propagation/transductive inference的方法，细看细看，这一篇一定要细看。太强了兄弟。intro里面也包含了很多的东西。 1.2.1. PART3 few-shot learning etc.（including one-shot learning) 淦淦淦，这尼玛比的定义能不能统一一哈 Few-Shot Learning: √Prototypical Networks for Few-shot Learning2017\\NIPS思路上好像和19年的cvpr那片有点像，先学习一个overall 再通过度量空间对newdata进行适应性的分配和训练。通过intro，我认为更像是一个简单的embedding的办法，将sample聚集到embedding space的一个原型上，在对其进行近邻标签传播算法把。但是里面有一些数学推导，可能是关于距离的，在我们后续需要划分指标的时候可以来看看这篇到底说了啥。（原型网络的数学推导。） √Meta-Learning for Semi-Supervised Few-Shot Classification2018、ICLR正好是上面那片原型网络的升级方法，这也太巧了把。重开一个新的课题，设置环境成为一个wild的环境，存在干扰项，将未标注的data也混杂进原型的训练中。 ×Conditional Networks for Few-Shot Semantic Segmentation2018\\ICLRworkshop track貌似有点弟弟，没提出什么有用的东西 ※Few-Shot Object Detection via Feature Reweighting2019/ICCV在一个base class 的dataset上进行meta training，然后通过 reweighting 操作，adapt to novel classes。global原型，meta的场景学习策略，transfer的reweighting操作，以及在few-shot问题种加入了很多算法并没有考虑的localization问题。这篇论文看起来还行。 ※※Meta-Transfer Learning for Few-Shot Learning2019\\CVPR通过多次的meta学习，来找到参数相对于原DNN网络（普通的meta都是用的浅层网络）而言的scaling和shifting，感觉和上一篇reweighting方法存在一定的相似性。 同时我们也知道基本的meta-learning 方法和场景图应用的方法存在极大的相似性。 此外在训练策略上，采用了一个HTmini-batch的变体策略。（figure1有简要说明，结合后面的策略观看） ×Deep Learning Models for Few-shot and Metric Learning这一篇看不了 √Learning to Compare: Relation Network for Few-Shot Learning2018\\CVPRmeta-learning 中的query 和support 不要搞混了。（前面还有一步是通过embedding来学习一个合适的feature）感觉上是一个基础的meta-learning框架，通过训练过程中对metric distance的学习，得到一个模型框架，然后通过模型将support data在metric space中与query data进行distance的衡量，从中选择shortest one作为classification的指标。 ※Dynamic Few-Shot Visual Learning Without Forgetting2018\\CVPR为了使得模型在学习新的类别的时候，对旧的类别的识别能力依旧能保留下来，提出了两个策略，一个是基于attention 的分类权重生成器，二是对ConvNet进行重新设计，使其提取出feature的表征向量和分类权重向量之间的余弦相似性。？具体的还没看。但我认为主要努力的方向好像不是很对。 Few-Shot Human Motion Prediction via Meta-Learning2018\\CVPR是一种结合了MAML、MRN、Meta-Learning的策略，本质还是一个few-shot的工作，没有提到怎么把这样的工作适应到真实的应用上，这一篇论文非常需要机器详细的阅读，不然的话不知道他到底是怎么操作的。 最终我们可以提出一个framework，通过对弱监督方法的嵌入，使得标注的任务变成一个人机交互的loop，通过我们对算法的干预，他将从标签的概率预测变成一个确定的指标预测，然后执行self-learning的方法，让自己逐渐变得更好，设定一个drop out，可以计算一个算法的最终所求时间。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 16:40:34 "},"Papers/FSL-Collection.html":{"url":"Papers/FSL-Collection.html","title":"2.0 R2","keywords":"","body":"1. Survey For Few-Shot-Learning1.1. Abstract1.2. Notation and Terminology1.3. Main Body1.3.1. Overview1.3.2. Data1.3.3. MODEL1.3.4. ALGORITHM1.3.5. Future Works1.4. Appendix1.4.1. Reference:1.4.2. Additional Vocabulary：1.5. FAQ1.5.1. 需要思考的点1.5.2. 但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！1. Survey For Few-Shot-Learning @aikenhong 2020 @h.aiken.970@gmail.com 另一个综述文章：https://zhuanlan.zhihu.com/p/61215293 对该文中一些内容有一些补充，可以看看 FSL简介：https://blog.csdn.net/xhw205/article/details/79491649 GCN用于FSL：https://blog.csdn.net/qq_36022260/article/details/93753532 1.1. Abstract FSL的根本目的就是弥合人工智能和人类之间的鸿沟，从少量带有监督信息的示例中学习。像人类一样有很高的泛化能力。这也能解决在实际应用场景中，数据难以收集或者大型数据难以建立的情景。 FSL的核心问题是：经验风险最小化器不可靠；那么如何使用先验知识去解决这个问题？ 三个主要的角度： 数据：使用先验知识增强数据的监督经验 模型：使用先验知识来降低假设空间 算法：使用先验知识来改变搜索最佳假设（来进行搜索？) 现阶段针对FSL提出的一些相关的机器学习方法： meta-learning; embedding learning; generative modeling etc. 本文的主要工作： 基于FSL的原有设定，在现阶段的FSL发展上给出正式定义，同时阐明具体目标以及解决方式 通过具体示例列举和FSL的相关学习问题，比较了相关性和差异性，更好的区分问题 指出核心问题：经验风险最小化器不可靠，这提供了更系统有组织的改进FSL的方向。 经验风险最小化器&#x1F449;：基于ML中的错误分解来分析的 整理，更好的理解 未来方向 1.2. Notation and Terminology 一般基于参数方法（因为非参数方法需要大量数据），在假设空间中搜索最优假设，并基于基于标签的Loss Function 来衡量效果。 1.3. Main Body 1.3.1. Overview 2.1：具体定义&示例 2.2：相关问题和FSL的相关性和差异 2.3：核心问题 2.4:现有的方法如何处理这个问题 Definition 基本定义：FSL是一类机器学习（由E，T，P定义），其中E只包含有限数量的带有目标T监管信息的示例。 研究方法：通常使用N-way K-shot的分类研究方法：从少量类别中的少量样本回归出ML模型， ​ Training Set Contains：KN examples. Typical scenarios of FSL: Reducing data gathering effort and computational cost: “raw images of other classes or pre-trained models ” 似乎有点迁移学习的味道了，改善从已有的类似数据集过来的model？ Learning for rare cases Acting as a test bed for learning like human. 和普通的ML的应用最明显的区别就是E中prior knowledge的应用，将T和先验知识结合起来。（such as Bayesian Learning [35,76]） Attention：Zero-shot：要求E中需要包含其他模态的信息（比如属性，wordnet，word embedding之类的） Relevant Learning Problems WSL:Weakly supervised learning: 重点在于learns from E containing only Weak supervised（such as： 不完全，不精确，不准确，或者充满噪声的监督信息），WS 中信息不完全只有少样本这一类情况就是FSL了，在此基础上基于Oracle还是人工干预的方法，可以进一步细分为： Semi-supervised learning： 从E中的少量标记样本和大量未标记样本中学习。示例：文本和网页分类；其中包含Positive-unlabeled learning这种特殊问题，只包含positive label的问题：具体而言就是，只知道用户现在用户中标记的好友，而与其他未标记人之间的关系是未知的。 Active Learning： 文章：选择信息量最大的未标记数据区query an ordacle？ 个人理解：选择信息量最大（通常用不确定性大的数据表示）来让人标注，从而构建数据集，让算法能够通过较少的数据标注操作实现更好的效果。 WSL with incomplete supervision 仅仅包括分类和回归的内容，而FSL还包含RL问题；WSL使用unlabel data 对E进行扩充，而FSL更多的使用各种类型的prior knowledge来扩充E，包括pre-train model ，其他领域的监督数据，未标记数据等等。 Imbalance learning： 数据集的分布不均衡，比如一些y值很少用到的情况。IL从所有可能的y中Train&test，FSL基于少量案例train&test y，同时也可能基于一些先验知识来。 Transfer learning： transfers knowledge from the source domain/task &#x1F449; target domain/task, where training data is scarce.其中Domin adaptation，是一种TL：source/target tasks are the same but the source/target domains are different.举例说明就是：情感识别，一个基于电影评论，一个基于日用品评论。 Transfer Learning广泛的应用于FSL，[7,82,85]将先验知识从源任务转移到Few-shot task，从一些训练数据丰富的源域转移 Meta-learning：感觉正文中讲的是个狗屎，后续通过附录中的看看 Meta-learning methods can be used to deal with the FSL problem. the meta-learner is taken as prior knowledge to guide each specific FSL task. Core Issue 经验风险最小化 Machine Learning其实是一个经验风险最小化的模型 R(h)=\\int \\ell(h(x), y) d p(x, y)=\\mathbb{E}[\\ell(h(x), y)] \\\\ R_{I}(h)=\\frac{1}{I} \\sum_{i=1}^{I} \\ell\\left(h\\left(x_{i}\\right), y_{i}\\right)\\\\ \\mathbb{E}\\left[R\\left(h_{I}\\right)-R(\\hat{h})\\right]=\\underbrace{\\mathbb{E}\\left[R\\left(h^{*}\\right)-R(\\hat{h})\\right]}_{\\mathcal{E}_{\\mathrm{app}}(\\mathcal{H})}+\\underbrace{\\mathbb{E}\\left[R\\left(h_{I}\\right)-R\\left(h^{*}\\right)\\right]}_{\\mathcal{S}_{\\mathrm{est}}(\\mathcal{H}, I)} 上面这1 3的区别一个是在全空间上，另一个是在是我们的假设空间中，能取到的最优解。 总体误差可以基于最小预期风险和最小经验风险来表示，如等式3。期望实和训练集的随机选择有关的，the approximation error 衡量了假设空间中的函数能够接近最优假设的程度，the estimation error 衡量了，最小经验误差代替最小期望误差在假设空间内的影响。 data （which provides Dtrain）数据角度 model which determines H（embedding function，转换到假设空间） algorithm（searches for the optimal h）学习算法，下降方向 不可靠的经验风险最小化 如果数据足够大的话，通过少量样本计算出来的假设空间就可以逼近实际上的最优假设空间，也就能得到一个很好的近似，但是在FSL中，可用的样本数很少，所以可能没办法产生很好的逼近，在这种情况下，产生的经验风险最小化指标hl过拟合，这就是FSL中的核心问题。 Taxonomy 为了解决FSL问题中经验风险最小化工具中hl的问题，prior knowledge是至关重要的，利用先验知识来扩充信息量的不足，基于先验知识的类别和使用方式就能对FSL works进行分类。 Data：通过数据增强等方式，增加数据量，从而使得经验风险最小化因子能够更加的准确。 Model：用先验知识来约束假设空间，使得需要搜索的范围变小，那么基于较少的数据也能够得到一个较好的估计，（相比原来） Algorithm：使用先验知识，来搜索最优假设的参数，基于这些先验知识提供一个较好的initialization，或者guiding the searching steps 1.3.2. Data 通过手工指定规则来进行数据增强的方式例如：:arrow_double_down: 很大程度上取决于领域的知识也需要人工成本，此外，这样的方式在数据集间的泛化能力很差，一般都是针对性设计，而且这样的不变性，不可能由人工穷举出来，所以这样的方式不能完全解决FSL问题。 translation, flipping, shearing, scaling, reflection, cropping, rotation. Advance data augmentation: Transforming Samples from Dtrain 对训练集的数据进行几何变化处理，生成其他的样本，构建一个更大的数据集。 从相似类中学习一组编码器（每个编码器代表一个类内可变性），将这些习得的变化量添加到样本中形成新的样本。 基于差异从其他类别中转移过来 从一个样本变成多个；连续属性子空间来添加属性变化 基本思路是一致的，通过变换，在原本数据的基础上，构建新的数据，只是有着不同的构建方式。详细的各种类型的构建可以看参考文献。 Transforming Samples from a Weakly Labeled or Unlabeled Data Set 基于弱标签或者无标签的数据来进行数据增强的情况，类似视频中有些事件之间变化比较大的情况，可以将这样的数据添加到训练集中来更清楚的预测。 :dagger:但是如何筛选哪些有需要的弱监督数据？ ⭐基于训练数据训练一个svm进行筛选，然后将具有目标的示例添加进数据集 ⭐Label Propagation,直接使用未标记的数据集 ⭐也有文章采取逐步从信息量最大的数据中筛选的做法 Transforming Samples from Similar Data Sets :tada: 汇总和改造相似的数据集，来扩充Few shot情况，基于样本之间的相似性度量来确立权重，典型的方法就是：使用GAN，生成器将Few-shot的训练集映射到大规模数据集，另一个生成器将大规模数据集的样本映射过来，从而训练出可以辅助样本迁移的模型。 Summary1 这些方法的使用取决于具体任务； :x: 缺点：通常是针对数据集量身定做的 :+1: 针对这个问题有人提出了AutoAugment :heavy_multiplication_x: 缺点：文本和音频的情况下就很难做这样的生成了 1.3.3. MODEL :fire: 如果仅仅基于简单的假设去考虑的话，那么可能在我们的假设空间中的最优和实际的最优(不足以模拟现实社会中的复杂问题)会有比较大的距离，但是如果考虑复杂多样的假设空间，那么标准的机器学习模型也是不可行的（数据量不足以优化到最优解），考虑使用先验知识，将复杂多样的假设空间H 约束到较小的情况下进行学习，这样的话经验风险最小化器将会更加的可靠，同时也降低了过拟合的可能性。 根据先验知识的类型，可以划分成如下几种FSL： Multitask Learning :fire: 多个相关任务协同训练，基于特定任务的信息和通用任务的信息来一起学习，其中利用某些/其他任务的大量数据（源任务），在训练过程中，通过学习到的参数来对只有Few-shot（target task）进行约束。基于训练中参数对target的约束方式可以分为 parameter sharing参数共享 160 61 95 12 基本的网络架构如下图 :zero: 有多种不同的架构，整体都是由共享层（参数是一致的）和特定于任务的层一起构建的，简单的描述一下如下： 初始共享然后分配到特定任务；2. 源任务（pre训练）训练共享层，目标任务训练目标层；3.分别单独学习再有共享的编码器嵌入成一体。 parameter tying参数绑定 45 151 85 ⭐基本思路：鼓励不同任务之间的参数存在相似性。对参数进行正则化是一种流行的方法。 :one: 有的方法对成对参数之间的差异及逆行了惩罚，从而确保参数分布的相似性 :two: 有的方法通过针对源任务和目标任务设置不同的CNN，之间使用特殊的正则化术语对齐。 Embedding Learning 基于先验知识（同时可以额外使用Dtrain中的任务特定信息）构建样本的一个低维嵌入，这样便能得到一个较小的假设空间，同时相似的样本会紧密接近，而异类的样本更容易区分。 Key Components： 将测试，训练样本用embedding函数（f，g）嵌入。f，g可以统一，但是分离的时候可以受获更好的准确度 相似性度量在嵌入空间（一般都是维度更低的空间）进行， 可以根据embedding函数的参数是否随任务变化分类 针对任务的嵌入模型 仅仅使用来自该任务的信息来学习针对性的嵌入模型。 通用的嵌入模型（task-invariant） 使用有足够样本且具有各种输出的大规模数据集，学习通用的embedding function，然后直接用于Fewshot。Recently, more complicated embeddings are learned [70, 150] by a convolutional siamese net [20] 通常而言，task-invariant不会使用Few-shot的数据集来更新embedding function参数，但是，其中很多情景都会模拟few-shot 的情景来训练embedding从而确保对此类任务有更好的概括性能。 ⭐Mathching Nets meta-learning / resLSTM / AL /set-to-set ⭐Prototypical Networks ​ embedding(xtest)不与每个g(xi)对比，而是每一类别的训练数据都有一个”原型“（原型公式如下），与原型对比，减少计算量。有两种变体：应用到matching-net 和 semi-supervised-108（软分配未标注的样本用以增强Dtrain） c_{n}=\\frac{1}{K} \\sum_{i=1}^{K} g\\left(x_{i}\\right)c_{n}=\\frac{1}{K} \\sum_{i=1}^{K} g\\left(x_{i}\\right) :zap:Other Method ​ ARC：利用attention+LSTM将xtest的不同区域和原型进行比较，然后将比较结果作为中间嵌入，在使用biLSTM（双向LSTM）进行最终嵌入； ​ Relation Net 使用CNN将Xtest和Xi拼接在一起，再使用另一个CNN输出相似度得分。 ​ GNN：利用GNN使用临近节点的信息 ​ SNAIL简单神经注意力学习器（RL通常看重时间信息）：temporal convolution +Attention，聚合临近步长和通过Attention选择特定时间步长的信息。 混合嵌入模型，可以编码 task-specific 和 task-invariant 的信息 虽然task-invariant可以再迁移的时候减少计算成本，但是针对一些特殊的少样本情况，他是无法直接适应的，比如说原本就是小概率事件（异常），这种情况下，基于Dtrain训练的先验知识来adapt通用的embedding模型，从而组成一个混合的结构，如下图所示。 ​ Learnet从多个meta-training set中学习meta-learner，并将训练实例映射成网络中的参数（convolutional Siamese net），这样f的参数就会随着输入改变。还有一些针对其的改进 TADAM：将类别原型平均化到嵌入中，并使用meta-learned 映射成圆形网络的参数 DCCN：使用固定的滤波器，并从Dtrain中学习组合系数。 Learning with External Memory 基于Dtrain 训练一个Embedding function，提取出 key-value的知识，存储在外部存储器中，对于新样本（test），用Embedding&相似度函数查询最相似的slots，用这些slots的组合来表示样本，然后用简单的分类器（like softmax）进行分类预测。由于对M操作成本高，所以M通常尺寸较小。当M未满时，可以讲新样本写如空闲的存储插槽。 key-value的表征，也就是memory中的定义在这个方法中至关重要，它决定了键值对对test的表征水平。根据存储器的功能，将这类方法分成两种类型： :one:Refining Representations: MANN：meta-learns embedding f，将同类的样本映射到同一个value，同一类的样本一起在内存中优化类表示。可以看成ProtoNet中精致的类原型。 当且仅当M不能很好的表征x的时候更新M。 The Abstract Memory： 使用两个M，一个基于大量数据训练出的固定键值对，另一个从固定键值对对少量类进行精炼提取。为此有的方法会注意保留M中的FS。 few-shot在M中很容易被其他samples的值表征从而取代，为了解决这个问题，提出的此算法&#x1F447; lifelong memory：通过删除oldest slot来update M，同时给所有slot的期限置为0，当新样本在经过M后输出的表征与实际输出匹配的时候，就合并，而不更新M。（但是还是没有真正的解决这个问题） :two:Refining Parameters: MetaNet、MN-Net： 对特定任务的数据进行fast 学习，而通用任务slow更新，然后结合memory的机制。（参数化学习 Generative Modeling 借助先验知识，从x的分布中估计先验p(x；$\\theta$ )的分布，从而估计和p(x|y)和p(y)，基于这样的先验数学模型进行后续的计算。而先验估计过程中通常是从别的数据集获悉的先验分布中，基于某个潜在的参数z迁移过来的如下式，这样就能基于既有的后验分布，约束H假设空间。 x \\sim \\int p(x | z ; \\theta) p(z ; y) d z 通常在识别，生成，反转，重建中有较常见的应用 Decomposable Components： 基于人类的认知，将数据分解成组件级别，在进行后续的识别和重组；利用类间的通用性； Groupwise Shared Prior： 新的FS类别，先通过无监督学习分组，共享组内的类别先验，然后基于组内的先验对其进行建模。 Parameters of Inference Networks：网络参数推理 p(z | x ; \\theta, \\gamma)=\\frac{p(x, z ; \\theta, y)}{p(x ; y)}=\\frac{p(x | z ; \\theta) p(z ; \\gamma)}{\\int p(x | z ; \\theta) p(z ; \\gamma) d z} 为了找到最优的$\\theta$ ，必须最大化以上的后验概率： 基于数据对其进行求解，inference network能够高效的迁移到新任务，但是inference network 需要大量的参数，所以通常需要在辅助的大规模数据集训练后才使用。很多经典的推理网络都可以在FSL上应用，比如VAE（可变自动编码器），autoregressive model，GAN，VAE+GAN Summary2 详细的优缺点，参考文章 存在相似任务或者辅助任务：多任务学习 ​ 包含足够的各种类别的大规模数据集：embedding方法 存在可用的内存网络：在内存顶部训练一个简单的模型（分类器），可以简单的用于FSL，主要是要精心设计更新规则。 除了FSL还想要执行生成和重构的任务的时候：generative modeling 1.3.4. ALGORITHM 算法层面的改进指的是在最优空间H搜索H*的策略，最基础的有SGD。:x:在FSL的情况下，数据会使得更新次数不够多，同时也没法基于交叉验证找到合适的补偿之类的。:arrows_counterclockwise:本节中的方法用先验知识来影响$\\theta$ ，具体体现为：:one:良好的初值；:two:直接学习优化器以输出搜索步骤；​ :zap:基于先验知识对策略的影响，对算法进行分类 Refining Existing Parameters :jack_o_lantern:基本思想：从相关任务中预训练模型的$\\theta$0作为一个良好的初始化，然后基于训练集的几次训练来adapt。 Fine-Tuning Existing Parameter by Regularization： 如何解决overfit的问题是此类预训练算法设计关键：其中一种方式就是依赖正则化操作来adapt参数。 正则化的方式主要有以下几种： | Method | Analysis | | ------------------------------------------- | ------------------------------------------------------------ | | Early-stopping | 监视训练过程，性能没有提高则停止学习 | | Selectively updating $\\theta$ | 根据具体问题，选择需要的部分来更新参数，不更新所有参数 | | Updating related parts of $\\theta$ together | 聚类$\\theta$，然后共同更新每个组，BP更新$\\theta$ | | Using a model regression network | 捕获任务无关的transformation，基于function进行embedding的映射？ | | | | Aggregating a Set of Parameters： 聚合相关模型：不贴切的比如眼口鼻到脸；具体使用上，unlabeled/similar label dataset,的pretrain model参数到FSL参数的适应。 unlabeled dataset: 把相似样本分组聚类，然后adapt similar dataset: 替换相似类别中的特征，重新使用已训练的分类器，然后对新类调整分类阈值。 Fine-Tuning Existing Parameter with New Parameters： 仅仅对模型迁徙可能没办法对FSL完全编码，所以我们在对参数进行adapt的时候加入一个新的参数，然后再Dtrain中同时adapt现存参数和learn新参数 Refining Meta-Learned Parameter 本节中细化meta-learned的参数学习：$\\theta$再过程中是持续优化的，不是固定的。 Model-Agnostic Meta-Learning（MAML）通过梯度下降来元学习 $ \\theta$ ,基于该参数，得到任务特定参数$\\phi~s$,更新公式类似如下形式$\\phi{s}=\\theta{0}-\\alpha \\nabla{\\theta{0}} \\mathcal{L}{\\mathrm{train}}^{s}\\left(\\theta{0}\\right) . $ 其中$L^s train$ 是训练样本的损失和， $\\alpha $ 是步长，该参数$\\phi~s$,对于样本的顺序不受影响，此外元学习中基本的参数更新公式如下$\\theta{0} \\leftarrow \\theta{0}-\\beta \\nabla{\\theta{0}} \\sum{T{s} \\sim P(T)} \\mathcal{L}{\\text {test }}^{s}\\left(\\theta{0}\\right)$ ，其中测试误差是整个过程中损失的和。通过元学习将参数转移。 最近针对MAML提出了主要再以下三个方面的改进： :zap:合并特定任务的信息：MAML为所有任务提供相同的初始化，但是这样忽视了特异性，所以，从一个好的初始化参数的子集中为新任务选择初值 :zap:使用meta-learned $\\theta$的不确定性去建模：结合AL :zap:改进refining过程：对$T~s$使用正则化？ Learning the Optimizer 不使用梯度下降，学习一种可以直接输出更新的优化器，无需调整步长$\\alpha$ 和搜索方向。 LSTM+Meta-Learner？ Discussion and Summary: 通过对现有参数进行微调，从而减少H需要的搜索量： 使用现有$\\theta$作为初始化：牺牲一些精度换取速度 另外两种策略都是依赖于元学习，元学习可以让参数和指定任务更为接近，还有一种直接充当优化器。 1.3.5. Future Works 在未来的FSL中使用多模态的prior knowledge SOTA网络架构的使用来改进data，algorithm，model； AutoML在FSL任务中的应用 meta-learning中动态学习中，如何避免catastrophic forgetting 在各领域中的应用：CV，bot，NLP，Acoustic signal process，etc :zap:Theories： FSL使用先验知识来弥补缺少监管信息的情况； FSL很多时候和domain adaptation 有关系 FSL的收敛性研究还没有完全了解 1.4. Appendix 1.4.1. Reference: 之后整理一些可能需要阅读的reference 只关注小样本的概念学习和经验学习的Another FSL survey: J. Shu, Z. Xu, and D Meng. 2018. Small sample learning in big data era. arXiv preprint arXiv:1808.04572 (2018). FS-RL，在仅给出少量状态和动作对组成的轨迹的情况下找到一种策略： [3,33] Bayesian Learning : [35,76] 1.4.2. Additional Vocabulary： 序号 希腊字母 Markdoown 序号 希腊字母 Markdoown 1 α \\alpha 19 β \\beta 2 γ \\gamma 20 δ \\delta 3 Γ \\Gamma 21 Δ \\Delta 4 ε \\varepsilon 22 ϵ \\epsilon 5 ζ \\zeta 23 η \\eta 6 Θ \\Theta 24 ι \\iota 7 θ \\theta 25 κ \\kappa 8 Λ \\Lambda 26 λ \\lambda 9 μ \\mu 27 ν \\nu 10 ξ \\xi 28 ο \\omicron 11 Π \\Pi 29 ρ \\rho 12 π \\pi 30 τ \\tau 13 Σ \\Sigma 31 Φ \\Phi 14 σ \\sigma 32 ϕ \\phi 15 Υ \\Upsilon 33 Ψ \\Psi 16 υ \\upsilon 34 ψ \\psi 17 Ω \\Omega 35 ω \\omega 18 φ \\varphi 36 Ξ \\Xi 术语或生词: empirical risk minimizer ：经验风险最小化器 ultimate goal ：最终目的 To name a few： 举几个例子 autonomous driving car：自动驾驶汽车 tackled：解决 paradigm：范式 ethic：道德 taxonomy：分类 the pros and cons of different approaches：不同方法的利弊 with respect to：关于 the approximation error：逼近误差 the estimation error：估计误差 alleviate：缓和，减轻 aggregation：聚集 simultaneously：同时，兼 penalized：受惩罚的 hybrid:混合的 interleaved：交错的 denominator：分母 注意区分： sufficient：足够 terminology：术语 refine：提炼 提纯 leverage：利用 latent：潜在的 1.5. FAQ [x] Testing Set需要在N-way上进行吗？应该是要的 [x] AL的query or oracle 是啥意思 [ ] According to whether the oracle or human intervention is leveraged, this can be further classified into the following 此处 oracle到底是什么意思 [x] Semi-Supervised 又和FSL有什么区别呢 [x] Imbalance Learning确实不是很理解 [x] Generative Modeling的具体要素不是很懂 [x] Core Issue中的三个h的关系还有点疑惑 app：在最优的情况下，能搜索到的最优解和实际最优解之间的差距 est：实际的假设空间中的最优解和基于少量样本的经验得到的假设空间中的最优解之间的距离。 [ ] Parameters of Inference Networks，不知道怎么理解，后续要补充 -- 1.5.1. 需要思考的点 [x] Few-shot-learning & meta learning的问题设置，就是多类中都有足量样本，然后随机的从多类中选取few-way和few-shot的data模拟多种meta环境（fewshot和fewway），单次训练都是小样本的情况，进行学习，在这种环境下学习到，一个模式，然后从而减少数据量的要求。（这样就哪里减少了数据量啊，我就没懂了，） [ ] 那么假如说没有多类动作（怎么构造多类动作）：不会，我们可以在网上爬取，或者自己拍摄，因为只需要少量有标注的数据即可，也就是positive数据可以比较容易获得。 [ ] 那么我们在构造增量的时候，也是考虑边际效益，然后当数据量达到一定规模的时候可以采用直接训练分类器的分类器，来对效果进行分类 [ ] Few-shot-learning中，训练集和测试集，标签已知和未知到底怎么弄，从代码中以及定义中分析的话怎么感觉是两个意思。我们需要的应该不能用到那个。 [ ] few-shot-learning应该指的是，新类只有很少的样本，但是旧类还是有大量标注样本的情况这个我们要好好分析。1.5.2. 但是这样的创新点在于更多的是算法的组合，有没有办法提出一个网络结构将这样的思路融合起来。！！！ © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-09 17:10:28 "},"Papers/SSL-MoCov3.html":{"url":"Papers/SSL-MoCov3.html","title":"2.1 MoCo V3","keywords":"","body":"1. MoCo V3 An Empirical Study of Training Self-Supervised Visual Transformers1.1. Motivation1. MoCo V3 An Empirical Study of Training Self-Supervised Visual Transformers @Aiken 2021 恺明大神对自监督学习+transformer的实证研究，针对Transformer再自监督学习学习框架中的训练不稳定问题提出了Random Patch Projection的解决方案。 Article；Paper； 1.1. Motivation ViT的方法在自监督学习的任务中，精度下降的主要原因是由于算法的不稳定性，容易陷入局部的最优值，本文主要聚焦于采用视觉领域的自监督框架进行Transformer的训练，CNN的训练方法已经是一个比较明确约定俗称的方法，而Transformer的训练架构实际上还没有被完全的构建。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 16:56:04 "},"Papers/SS_OD_SoftTeacher.html":{"url":"Papers/SS_OD_SoftTeacher.html","title":"2.2 SoftTeacher","keywords":"","body":"1. E2E Semi-Supervised Object Detection with Soft Teacher1.1. Abstrast and Intro1.1.1. Multi-Stage1.1.2. End to End1.2. Related works1.3. Methodology1.3.1. Framework1.3.2. Soft Teacher1.3.3. Box Jittering1.3.4. Experiment1. E2E Semi-Supervised Object Detection with Soft Teacher @ Article: ICML from Microsoft & Huazhong Keda @ Code: Github @ Noteby: Aikenhong @ Time: 20210914 1.1. Abstrast and Intro in the session we will using describe the main idea of this article. 这篇文章的重点在于Soft Teacher，也就是用pseudo label做为弱标注，逐步提高伪标签的可靠性。 不同于多阶段的方法，端到端的方法再训练中逐步的提升伪标签的质量从而再去benifit目标检测的质量。 这样E2E的框架主要依赖于两部分技术: soft teacher: 每个未标记边界框的分类损失由教师网络产生的分类分数进行加权 box jitter 窗口抖动: 选择可靠的伪框来学习框回归 在目标检测上获得SOTA的效果; 1.1.1. Multi-Stage 在半监督的情况下，关注的主要是基于伪标签的方法，是目前的SOTA，以往的方法采用多阶段的方式。 使用标记数据训练初始检测器 未标记数据的伪标记，同时基于伪标签进行重新训练 局限：初始少量标注的局限，初始的检测器的伪标签质量 1.1.2. End to End Soft Teacher基本思路：对未标记的图像进行标记，然后通过标记的几个伪标签训练检测器. 具体而言： 采样标注和未标注图片形成Batch 双模型：检测（student）、标记（teacher） EMA：T模型是S模型的EMA 这种方式避免了多阶段方案实现上的复杂，同时实现了飞轮效应==S、T相互加强; 此外Soft Teacher直接对学生模型生成的所有候选框进行评估，而不是使用伪框来为这些候选框进行分类回归。 这样能使用更多的直接监督信息 具体而言： 使用高阈值来分割前景，确保不会错误的将背景分类成前景，确保正伪标签的高精度； 使用可靠性度量来加权背景候选的损失； 教师模型产生的检测分数可以很好的作为可靠性度量 Box Jitter为了更可靠的训练学生网络的本地分支，指的是： 我们对前景框候选进行多次抖动 根据教师模型的位置分支对这些候选进行回归 将回归框的方差作为可靠性度量 可靠性高的用来训练 1.2. Related works Semi-Supervised Learning in Image Classification & object detection consistency based pesudo-label based new idea：使用弱数据增强生成伪标签和强增强来学习检测模型，区分两部分工作 Object Detection Based on Faster R-CNN to compare with other method 1.3. Methodology 可以从下面的图中看出基础的实现逻辑： 1.3.1. Framework 训练（Loss）是基于Batch进行，对于标记数据和未标记数据的损失处理时分开的， 对于未标记数据，我们需要通过教师模型来得到一个softlabel，包括分类和回归两个任务，然后得到最终的损失值。 L = L_s + \\alpha L_u 两者都要通过各自的图像数量进行归一化，以标注数据为例 L_s = \\frac{1}{N_l}\\sum_{i=1}^{N_l}(L_{cls}(I_l^i)+(L_{reg}(I_l^i)) 如何启动教师模型： 随机初始化学生模型和教师模型，后续通过学生模型的EMA来进行教师模型的更新。 目标检测的伪标签定义： 教师模型检测后NMS消除冗余，然后使用阈值来抑制非前景的候选； 获取高质量的伪标签： 对教师模型的伪标记使用弱增强，学生模型训练使用强增强 1.3.2. Soft Teacher 检测器的性能取决于伪标签的质量，如果在前景分数上使用较高的阈值过滤掉大部分学生生成的低置信度候选框可以得到更好的结果，当阈值设置为0.9时性能最佳，但是召回率迅速下降。 一般方法：使用学生生成的候选框和教师生成的候选框的IoU来分配前景和背景，可能会损坏性能。 软教师：我们评估学生生成的候选框作为真实背景的可靠性，用于衡量背景分类损失； b^{fg}_i、b^{bg}_i分别是分配为前景的框和分配为背景的框，具有可靠权重的伪标记图像的分类损失定义为： \\mathcal{L}_{u}^{\\mathrm{cls}}=\\frac{1}{N_{b}^{\\mathrm{fg}}} \\sum_{i=1}^{N_{b}^{\\mathrm{fg}}} l_{\\mathrm{cls}}\\left(b_{i}^{\\mathrm{fg}}, \\mathcal{G}_{\\mathrm{cls}}\\right)+\\sum_{j=1}^{N_{b}^{\\mathrm{b}_{b}}} w_{j} l_{\\mathrm{cls}}\\left(b_{j}^{\\mathrm{bg}}, \\mathcal{G}_{\\mathrm{cls}}\\right) w_j = \\frac{\\gamma_j}{\\sum_{k=1}^{N_b^{bg}}\\gamma_k} \\mathcal{G}_{cls}表示用于分类（教师生成的）伪框集，l_{cls}()是框分类损失，r_j是第j个背景的可靠性分数； 我们通过教师模型产生的背景分数可以很好的代替可靠性： 使用教师模型（BG-T）通过检测头来获取样本的背景分数 还研究了：学生模型，学生模型和学生模型之间的差异 1.3.3. Box Jittering 图三b可以看到，候选框的定义准确率和前景分数不是一个正相关的关系，他不一定能提供准确的定位信息。 需要更好的候选框，在教师生成的候选框bi上做抖动采样，将抖动框输入教师模型获得调整后的框 \\hat{b_i} = refine(jitter(b_i)). 抖动$N{jittle}$次后得到${\\hat{b}{i,j}}$集合，然后将可靠性定义为box回归方差： \\overline{\\sigma}_i = \\frac{1}{4}\\sum_{k=1}^4\\hat{\\sigma}_k 其中： \\hat{\\sigma}_k = \\frac{\\sigma_k}{0.5(h(b_i)) + w(b_i)} 较小的框回归方差表示较高的本地可靠性，但是大量的计算也是不可忍受的，所以我们一般只计算前景分数大于0.5的框的可靠性 回归方差计算： \\mathcal{L}_{u}^{\\mathrm{rcg}}=\\frac{1}{N_{b}^{\\mathrm{fg}}} \\sum_{i=1}^{N_{b}^{\\mathrm{f}_{8}}} l_{\\mathrm{reg}}\\left(b_{i}^{\\mathrm{fg}}, \\mathcal{G}_{\\mathrm{reg}}\\right) \\mathcal{L}_{u}=\\frac{1}{N_{u}} \\sum_{i=1}^{N_{u}}\\left(\\mathcal{L}_{u}^{\\mathrm{cls}}\\left(I_{u}^{i}, \\mathcal{G}_{\\mathrm{cls}}^{i}\\right)+\\mathcal{L}_{u}^{\\mathrm{rcg}}\\left(I_{u}^{i}, \\mathcal{G}_{\\mathrm{rcg}}^{i}\\right)\\right) 1.3.4. Experiment 实验细节 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-10-09 10:34:26 "},"Papers/GANs.html":{"url":"Papers/GANs.html","title":"3. GANs ","keywords":"","body":"1. `fGAN：对GAN理论的深度理解1.1. 基本理论体系和推演1.1.1. Fenchel Conjugate共轭1.1.2. F-Div GAN推导1.2. JS Div不是最佳的Div1.2.1. LSGAN最小二乘1.2.2. Wasserstein-GAN1. `fGAN：对GAN理论的深度理解 @Aiken 2021 onenote部分的拓展编写，到时候拷过去，整合在一起。 fGAN: 不只是JS-Div散度，我们可以将所有的散度都应用到GANs的框架中。该部分的阅读是对GAN的基本理论最重要的文章之一。 1.1. 基本理论体系和推演 首先给出fGAN中提出的基本理论：可以将所有的Div放入GANs的框架中，来做那个核心的关键演化判别指标： \r D_{f}(P||Q) = \\int_xq(x)f(\\frac{p(x)}{q(x)}dx)\r 上述公式将衡量P和Q两个分布之间的差距，公式中的$f$可以是很多不同的版本，但是要求满足如下的两个条件： 是一个凸函数；$f(\\frac{(x1+x2)}{2})\\leq \\frac{[f(x1)+f(x2)]}{2}$，需要注意国内外的凹凸相反 $f(1)=0$。 而我们知道$q(x)$是概率密度分布函数，实际上可以看成凸函数性质的推广，所以我们可以证得： \r D_{f}(P||Q) = \\int_xq(x)f(\\frac{p(x)}{q(x)}dx) \\geq\r f(\\int q(x) \\frac{p(x)}{q(x)} dx) = f(1) = 0\r 显然当我们取得合适的f，KL（$f(x) = xlog(x)$）; ReverseKL($-log(x)$)；chi square ($f(x) = (x-1)^2$)； 1.1.1. Fenchel Conjugate共轭 补充Fenchel共轭的知识来对后续的fGAN推导进行补充，定理内容如下： 每个凸函数都有一个对应的共轭函数读作$f^*(x)$ \r f^*(x) = \\max \\limits_{x\\in dom(f)} xt - f(x)\r t是给定的，对于所有的变量t， $xt-f(x)$对应了无数条直线： 举个例子$f(x)=xlog(x)$时，我们可以将对应的$f^*(x)$画出来。 实际上就是对给定的t，求$g(x)$共轭方程的最大值的过程，求个导，然后就可解得$x->t$然后带回就能得到共轭方程。 介绍共轭方程主要是为了和$f(x)$进行转化 \r f^{*}(t)=\\sup _{x \\in \\operatorname{dom}(f)}\\{x t-f(x)\\} \\quad \\Leftrightarrow \\quad f(x)=\\max _{t \\in \\operatorname{dom}\\left(f^{*}\\right)}\\left\\{x t-f^{*}(t)\\right\\}\r 1.1.2. F-Div GAN推导 将转化方程带入，利用简单的不等式转化，我们就能将之前的F-Div转换为一个类似GAN的式子： \r \\begin{aligned}\r D_{f}(P \\| Q) &=\\int_{x} q(x) f\\left(\\frac{p(x)}{q(x)}\\right) d x \\\\\r &=\\int_{x} q(x)\\left(\\max _{t \\in \\operatorname{dom}\\left(f^{*}\\right)}\\left\\{\\frac{p(x)}{q(x)} t-f^{*}(t)\\right\\}\\right) d x \\\\\r & \\geqslant \\int_{x} q(x)\\left(\\frac{p(x)}{q(x)} D(x)-f^{*}(D(x))\\right) d x \\\\\r &=\\int_{x} p(x) D(x) d x-\\int_{x} q(x) f^{*}(D(x)) d x \\\\\r & \\approx \\max _{D} \\int_{x} p(x) D(x) d x-\\int_{x} q(x) f^{*}(D(x)) d x\r \\end{aligned}\r 解释一下：第三行就是由于t是随便取值的；最后一行就是我们要求一个D使得式子最大，上界实际上就是第二行的式子。 这样我们就能推导出F-Div的变体： \r \\begin{aligned}\r D_{f}(P \\| Q) & \\approx \\max _{D} \\int_{x} p(x) D(x) d x-\\int_{x} q(x) f^{*}(D(x)) d x \\\\\r &=\\max _{D}\\left\\{E_{x \\sim P}[D(x)]-E_{x \\sim Q}\\left[f^{*}(D(x))\\right]\\right\\}\r \\end{aligned}\r 对于生成器来说，我们就是要找到一个PG使得： \r \\begin{aligned}\r G^{*} &=\\arg \\min _{G} D_{f}\\left(P_{\\text {data }} \\| P_{G}\\right) \\\\\r &=\\arg \\min _{G} \\max _{D}\\left\\{E_{x \\sim P_{\\text {data }}}[D(x)]-E_{x \\sim P_{G}}\\left[f^{*}(D(x))\\right]\\right\\} \\\\\r &=\\arg \\min _{G} \\max _{D} V(G, D)\r \\end{aligned}\r 这样我们的推导过程就结束了，然后我们也可以使用更多的Div Function，使用不同的Div距离直接选择对应的函数就可以了。 1.2. JS Div不是最佳的Div 由于分布的数据之间是没有重合的，使用JS Div的时候就很难衡量出他的距离，Equally Bad 为什么如果两个分布完全没有重合的话，那么这两个分布的 JS Div 会是一样的? 前面有提到，JS Div 是通过判别器计算出来的，而判别器的本质是二分类器，只要$PG$与$P{data}$完全没有重合，判别器就能 100%地鉴别出$PG(x)$与$P{data}(x)$ 的差异，因此二者的 JS Div 就是一样的。 1.2.1. LSGAN最小二乘 解决的就是没有重合的问题，解决思路如下：让判别器始终都不能100%的鉴别出差异，这样就能保证在没有重合的时候也能分辨出差异程度。 当我们的D太好的时候（能将数据完全分开）这种时候生成器就优化不了了，也是Equal Bad带来的最大问题。那么如果我们将最终的激活从sigmoid换成linear激活层，这样训练出来的D就会是一个线性的直线， 这样只有当完全重合的时候D才会是一个没有梯度的直线，但是这个也并没有真正的解决这个问题，而只是绕开了这个问题。 真正解决了这个核心问题的是下面的WGAN 1.2.2. Wasserstein-GAN 核心思想：用Wasserstrin距离（EM距离）取代JS距离 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-12 15:47:28 "},"Papers/StyleGAN.html":{"url":"Papers/StyleGAN.html","title":"3.1 StyleGAN","keywords":"","body":"1. StyleGAN V11.1. Related Work：1.2. Structure：1.2.1. Part1：AdaIN1.2.2. Part2：Noise Generator1.2.3. Part3 解纠缠（Mapping Function）2. V2 Analyzing and Improving the Image Quality of StyleGAN1. StyleGAN V1 @AikenHong 2020 10.8 《A Style-Based Generator Architecture for Generative Adversarial Networks》 1.1. Related Work： 继承的文献工作： ProGAN 参考解读： 《其中子链接值得一看》（包括源码解析啥的）（甚至还有GAN的笔记） 《StyleGan源码解析和拓展应用》 《秃头生成器1》《秃头生成器2》 NO.3 Contribution（Problem）： 解纠缠：Mapping Network Noise Generator AdaIN before all conv 1.2. Structure： 1.2.1. Part1：AdaIN 复习一下IN（inception normalization） AdaIN(x_i,y) = y_{s,i}\\frac{x_i-\\mu(x_i)}{\\sigma(x_i)}+y_{b,i} 1.2.2. Part2：Noise Generator 通过独立分辨率的高斯误差生成器，独立生成误差，然后控制例如毛发，胡须，雀版等等的随机生成。 1.2.3. Part3 解纠缠（Mapping Function） 解纠缠定义：由线性空间组成的潜在空间，使得每个线性子空间控制一个变化因子。 如果使用原始输入的话，潜在空间各种因子的采样概率需要与训练数据集中的分布匹配，各factor就还是纠缠在一起，不能使得较好的独立性存在。 解决方法： 通过Mapping Network，将input Z -> W，在 W这个latent layer中提取出来的factor，就不需要遵循既有的分布，实现了解纠缠，使得各个变量能够独立的对特征进行控制。 附加问题： 如何衡量解纠缠效果，空间因子的分离程度：“线性空间中的插值可能对图像产生非线性变化”基于latent space中的变化和图像发生的变化来衡量分离的效果，潜伏空间中较小的变化也应该导致的是图像上较小（平滑）的变化。 度量标准： 基于感知的图像对距离： （通过2个加权VGG16 Embedding 之间的距离），假如我们再潜在空间的分割时线性的，也就是每一段都是线性路径，就能使得线性变化成立，理论上无限细分是可行的，实际上使用Σ = 1e-4,所以Z中平均的感知距离为：Lz，同理求得W中的Lw 线性可分离性： \"如果充分的解纠缠，则应该找到始终和各个变化因素相对应的方向向量\"。该度量标准是通过量化线性超平面，能够将隐含空间的点分成两个独立的集合的程度。这样每个集合能够对应图像的特定二值属性。（男女） 在判别过程中使用和判别器又相同架构的分类器：保留属性，生成图像，进行分类，去除低置信度，得到带标签的钱再空间向量，svm预测标签，进行分类，计算条件熵 球面插值是归一化输入隐层空间中进行插值的最佳方法。 没有归一化的情况下就使用线性插值就好了 2. V2 Analyzing and Improving the Image Quality of StyleGAN Relted Work：Based on styleGAN. 中文译文 Contribution（Problem）： 重新设计了生成器归一化方法，改善了图像的质量： 同时路径长度调节器使得生成器的过程可逆，从而实现了网络可视化和方便对网络结构进行分析。 解决生成图像中水滴伪影的问题，将StyleGAN中的感知路径作为新的正则化器。 Part1：修改AdaIN的具体执行，消除伪像** 架构变化主要是：B->C->D © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-30 16:16:44 "},"Papers/RL-C1.html":{"url":"Papers/RL-C1.html","title":"4. Reinforcement Learning","keywords":"","body":"1. 强化学习2. Chapter1 模型基础2.1. Theory理论基础3. Chapter2 马尔可夫决策过程（MDP）3.1. 马尔可夫性简化3.2. MDP的价值函数和贝尔曼方程3.3. 状态价值函数和动作价值函数的递推关系3.4. 最优价值函数4. Chapter3 动态规划（DP）求解4.1. 策略评估求解预测问题4.2. 策略迭代求解控制问题4.3. 价值迭代求解控制问题4.4. 异步动态规划算法5. Chapter 4 用蒙特卡罗法求解5.1. Monto-Calo 求解5.2. MC求解预测问题（策略评估）5.3. MC求解控制问题（策略迭代）6. Chapter5 用时序差分法（TD）求解6.1. TD预测问题求解6.2. n步时序差分6.3. TD（$\\lambda$）6.4. TD控制问题求解1. 强化学习 Created by: Aiken H Detail: survey Finished?: No Tags: Paper URL1: https://www.cnblogs.com/pinard/category/1254674.html URL2: https://github.com/ljpzzz/machinelearning URL3: https://datawhalechina.github.io/easy-rl/#/ 2. Chapter1 模型基础 强化学习（一）模型基础 强化学习是介于监督和无监督学习之间的，强化学习没有输出值，但是有reward： 同时这个reward是事后给出的，而不是及时回馈的。而无监督学习是只有数据特征，同时数据之间是独立的，没有前后依赖的关系。 2.1. Theory理论基础 简化模型介绍： 上面的大脑代表我们的算法执行个体，我们可以操作个体来做决策，即选择一个合适的动作（Action）At。下面的地球代表我们要研究的环境,它有自己的状态模型，我们选择了动作At后，环境的状态(State)会变，我们会发现环境状态已经变为St+1,同时我们得到了我们采取动作At的延时奖励(Reward)Rt+1。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值。。。这就是强化学习的思路。 强化学习的模型关键要素： 环境的状态S：t时刻环境的状态$S_t$是它环境状态集中的某一个状态 个体的动作A：个体在某个时刻可能做出的动作集合 环境的奖励R：个体在某个时刻对应状态下做出的动作$A_t$得到的奖励会在t+1时刻得到 个体的策略$\\pi$：个体根据当前的环境选择采取动作的策略分布（函数），一般表示为一个条件概率分布的形式，概率大的动作被个体选择的概率显然更高 \\pi(a|s)= P(A_t = a | S_t = s) 在策略$\\pi$和状态s采行动后的价值$v_\\pi(s)$：一般是一个期望函数，因为我们不能每次只能选择当前的reward最大的策略，而是需要考虑大局，所以我们要有一个综合的（当前和后续）的延时奖励。 v_\\pi(s) = \\mathbb{E}(R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... |S_t = s) 奖励衰减因子$\\gamma$：也就是上式的权重，极端值考虑贪婪和一致等同，范围在[0,1] 环境的状态转移模型：也就是环境从s经过a后转化下一个状态的状态机，也可以表示为一个概率模型$P_{ss^‘}^a$ (s→s' , a) 探索率$\\epsilon$：主要用于训练迭代中，我们一般选择当前价值最大的动作，但是为了搜索空间的完备，我们会用$\\epsilon$的概率去选择非最大价值的动作，来提升训练的鲁棒性 SUMMARY：主要介绍了强化学习模型的workflow以及其中需要考虑的8个主要参数和函数架构。最主要的机制还是Policy和reward设计这一块 3. Chapter2 马尔可夫决策过程（MDP） 强化学习（二）马尔科夫决策过程(MDP) Easy-RL 在这里可能需要补充一下马尔可夫链的相关理论知识，先粗略的看完这部分再说 3.1. 马尔可夫性简化 环境的真实转化状态可能和之前的多个时刻相关，这样会导致建模困难，于是我们对环境的状态转移模型进行马尔可夫性假设。也就是： 转化到下一个状态s'只和当前的状态s相关，与之前的状态无关 同样的我们对Policy、价值函数也做了同样的马尔可夫性假设来简化。 其中：$G_t$代表收获（return），是从某一个状态开始采样直到终止状态时所有奖励的有衰减的和。 1.\\ P_{ss'}^a = \\mathbb{E}(S_{t+1} = s'|S_t=s,A_t=a) 2. \\ \\pi(a|s) = P(A_t = a | S_t = s) 3. \\ v_\\pi(s) =\\mathbb{E}_\\pi(G_t|S_t =s) = \\mathbb{E}(R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... |S_t = s) SUMMARY：由于环境的复杂时序关系，我们需要进行相应的马尔可夫性的假设，让下一个时刻的状态或者预测值只和当前时刻有关，从而简化并假设出模型 3.2. MDP的价值函数和贝尔曼方程 在上述价值表达式的基础上，加入考虑动作a带来的价值影响，我们就可以得到下面的动作价值函数： q_{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left(G_{t} \\mid S_{t}=s, A_{t}=a\\right)=\\mathbb{E}_{\\pi}\\left(R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid S_{t}=s, A_{t}=a\\right) 我们可以通过价值函数的公式得到价值函数的递推关系（贝尔曼方程）： \\begin{aligned} v_{\\pi}(s) &=\\mathbb{E}{\\pi}\\left(R{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid S_{t}=s\\right) \\\\ &=\\mathbb{E}{\\pi}\\left(R{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\ldots\\right) \\mid S_{t}=s\\right) \\\\ &=\\mathbb{E}{\\pi}\\left(R{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right) \\\\ &=\\mathbb{E}{\\pi}\\left(R{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right) \\end{aligned} 一个状态的价值由该状态的奖励以及后续状态价值按照一定衰减比例联合而成，同样的有： q_{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left(R_{t+1}+\\gamma q_{\\pi}\\left(S_{t+1}, A_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right) SUMMARY：基于马尔可夫假设之后，我们可以将价值函数（动作、状态）表示一个递推的形式，这个递推的形式也被叫做贝尔曼方程。 3.3. 状态价值函数和动作价值函数的递推关系 基于状态价值函数的定义以及动作价值函数的定义，我们很容易得到两个价值函数之间的转化关系： 状态价值函数是动作价值函数对于所有可能动作对于policy的期望。 利用贝尔曼方程，我们也能反推得状态价值函数来表示动作价值函数： 当前的reward和可能转移到所有后续状态的价值函数的加权和 v_\\pi(s) = \\sum_{a\\in A} \\pi({a|s}) q_\\pi(s,a) q_\\pi(s,a) = R_s^a + \\gamma \\sum _ {s'\\in S} P_{ss'}^a v_\\pi(s') 将上述两个式子互相结合起来，我们可以得到如下的简化（变量）算式（只包含一种价值函数） \\begin{gathered}v_{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R_{s}^{a}+\\gamma \\sum_{J \\in S} P_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right)\\right) \\\\q_{\\pi}(s, a)=R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P_{s s^{\\prime}}^{a} \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\end{gathered} 3.4. 最优价值函数 这一部分看原文，结合相应的例子一起看，后续可能需要看EasyRL中的markov的相关解读来进行深入的理解和计算的分析。 解决一个强化学习的问题意味着要找一个最有的policy（策略），让Argent在和环境交互的过程中获得比其他所有策略都更多的收获，找到这个策略，也就意味着我们解决了这样一个强化学习的问题。 求解最优策略→ 求解最优的价值函数，使得（动作、状态）价值函数获取到最大值的策略就是最优策略。 对于最优策略我们将动作函数定义为： \\pi_{*}(a \\mid s)=\\left\\{\\begin{array}{ll}1 & \\text { if } a=\\arg \\max _{a \\in A} q_{*}(s, a) \\\\0 & \\text { else }\\end{array}\\right. 有： v_*(s) = \\max_a q_*(s,a)\\\\q_{*}(s, a)=R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P_{s s}^{a} v_{*}\\left(s^{\\prime}\\right) 这样我们就可以最终得到： \\begin{gathered}v_{*}(s)=\\max _{a}\\left(R_{s}^{a}+\\gamma \\sum_{g^{\\prime} \\in S} P_{s s^{\\prime}}^{a} v_{*}\\left(s^{\\prime}\\right)\\right) \\\\q_{*}(s, a)=R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P_{s s^{\\prime}}^{a} \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\end{gathered} 4. Chapter3 动态规划（DP）求解 强化学习（三）用动态规划（DP）求解 用动态规划来求解强化学习是自然的 关键的两点： 问题的最优解可以由递归的最优解来得到 子问题状态间的转移 从上面推出的贝尔曼方程，这个递推公式实际上就是DP求解的状态转移等式，然后相应的Value什么的也和DP求解过程的需求是一一对应的。 关键的方程，通过这种递推公式，我们可以通过上一个迭代周期的状态价值去计算当前迭代周期状态S的状态价值，这也就是动态规划的一个求解的自然过程。 基于贝克曼方程推导出来，推导过程已经在上面了 v_{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R_{s}^{a}+\\gamma \\sum_{J \\in S} P_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right)\\right) 已知条件：状态集S, 动作集A, 模型状态转化概率矩阵P, 即时奖励R，衰减因子γ, 给定策略π 4.1. 策略评估求解预测问题 策略评估：求解给定策略的状态价值函数的问题，即强化学习的预测问题。 求解思路： 从任何一个状态价值函数开始，按照给定的策略，结合关键的贝尔曼递推期望方程，状态转移，reward，更新状态价值函数，直至最终收敛。 具体而言： 假设第k轮我们已经计算出了所有的状态的状态价值，然后再k+1轮的时候利用k轮的值通过贝尔曼方程来进行更新。 v_{k+1}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R_{s}^{a}+\\gamma \\sum_{s' \\in S} P_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right)\\right) 具体案例上面的网站中去看：（很容易理解） 4.2. 策略迭代求解控制问题 控制问题：需要同时求解状态价值函数和策略 策略迭代：从一个初始任意的策略状态，不断地迭代，调整我们的策略，从而得到一个最优的策略。 求解思路：贪婪法 具体而言： 个体在某个状态下选择的行为，是其能够达到后续所有可能的状态中，状态价值最大的那个状态， 策略迭代过程的演示：逐步的迭代策略和相应的价值函数，最终使得两者同时收敛 4.3. 价值迭代求解控制问题 和上述的策略迭代的问题一样，如果我们使用贪婪的策略去及时调整策略，而不是等到收敛了才调整策略的话，就能很快的减少迭代次数，这样我们状态价值的更新方法也会不太一样，也能更快的收敛 v_{k+1}(s)=\\max_{a \\in A} \\left(R_{s}^{a}+\\gamma \\sum_{s' \\in S} P_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right)\\right) 4.4. 异步动态规划算法 在前几节我们讲的都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中，我们使用这些保存起来的状态价值来计算新一轮的状态价值。 另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。 常见的异步动态规划算法有三种： 第一种是原位动态规划 (in-place dynamic programming)， 此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。 第二种是优先级动态规划 (prioritised sweeping)：该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。 第三种是实时动态规划 (real-time dynamic programming)：实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。 SUMMARY 动态规划是我们讲到的第一个系统求解强化学习预测和控制问题的方法。它的算法思路比较简单，主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。 动态规划的缺点：实际上是一种遍历的方式 动态规划算法使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。 5. Chapter 4 用蒙特卡罗法求解 强化学习（四）用蒙特卡罗法（MC）求解 ❓ 由 1. DP方法的全回溯机制（完全遍历）带来的过度的计算复杂度，对于复杂问题的求解困难 2. 很多时候对于状态转化模型P的未知 DP中问题预测和控制问题的定义是在P已知的情况下定义的，这种称之为：基于模型的强化学习问题 而一般性预测和控制，也就是在状态转化概率矩阵P未知的情况下求解1. 状态价值函数 和2. 1+最优策略的问题 我们需要考虑其他的方法，而不能使用DP方法来求解这样的RL问题——Monto-Calo是一种可行的方法 已知条件：状态集S, 动作集A, 即时奖励R，衰减因子γ，探索率ε 5.1. Monto-Calo 求解 基于采样的思路：蒙特卡罗法通过采样若干经历完整的状态序列(episode)来估计状态的真实价值。 经历完整就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。 有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。 关键公式回顾： v_\\pi(s) = \\mathbb{E}(R_{t+1} + \\gamma R_{t+2} + \\gamma ^2 R_{t+3} + ... |S_t = s) 5.2. MC求解预测问题（策略评估） 思路：求解某个s的状态价值：对所有采样到的状态序列中，出现该状态之后的收获再取平均值来近似求解。 G_t = R_{t+1} + \\gamma R_{t+1} + ...+ \\gamma ^{T-t+1}R_T \\\\ V_\\pi (s) \\approx average(G_t), s.t. S_t = s 一个状态在一个状态序列中多次出现的处理 主要有两种解决方式： First Visit： 只统计第一次出现的来进行均值的计算 Every Visit：每一次出现都加入均值的计算，这种方式更适合样本量少的情况，但是计算量要更大一些。 累进更新平均值（Incremental mean） 如果我们将每个状态序列的值都记录下来在最后进行更新的话，会耗费大量的存储空间，所以我们使用累计更新均值的方法来进行不同轮次之间的迭代。 换言之：统计当前的均值和状态遍历到的次数。 \\mu_k = \\frac{1}{k} \\sum_{j=1}^{k}x_j = \\frac{1}{k}(x_k+\\sum_{j=1}^{k-1}x_j) = \\frac{1}{k}(x_k+(k-1)\\mu_{k-1}) = \\mu_{k-1} + \\frac{1}{k}(x_k-\\mu_{k-1}) 然后我们就可以将状态价值公式的更新过程修改成： N(S_t) = N(S_t)+1\\\\ V(S_t) = V(S_t) + \\frac{1}{N(S_t)}(G_t-V(S_t)) 这种情况下的存储空间（内存消耗）就是固定的了。 对海量数据做分布式迭代的时候$N(S_t)$计算不确定的情况 V(S_t) = V(S_t) + \\alpha(G_t-V(S_t)) 动作价值函数也是类似的方法。 5.3. MC求解控制问题（策略迭代） 和策略迭代的方式也是类似的，也是先做策略评估，然后通过一定的方法（比如贪婪策略）更新策略。 和DP相比的不同有如下几点： 策略评估的方法不同 MC优化最优动作价值函数而不是状态价值函数 DP一般使用贪婪法，MC使用$\\epsilon$-贪婪法 $\\epsilon$-贪婪法： 一般设置一个较小的值，然后用1-$\\epsilon$来选择最大行为价值的行为，然后剩下的就随机在m个可行行为中随机选择 \\pi(a \\mid s)=\\left\\{\\begin{array}{ll}\\epsilon / m+1-\\epsilon & \\text { if } a^{*}=\\arg \\max _{a \\in A} Q(s, a) \\\\\\epsilon / m & \\text { else }\\end{array}\\right. 为了使得算法收敛；$\\epsilon$会逐渐减小，并趋于0。 这样会得到一个和动态规划类似的图 具体的算法流程： 在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。在线和离线的区别我们在后续的文章里面会讲。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。 输入：状态集S, 动作集A, 即时奖励R，衰减因子γ, 探索率ϵ　 输出：最优的动作价值函数q∗和最优策略π∗　 初始化所有的动作价值Q(s,a)=0， 状态次数N(s,a)=0，采样次数k=0，随机初始化一个策略π　 k=k+1, 基于策略π进行第k次蒙特卡罗采样，得到一个完整的状态序列:S1,A1,R2,S2,A2,...St,At,Rt+1,...RT,ST 对于该状态序列里出现的每一状态行为对(St,At)，计算其收获Gt, 更新其计数N(s,a)和行为价值函数Q(s,a)： G_t = R_{t+1} + \\gamma R_{t+1} + ...+ \\gamma ^{T-t+1}R_T \\\\N(S_t,A_t) = N(S_t,A_t)+1\\\\ Q(S_t,A_t) = Q(S_t,A_t) + \\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t)) 基于新计算出的动作价值，更新当前的ϵ−贪婪策略： \\epsilon = \\frac{1}{k}\\\\\\pi(a \\mid s)=\\left\\{\\begin{array}{ll}\\epsilon / m+1-\\epsilon & \\text { if } a^{*}=\\arg \\max _{a \\in A} Q(s, a) \\\\\\epsilon / m & \\text { else }\\end{array}\\right. 如果所有的Q(s,a)收敛，则对应的所有Q(s,a)即为最优的动作价值函数q∗。对应的策略π(a|s)即为最优策略π∗。否则转到第二步。 SUMMARY:实际上MC方法就是一个简单的采样渐进求平均的方法，在不断的迭代过程中找到相应的槿近似值。 6. Chapter5 用时序差分法（TD）求解 强化学习（五）用时序差分法（TD）求解 蒙特卡洛法虽然灵活，不需要环境转化概率模型，但是也有限制：所有的采样序列都需要是完整的状态序列，如果没有完整的状态序列，就不能使用Monto-Calo了。 在不完整的状态序列的情况下，可以使用时序差分算法（Temporal-Difference，TD），这也是一种不基于模型的算法（也就是没有环境转移的情况下） 关键公式回顾： 蒙特卡洛：G_t = R_{t+1} + \\gamma R_{t+1} + ...+ \\gamma ^{T-t+1}R_T \\\\ 贝尔曼（TD）：v_{\\pi}(s) = =\\mathbb{E}{\\pi}\\left(R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right) 由于如果使用G的公式的话，我们需要有T时刻的R来进行计算分析， 为了简化这个过程，我们使用贝尔曼的递推式来进行时序差分的分析（实际上是同个等式） 也就是： 使用$R{t+1} + \\gamma v(S{t+1})$（也称为TD目标值） 来代替收获$Gt$，同时令$R{t+1} + \\gamma v(S_{t+1}) - V(S_t)$称为TD误差，用TD目标值来代替收获G的过程称为引导。这样的话我们只需要两个连续的状态和对应的奖励，就可以尝试求解强化学习的问题了。 6.1. TD预测问题求解 预测问题的求解思路大体上是类似的，但是和MC有两个主要的不同点: 一个是$G_t$收获的表达式不同 G(t) = R_{t+1} + \\gamma v(S_{t+1}) 二是迭代的系数稍微有些不同，因为没有完整的序列，所以就没有对应的次数N，所以就用一个[0,1]的系数来代替 V\\left(S_{t}\\right)=V\\left(S_{t}\\right)+\\alpha\\left(G_{t}-V\\left(S_{t}\\right)\\right) 具体的例子请参考相应的链接，这里写的特别的清楚！GO TO URL 从例子中我们可以看到MC和TD主要的几点区别： 时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。‘ 时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。 虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。 所以后续的主流的强化学习方法都是基于时序差分的，后面的文章也会主要基于时序差分来拓展讨论。 SUMMARY: 实际上TD和对应的DP最大的区别就在于G(t)的计算，从这里可以体现出DP主要依靠的是当前值再所有出现的序列中的状态值的平均，而TD可以依靠其他变量进行递推的这点优势。 6.2. n步时序差分 前面我们的递推式只考虑了一步差分来进行近似，但是实际上我们可以将差分式子变形，变成二次差分项 G_t^{(2)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+1}) 也可以一次类推到n步的差分项，当n趋于无穷的时候，实际上就等价于MC方法了。 6.3. TD（$\\lambda$） n步时序差分选择多少步数是一个超参数调优的过程，为了再不增加计算复杂度的时候综合考虑所有步数的预测，引入一个新的[0,1]的参数λ，定义λ-收获是n从1到∞所有步的收获*权重的和，每一步的权重带有一定的比例，如下： G_t^\\lambda = (1-\\lambda)\\sum_{n=1}^\\infin \\lambda^{n-1}G_t^{(n)} 因此我们就能得到TD（λ）的迭代公式：Q也是类似的，就不重新写一次了 V(S_t) = V(S_t)+\\alpha(G_t^\\lambda - V(S_t)) 权重衰减的原因如下，随着n增大，权重成集合级数衰减，在T时刻把所有剩余的权重给最终状态，这样可以使得权重嘉禾为1，里当前越远权重越小。 从前向来看TD(λ)， 一个状态的价值V(St)由Gt得到，而Gt又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)有着和蒙特卡罗法一样的劣势。当λ=0 时,就是第二节讲到的普通的时序差分法，当λ=1 时,就是蒙特卡罗法。 从反向来看TD(λ)，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为频率启发(frequency heuristic) 式；而把电击归因于最近少数几次状态的影响，则称为就近启发(recency heuristic) 式。 如果给每一个状态引入一个数值：效用(eligibility, E) 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发。而所有状态的效用值总称为效用迹(eligibility traces,ES)。定义为： \\begin{gathered}E_{0}(s)=0 \\\\E_{t}(s)=\\gamma \\lambda E_{t-1}(s)+1\\left(S_{t}=s\\right)=\\left\\{\\begin{array}{ll}0 & t 可以看到一个状态要是重复出现的话都会让效用迹增加，不然的话就会一直衰减。 这样最终TD（λ）的股票公式就可以更新为：（反向公式这应该是） \\begin{gathered}\\delta_{t}=R_{t+1}+\\gamma v\\left(S_{t+1}\\right)-V\\left(S_{t}\\right) \\\\V\\left(S_{t}\\right)=V\\left(S_{t}\\right)+\\alpha \\delta_{t} E_{t}(s)\\end{gathered} 然后可以看出这两个公式是存在一致性的。 6.4. TD控制问题求解 实际上还是使用同样的ε-贪婪进行策略和价值迭代。 在线控制最常见的是SARSA算法 离线控制比在线控制多了一个策略，用贪婪发来更新价值函数，用一样的来进行动作选择，最常见的是Q-Learning算法。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:36 "},"Papers/RL-Reward_is_enough.html":{"url":"Papers/RL-Reward_is_enough.html","title":"4.1 Reward Is Enough","keywords":"","body":"1. Reward is enough1.1. 对reward构建AGI的可行性的分析和探讨1. Reward is enough Desc: RL Finished?: Yes Tags: Paper 通用人工智能，是否能通过强化学习的奖励机制就实现 实现AGI，强化学习就够了？Sutton、Silver师徒联手：奖励机制足够实现各种目标 1.1. 对reward构建AGI的可行性的分析和探讨 这篇文章实际上没有给出一个很好的方案通过reward来实现各种AGI的设计，但是给出了在每一种场景下的AGI的reward设计的设想把。和对用reward进行设计的可行性分析。 同时分析了：感知、社交、语言、泛化、模仿，这几个方面 类似地，如果人工智能体的经验流足够丰富，那么单一目标（例如电池寿命或生存）可能隐含地需要实现同样广泛的子目标的能力，因此奖励最大化应该足以产生一种通用人工智能。 这不久回到了最基础的问题，没有这种长线以及大量数据交互以及全面场景的经验流，来支撑这样一个AGI的学习，所以这不也是在现阶段上纸上谈兵嘛？ 对这篇论文我的总结是，我不推荐详细阅读，我觉得收益有限，太理想化，其实和强化学习本身的假设也没有太多新东西，我们可以假设强化学习能带来一个AGI，但是对应的约束和限制确实是有点多了。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:36 "},"Papers/RL-DouZero.html":{"url":"Papers/RL-DouZero.html","title":"4.2 DouZero","keywords":"","body":"1. DouZero：Mastering DouDizhu with Self-Play Deep Reinforcement Learning1.1. 算法的设计和思路1.2. 蒙特卡洛方法存在的问题1.3. 蒙特卡洛方法在该任务上存在的优势1.4. Reference1. DouZero：Mastering DouDizhu with Self-Play Deep Reinforcement Learning Desc: GAME, RL Finished?: Yes Tags: Paper URL1: https://arxiv.org/abs/2106.06135 URL2: https://github.com/kwai/DouZero URL3: https://github.com/datamllab/rlcard-showdown） 使用蒙特卡洛方法进行自我对弈不断更新预测模型的方法，这实际上也是普通人对于强化学习如何在self-play中实现自我更新的最基础的想法把： 自我对弈（记录动作序列）- 用最终的胜负（价值）更新网络。 1.1. 算法的设计和思路 算法的目标是学习一个价值网路。网络的输入是当前状态和一个动作，输出是在当前状态做这个动作的期望收益（比如胜率）。简单来说，价值网络在每一步计算出哪种牌型赢的概率最大，然后选择最有可能赢的牌型。蒙特卡罗方法不断重复以下步骤来优化价值网络： 用价值网络生成一场对局 记录下该对局中所有的状态、动作和最后的收益（胜率） 将每一对状态和动作作为网络输入，收益作为网络输出，用梯度下降对价值网络进行一次更新 其实，所谓的蒙特卡罗方法就是一种随机模拟，即通过不断的重复实验来估计真实价值。、 如下图所示，斗零采用一个价值神经网络，其输入是状态和动作，输出是价值。首先，过去的出牌用 LSTM 神经网络进行编码。然后 LSTM 的输出以及其他的表征被送入了 6 层全连接网络，最后输出价值。 系统训练的主要瓶颈在于模拟数据的生成，因为每一步出牌都要对神经网络做一次前向传播。斗零采用多演员（actor）的架构，在单个 GPU 服务器上，用了 45 个演员同时产生数据，最终数据被汇集到一个中央训练器进行训练。比较有趣的是，斗零并不需要太多的计算资源，仅仅需要一个普通的四卡 GPU 服务器就能达到不错的效果。这可以让大多数实验室轻松基于作者的代码做更多的尝试。 该方法的设计和实现上听起来都挺简单的，可以找个时间自己测试一下，玩一玩这个东西，对于我来说，看看他们怎么用这个lstm去进行历史编码的，以及在对transformer了解后，看看如何用transformer去替代这样的lstm是我这边的研究重点。 1.2. 蒙特卡洛方法存在的问题 蒙特卡罗方法在强化学习领域中被大多数研究者忽视。学界普遍认为蒙特卡罗方法存在两个缺点： 蒙特卡罗方法不能处理不完整的状态序列 蒙特卡罗方法有很大的方差，导致采样效率很低。 但是斗地主中，可以产生转正的状态序列，同时很容易通过并行来采集大量的样本降低方差，主要是实现上简单，但是可能也是需要大量的数据把。 1.3. 蒙特卡洛方法在该任务上存在的优势 很容易对动作进行编码。斗地主的动作与动作之前是有内在联系的。以三带一为例：如果智能体打出 KKK 带 3，并因为带牌带得好得到了奖励，那么其他的牌型的价值，例如 JJJ 带 3，也能得到一定的提高。这是由于神经网络对相似的输入会预测出相似的输出。动作编码对处理斗地主庞大而复杂的动作空间非常有帮助。智能体即使没有见过某个动作，也能通过其他动作对价值作出估计。 不受过度估计（over-estimation）的影响。最常用的基于价值的强化学习方法是 DQN。但众所周知，DQN 会受过度估计的影响，即 DQN 会倾向于将价值估计得偏高，并且这个问题在动作空间很大时会尤为明显。不同于 DQN，蒙特卡罗方法直接估计价值，因此不受过度估计影响。这一点在斗地主庞大的动作空间中非常适用。 蒙特卡罗方法在稀疏奖励的情况下可能具备一定优势。在斗地主中，奖励是稀疏的，玩家需要打完整场游戏才能知道输赢。DQN 的方法通过下一个状态的价值估计当前状态的价值。这意味着奖励需要一点一点地从最后一个状态向前传播，这可能导致 DQN 更慢收敛。与之相反，蒙特卡罗方法直接预测最后一个状态的奖励，不受稀疏奖励的影响。 1.4. Reference 快手开源斗地主AI，入选ICML，能否干得过「冠军」柯洁？ DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning GitHub - kwai/DouZero: [ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning | 斗地主AI © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:35 "},"Papers/RL-MobaAI_Tencent.html":{"url":"Papers/RL-MobaAI_Tencent.html","title":"4.3 MobaAI Tencent","keywords":"","body":"1. 腾讯绝悟1.1. Introduction and Related Research.1.1.1. Neural Network Architecture Include1.1.2. Contributions1.2. Framework Design1.3. System Design1.4. Module Detail1.4.1. AI-Server1.4.2. Dispatch Module1.4.3. Memory Pool1.4.4. RL Learner1.5. Algorithm Design1.5.1. 网络架构1.5.2. Dual-clip PPO1. 腾讯绝悟 Created by: Aiken H Desc: GAME, RL Finished?: Yes Tags: Paper 《Master Complex Control in MOBA Games with Deep Reinforcement Learning》 论文阅读笔记 @Aiken H 2021.06 1.1. Introduction and Related Research. MOBA游戏的复杂度和状态空间都远比以前的围棋之类的运动更大，所以难度会更大一些 早一些的游戏ai使用的是（2015） Deep Q-Network 通过 supervised learning and self-play 结合的训练策略在围棋上击败了专业人类，而最近更多的使用了DRL（Deep Reinforcement Learning）的方法在近几年被进一步的应用。 1.1.1. Neural Network Architecture Include 1.1.2. Contributions the encoding of Multi-modal inputs 多模态输入 the decoupling of inter-correlations in controls 控制内关联解码 exploration pruning mechanism 剪枝设置 Action mask for efficient exploration ❓效率 attack attention(for target selection) Attention机制做目标选择 LSTM for learning skill combos LSTM 机制做技能释放和链接 Optimize by multi-label proximal policy algorithm(improved PPO) dual-clip PPO 帮助训练的收敛 present a systematic and thorough study develop a deep reinforcement learning framework which provides scalable and off-policy training we develop an actor-critic neural network 跳转上面的网络架构 1.2. Framework Design (S.O.A.P,$\\gamma$,$\\tau$,$\\rho_0$) to denote infinite-horizon ： 使用元组去模拟整个动态强化的过程,过程主要的是最大化累计reward S 状态空间 O 观察状态空间 A 动作空间 $\\rho_0$ 初始状态分布 $\\gamma$ 折扣系数 目标MAX：$\\mathbb{E}[\\sum_{t = 0}^{T} \\gamma^t \\tau(s_t,\\alpha_t)]$ $\\tau: S \\times A \\rightarrow \\mathbb{R}$ 奖励函数 $\\pi： O \\times A \\rightarrow [0,1]$ 策略 $P:S\\times A \\rightarrow S$ 状态转移分布 SUMMARY: This Part is about the basic rule of the RL setting. 1.3. System Design The whole system and workflow design will be shown on this part 由于MOBA游戏复杂的Agent（Players和Characters） 会带来高方差的随机梯度，所以再这种模型的训练中，我们可能会需要一个大的Batach Size来防止Invariant Shift的这种现象，同时并加速训练的有效和收敛性。于是我们设计了一个规模可变，弱耦合的网络架构。 模型整体由四个部分组成：RL-Learner、AI-Server、Dispatch-Module（调度）、Memory-Pool（记忆池） AI-Server:：与环境进行交互模拟（self-play） RL Learning：核心学习网络 Memory Pool：数据存储，为RL提供训练和搜索的实例 Dispatch：数据的收集，压缩，传输 模块之间相互解耦，可以灵活配置， 1.4. Module Detail 1.4.1. AI-Server 传统策略：提供了游戏环境和AI模型之间的交互逻辑，通过相互镜像的策略来进行self-play，从而生成episodes. 对手策略：基于游戏状态中提取的特征使用玻尔兹曼搜索，预测英雄行文（基于softmax的分布采样，发送给CPU执行），返回reward和下一个state CPU版本的FeatherCNN，转换到LOCAL上进行inference 1.4.2. Dispatch Module 和多个AI-Service绑定，是一个收集数据样本的服务器，主要包括：奖励，特征，动作概率等 将这些数据压缩和打包，然后发到内存池中 1.4.3. Memory Pool 服务器：内部实现为用于数据存储的内存高效循环队列 支持不同长度的样本，以及基于生成时间的数据采样 1.4.4. RL Learner 分布式训练环境，为了使用Big Batch，使用多个RL Learner并行的从Memory Pool 获取数据，然后通过ring allreduce算法来集成梯度 通过共享内存（而不是socket）和pool来进行数据交换，从而减少IO需求 P2P的在策略更新和AI service进行快速同步 SUMMARY: 经验生成和参数学习是分离的，以及Memory和Dispath的架构，能够使得算法能够很容易的拓展到百万歌CPU内核和数千个GPU。 1.5. Algorithm Design An Actor-Critic Network 用来建模游戏中的动作依赖关系 1.5.1. 网络架构 由下图说明了网络的状态和动作，为了有效的训练这个网络，提出了一些新颖的策略：目标注意力机制（选择目标）；LSTM用来学习技能combo，和动作选择；控制依赖解耦来建立一个PPO；（先验引入）基于游戏知识的剪枝（Action mask）；优化PPO成dual-clipped PPO 保证大批次和大偏差的收敛性 对图像、Unit、GameInfo分别提取特征后整合成固定长度的Feature，通过LSTM（考虑时间维度的表征）得到最终的表征，然后通过FC对特征进行分类解码（也可以说是预测把），同时基于状态编码的注意力机制来整合出对象预测， 目标注意力：p(t|a) = Softmax(FC(h_{LSTM}·h_{key}^T) p(t|a)是units上的注意力分布，维度是状态中的单元数。 为了解决多标签策略网络中，同一个动作不同标签之间的关联显示建模困难的问题，独立处理一个动作中的每个标签解耦他们的相互关联。 原始的PPO objective：E:有限批次的经验平均值，其余的参见上面的对照表 \\max_{\\theta} \\hat{\\mathbb{E}}_{s\\sim \\pi_{\\theta_{old}}}[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}]\\hat{A_t} 参数解耦之后可以看到： \\max_{\\theta}\\sum_{i=0}^{N_a-1} \\hat{\\mathbb{E}}_{s\\sim \\pi_{\\theta_{old}}}[\\frac{\\pi_{\\theta}(a_t^i|s_t)}{\\pi_{\\theta_{old}}(a_t^i|s_t)}]\\hat{A_t} 有两个优点：简化策略的结构（不考虑相关性）；增加策略的多样性，为了多样性，我们开始的训练过程中，随机了初始化agent的位置。 缺点：进一步增加的策略训练的复杂度，所以通过action mask来进行简化，在最终输出层来合并动作元素之间的相关性，从而修建需要的策略搜索空间， 1.5.2. Dual-clip PPO 令$\\taut(\\theta) = [\\frac{\\pi{\\theta}(at|s_t)}{\\pi{\\theta_{old}}(a_t|s_t)}]$,由于这个值可能会很大，导致过大的策略偏差，为了缓解这个问题，我们引入 L^{CLIP}(\\theta)\\hat{\\mathbb{E}}_t[\\min(\\tau_t(\\theta)\\hat{A_t},clip(\\tau_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A_t})] 来惩罚政策的极端变化，但是另一种情况下的极端值也会带来无界偏差，所以还有另一端的优化,其中c>1是一个下线常数 \\hat{\\mathbb{E}_t}[\\max(\\min(\\tau_t(\\theta)\\hat{A_t},clip(\\tau_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A_t}),c\\hat{A_t})] © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-26 00:08:35 "},"Papers/IL-MGSvF.html":{"url":"Papers/IL-MGSvF.html","title":"5.1 MgSvF","keywords":"","body":"1. MgSvF：Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning1.1. Intro1.2. Related Work1.3. Main1.4. FrameWork1. MgSvF：Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning @Author & Paper：Arxiv @Note：Aikenhong 2021/11/12 Other’s Note 1 1.1. Intro 旧知识的缓慢忘记和新知识的快速适应的困境：主要探讨Incremental中的Old和New的相互牵制和适应的问题， 旧知识的缓慢遗忘会导致对新任务的欠拟合，而快速适应会导致灾难性的遗忘，如何对这两种策略之间进行权衡，是一个重要的问题。 多尺度混合的解决这个问题： Intra-space： 新类别的特征在同一个特征空间中 inter-saoce：新旧类别的特征在不同的特征空间中 本文提出的多粒度策略： 提出了一种频率感知的正则化操作，加速空间内的增量学习能力 新的特征空间组合操作，提升空间间的学习性能 实际上新类和旧类的特征最好是通过自监督或者无监督的特征学习方法归化到同一个特征空间中，在这种情况下对Classifier进行调整可能是一种更好的策略。通过混合特征空间来得到一个泛化能力更高的特征表示器。 传统的策略：无论是累加还是进行数据混合进行梯度计算，这种方式应该是将类别之间的梯度进行直接的叠加。 是否可以自行混合不同类别之间的学习梯度？通过对梯度的下降方程求解来得到一个旧类和新类之间的更好的下降方法。 具体的操作上就是对step进行处理，通过mixdataset对梯度进行分开计算 在混合策略上可以考虑梯度的下降方向，对不同的维度进行加权计算？ 上述的策略难以实施的点在于框架中的梯度是自动计算的，我们可以对损失进行加权，但是很难重新计算不同节点之间的梯度值 退而求其次的方法就是对新旧类的损失进行加权处理, 或者直接的混合数据 如果我们能获取梯度的方向, 或许我们能在每次迭代的过程中获得一个更好的加权值 首先可以尝试对梯度进行获取,Grad 我们在蒸馏的过程中通过MLP对不同的类别进行聚类划分, 这种方式的聚类和传统机器学习聚类的优劣又如何对比解释呢. 能不能用PCA方法或者multi-head策略来对特征进行处理, 这种类似因果的方式来分析特征中的冗余维度 上述的分析基于Mix Guide make error 的想法, 实际上还有一个问题就是Feature’s capabliity 不足的问题 New Key Word： Few-Shot class-incremental Learning 有大规模训练样本的第一个任务和具有有限样本的新类学习两阶段任务的这种场景 1.2. Related Work 框架策略: 复习策略 正则化策略 1.3. Main 该文认为统一的特征空间是相互关联的，很难相互解开进行svf分析，同时新知识和旧知识的学习方向通常而言不一致，甚至有时是相互矛盾的，所以他认为需要一个全新的特征空间。 但是在Few-Shot的情境下，新的特征空间的泛化能力可能很差，本身带来的准确率就很有问题把 使用离散余弦变化，建立一个空间内的SVF特征分解方案，实现了一个像互不相关的正交频率空间，同时在不同频率上对新旧两种知识的重要性不同，低频分量对于保存旧知识的贡献更大，遗忘率则随着频率的增加而增加。 所以逼近旧特征空间的低频分量的正则项权重更高, 具体而言就是独立的特征空间的更新比其他空间更新更慢, 通过对特征空间的组合来组成上述空间的方法, 是十分灵活的, 即使是简单的串联也能带来巨大的性能提高 这确实是我没想到的, 也就是如果我们使特征并行化, 对最终准确率的提升增益是更大的, 这是为何. 1.4. FrameWork 实际上就是维护两个模型, 然后进行特征的串联, 进行分类. 每次只对新的数据进行训练, 不会使用旧的数据. © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 16:56:27 "},"Algorithms/":{"url":"Algorithms/","title":"6. Algorithms","keywords":"","body":"1. Algorithms1. Algorithms @Aikenhong 2021 整理一些常用或者著名的算法，在整理算法的时候，主要按照算法的原理和算法的实现来进行整理，如果行有余力的情况下可能会多维护一个语言版本，通常情况下以Python作为基础的实现语言。 在算法的实现层面，遵循奥卡姆剃刀的原则，尽量的使代码精简化：不过于执着从底层开始实现；如果有可用的Package，则整理相应的Package的用法以及对应的实例即可。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 12:53:59 "},"Algorithms/Astar.html":{"url":"Algorithms/Astar.html","title":"6.1 Astar","keywords":"","body":"1. A* 算法1.1. 基本思想1.1.1. 相关类别定义1.1.2. 具体代码实现1. A* 算法 Created by: Aiken H Detail: algorithm Tags: Code wikipedia；medium；pythonf A* 是一种在平面图形上计算最优路径的方法，通常用来做2D游戏的最短寻路，该算法是一种Dijkstra算法的变体，使用启发式的策略来提高算法的搜索效率。 1.1. 基本思想 基于启发式的方法，基于BFS去做最短路径搜索，借助类似Manhattan距离作为启发，每次加入后续的多个点，然后按照后续点的属性去排序，不断的加入close的区间，直到第一次遍历到终点就是最短的路径。 f(n) = g(n) + h(n) f代表的是经过当前点，从起点到最终点的距离，g代表的是从起点到当前节点的距离，h代表的是启发式方法到终点的距离。 维护三个list：open(候选列表)、close（状态确定的列表）、children（等待操作的列表） 首先用bfs的方法，找到当前节点的可达后续节点，将这些节点加入children，确定child不在close_list中后，若在open中则判断哪个是最优解，然后更新openlist，并将这些都加入open。 每次遍历的当前节点都从open中总距离最小的选，然后放入close。直到openlist为空。 1.1.1. 相关类别定义 class node(): def __init__(self, parent=None, position=None): self.parent = parent self.position = position self.g = 0 self.h = 0 self.f = 0 def __eq__(self, o: object) -> bool: return o.position == self.position 1.1.2. 具体代码实现 def asterS(map,slope,start,end): # 在astar算法的基础上，我们需要加入的是高度的约束 # 阻碍的条件是高度不同同时没有slope的存在，这种就是wall # 其余的和Astar算法应该是一样的 # init the start and end node start_node = node(None,start) end_node = node(None,end) # init the open and closed lists open_list = [] close_list = [] # add the start node to the open list open_list.append(start_node) # loop util find the end_node while len(open_list)>0: # make the best node as current_node # init 1 current_node = open_list[0] current_index = 0 for index, nod in enumerate(open_list): if nod.f= map.shape[0] or node_pos[1] = map.shape[1]: continue # mkae sure walkab mapflag = map[current_node.position[0], current_node.position[1]] != map[node_pos[0], node_pos[1]] slopeflag1 = slope[node_pos[0], node_pos[1]] == 0 or slope[current_node.position[0], current_node.position[1]] == 0 slpopeflag2 = slope[node_pos[0], node_pos[1]] != slope[current_node.position[0], current_node.position[1]] if mapflag and (slopeflag1 or slpopeflag2): continue # we need creat node first to find out if it is in the openlist or closed list new_node = node(current_node, node_pos) children.append(new_node) # loop those children # using the __eq__ to judge if it's already traveled. for child in children: if child in close_list: continue # create f,g,h for the legal child child.g = current_node.g + 1 child.h = manhattan(child.position, end_node.position) child.f = child.g + child.f # if the child is already in the open list, compare it if child in open_list: open_index = open_list.index(child) open_node = open_list[open_index] if child.g > open_node.g: continue open_list.pop(open_index) # if it is not in the open/closelist or better than that in open list, we add it. open_list.append(child) © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-30 12:02:01 "},"Algorithms/Hungarian.html":{"url":"Algorithms/Hungarian.html","title":"6.2 Hungarian","keywords":"","body":"1. Hungarian Algorithm1.1. Method1.1.1. 匈牙利算法1.1.2. KM1.2. Code1. Hungarian Algorithm @AikenHong 2021 @Code: Scipy（repo） @Reference: 匈牙利算法&KM算法 该篇笔记用来介绍匈牙利算法和KM算法(Kuhn-Munkres Algorithm)，这两个算法通常用来做目标之间的匹配问题。 常用于：多目标跟踪，和深度聚类中的标签匹配问题。 1.1. Method 这两种方法实际上解决的问题都是： 二分图的最大匹配问题； 首先需要对二分图有个基本的了解： 实际上就是将数据分为两组，各组的每一个点都去另一个组找对应的匹配，我们希望将两组中，相关的数据尽可能的准确的匹配起来。 可以想象成，是同一个数据在不同的映射下的不同表征需要做这样的匹配关系。 解决这种问题的方式就是使用匈牙利算法或者KM算法 1.1.1. 匈牙利算法 匈牙利算法是一种在多项式时间内求解任务分配问题的组合优化算法 匈牙利算法可以将二分图中的连线，看成是我们认为可能是相同的目标（不带权值），实际上就是从上到下假想成立，然后进行唯一匹配的搜索，有点像BackTrack的过程。 整体算法的成功率或者准确率实际上十分依赖与连线的准确率，对算法输出预测的准确度要求更高。 1.1.2. KM KM解决的是带权的二分图的最优匹配的问题。 相当于我们给每条线都给出一个置信度预测值，基于这样的权值图去计算对应的匹配关系 Step1: 将左边节点标上与他所关联的最大权值的边的数值 Step2: 寻找匹配，原则如下 只有权重和左边分数相同的边才进行匹配； 如果找不到边，此条路径的所有左边顶点-d，右侧+d，这里我们将d取值为0.1 对于考虑换边的另一个节点，如果无法换边，也需要对应的进行-d 具体的例子可以这么看（最好还是看blog）： 1.2. Code 使用scipy中的集成版本实现，但是要注意对应的输入是二分图的cost_matrix 算法的实现应该是将最大的权值转换成了最大代价来进行计算的，所以为了使用KM算法，我们首先应该构造对应的损失矩阵。 假如我们使用相似度指标计算的话，对应的大小关系应该做一个反转，可以直接用负号进行计算，计算完相似度直接取一个负值进行计算。 from scipy.optimize import linear_sum_assignment row_ind, ol_ind = linear_sum_assignment(cost_matrix, maximize) © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-02 19:27:39 "},"Papers/NIPS中国预讲会.html":{"url":"Papers/NIPS中国预讲会.html","title":"NIPS20","keywords":"","body":"1. NIPS 中国预讲会1.1. Long-Tailed Classicification by Keeping the Good and Removing the Bad Momentum Causal Effect1.2. 3D视觉部分的启发1.3. self-supervised and transfer1. NIPS 中国预讲会 1.1. Long-Tailed Classicification by Keeping the Good and Removing the Bad Momentum Causal Effect re-sampling 、re-weighted（这些方法都需要提前预知分布，就和人类的本能比较接近了）， 问题一般是对于头部的过拟合，对尾巴的欠拟合 two-stage re-balancing 基于优化器的动量调整 长尾数据分布本身带来的优化偏折项。 可以被广泛的应用于各种不同的数据集 1.2. 3D视觉部分的启发 光度法3维重建 1.3. self-supervised and transfer MOCO: 无监督的的预训练+fine-tune在很多地方超过有监督了。 （主要是潜力方面） ImageNet-1k linear evaluation：用特征直接接简单的线性来进行分类，来评估本身的特征提取效果。 SimCLR 简化版的MOCO，还有一些其他的发现，data-augmentation 这些trick用在MOCO上甚至效果更高了比SimCLR NIPS： BYOL：moco需要很大的负样本，这个方法不需要负样本，但是设计了一个不对称的设计， SwaV：Deep clustering + contrastive learning InfoMin：Pascal 上做 和对Augmentation的仔细研究 PIC：将2stage->1stage（不用做对比了） After NIPs： higher accuracy better understand 为什么不需要负样本，而不会坍缩 hekaiming ：孪生网络的工作。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-30 00:02:22 "},"Papers/YOLOv4.html":{"url":"Papers/YOLOv4.html","title":"YOLOv4","keywords":"","body":"1. Optimal Speed and Accuracy of Object Detection1.1. Abstract1.2. INTRODUCTION1.3. RELATED WORK1.4. Architecture1.5. Experiments1. Optimal Speed and Accuracy of Object Detection @AikenHong 20200726 基于YOLO v4 掌握一些CV方面训练的Trick，同时针对Typora的使用进行一个熟悉掌握。GITHUB CODE 一些相关的参考资料 ⚡️https://zhuanlan.zhihu.com/p/150127712 ⚡ 机器之心YOLOv4 ⚡️https://www.zhihu.com/question/390191723/answer/1177584901 本文中一些其他的收获 • 其他可替代的Object Detection的SOTA算法有哪些 • BoS，BoF方法 • 简直是一个Tricks的综述 1.1. Abstract 本文对近期再CNN上的一些Feature方法进行了尝试组合，并实现了新的SOTA，其实就是一些通用的**Trick**的组合尝试，包括 • 加权残差连接（WRC） • Cross-Stage-Partial-connection，CSP • Cross mini-Batch Normalization，CmBN • 自对抗训练（Self-adversarial-training，SAT） • Mish 激活（Mish-activation） • Mosaic 数据增强 • DropBlock 正则化 • CIoU 损失 基于该文章我们可以了解一下这些方法的主要思路和后续的应用价值。YOLOv4 更快，更准确，只需要比较小的计算需求即可 1.2. INTRODUCTION • 更快更强，从速度和准确率，以及训练需求上提升实际运用价值 ​ 这里有一些其他的SOTA可以列一下：EfficientDet、ATSS，ASFT，CenterMask • AP：平均准确率 FPS：每秒传输帧率嘛？ 主要贡献可总结如下 建立了一个高效强大的目标检测模型。它使得每个人都可以使用 1080Ti 或 2080Ti 的 GPU 来训练一个快速准确的目标检测器。 验证了当前最优 Bag-of-Freebies 和 Bag-of-Specials 目标检测方法在检测器训练过程中的影响。 Bag-of-freebies: 仅仅只增加training cost或者只改变training strategy的方法。典型例子：数据增强 bag-of-specials: 增加少量推理成本，但能提高准确率的插件模块（**plugin modules）和后处理方法（post-processing method）**被称为BoS。 修改了 SOTA 方法，使之更加高效，更适合单 GPU 训练。这些方法包括 CBN、PAN、SAM 等。 PAN: Path aggregation network for instance segmentation SAM: CBAM: Convolutional block attention module 1.3. RELATED WORK 基本架构 object detector 通常由backbone和head两部分构成，其中backbone是再imagenet上预训练的骨架 GPU: VGG [68], ResNet [26], ResNeXt [86],or DenseNet [30] CPU: SqueezeNet [31], MobileNet[28, 66, 27, 74], or ShuffleNet [97, 53] head则是用来预测物体类别和边界框的网络架构 One-Stage: YOLO [61, 62, 63], SSD [50],and RetinaNet [45] Anchor-free：CenterNet [13], CornerNet [37, 38], FCOS [78], etc. Two-Stage: R-CNN [19] series: fast R-CNN [18], faster R-CNN [64], R-FCN [9],and Libra R-CNN [58] anchor-free: Rep-Points[87] 近年来在backbone和head之间插入neck用以收集不同stage的feature-maps FPN、PAN、BiFPN、NAS-FPN、etc. To sum up, an ordinary object detector is composed of several parts: Bag-of-freebies: 仅仅只增加training cost或者只改变training strategy的方法。典型例子：数据增强 目标检测中的多种数据增强：包括对图像遮挡的处理，随机擦除和基本的数据增强，也有feature map中类似的操作 解决数据存在偏差的问题：例如数据不平衡 BoundingBox回归方法：MSE-》IoU，也就是一些边界回归上的损失函数，CIoU、GIoU、DIoU、MSE等 Bag of specials 增加少量推理成本，但能提高准确率的插件模块（plugin modules）和后处理方法（post-processing method）被称为BoS。 Plugin modules：例如扩大接受域，引入注意力机制，增强特征集成能力等等， post-processing method：筛选预测结果的方法 扩大接受域：SPP（将SPM集成到CNN中）、ASPP、RFB。 Attention module： Channel-Wise：SE Point-Wise：SAM feature integration： 将低层的物理特征集成到高层语义特征 skip connection、hyper-column FPN出现后：SFAM、ASFF、BiFPN activation function： 解决softmax和sigmoid中出现的梯度消失问题：ReLU、LReLU、PReLU、ReLU6、SELU、Swish、hard-Swish、Mish post-process： – NMS用于处理预测同一对象的一些BBox，并保留响应速度更快的BBox – 还有一些相关变体 – anchor-free的方法不需要这部分 1.4. Architecture 找到最优的input network resolution，conv layer number， the parameter number(filter size2 * filters * channel / groups) 以及 the number of layer outputs(filters)之间的最有平衡 挑选能够增加感受域的额外单元（additional block），以及最佳参数聚合方法 YoloV4 的基本目标是提高生产系统中神经网络的运行速度，同时为并行计算做出优化，而不是针对低计算量理论指标（BFLOP）进行优化。YoloV4 的作者提出了两种实时神经网络： （Backbone） • 对于 GPU，研究者在卷积层中使用少量组（1-8 组）：CSPResNeXt50 / CSPDarknet53； • 对于 VPU，研究者使用了分组卷积（grouped-convolution），但避免使用 Squeeze-and-excitement（SE）块。具体而言，它包括以下模型：EfficientNet-lite / MixNet / GhostNet / MobileNetV3。 分类器和检测器需求上的区别： 架构选择part1 CSPDarknet53 在cspdarknet52上添加了spp block，用PANet取代v3中的FPN，yolov3作为head 架构选择part2：selection of BoF or BoS CNN的优化通常有这几个方面： Activations: ReLU, leaky-ReLU, parametric-ReLU, ReLU6, SELU, Swish, or Mish Bounding box regression loss: MSE, IoU, GIoU,CIoU, DIoU Data augmentation: CutOut, MixUp, CutMix Regularization method: DropOut, DropPath [36], Spatial DropOut [79], or DropBlock Normalization of the network activations by their mean and variance: Batch Normalization (BN) [32], Cross-GPU Batch Normalization (CGBN or SyncBN) [93], Filter Response Normalization (FRN) [70], or Cross-Iteration Batch Normalization (CBN) [89] Skip-connections: Residual connections, Weighted residual connections, Multi-input weighted residual connections, or Cross stage partial connections (CSP) 架构选择Part3 ：额外的改进 使得架构能够更适合在单个GPU上进行运算，设计了一些改进 • 新的数据增强方法：mosaic &SAT（self-Adversarial Training） • 在遗传算法中使用了最佳的超参数 • 修改SAM，PAN和CmBN使得设计适合更有效的训练和检测 Mosaic（马赛克）数据增强，把四张图拼成一张图来训练，变相的等价于增大了mini-batch。这是从CutMix混合两张图的基础上改进； Mosaic数据增强 Self-Adversarial Training(自对抗训练)，这是在一张图上，让神经网络反向更新图像，对图像做改变扰动，然后在这个图像上训练。这个方法，是图像风格化的主要方法，让网络反向更新图像来风格化图像（对风格化感兴趣，可以看看我写的一篇介绍谷歌的一个实时任意风格化的文章）；对自身实行对抗攻击 跨最小批的归一化（Cross mini-batch Normal），在CBN的基础上改进； BN, CBN，CmBN的对比 修改的SAM，从SAM的逐空间的attention，到逐点的attention； [image: https://pic4.zhimg.com/50/v2-440bfacec2a426272ef06e94a16837bb_hd.jpg?source=1940ef5c] SAM和修改的SAM对比图 修改的PAN，把通道从相加（add）改变为concat，改变很小； [image: https://pic4.zhimg.com/50/v2-a1f0ccf10cea594c1aebcc98111c6dd5_hd.jpg?source=1940ef5c][image: https://pic4.zhimg.com/80/v2-a1f0ccf10cea594c1aebcc98111c6dd5_720w.jpg?source=1940ef5c] PAN和修改的PAN 最终整体架构表示： 1.5. Experiments 实验中的一些参数设置和具体的表达就从文章中看吧，还有各种trick的表达效果,其实很重要，可以省下很多的研究时间。 • Influence of different features on Classifier training • Influence of different features on Detector training • Influence of different backbones and pretrained weightings on Detector training • Influence of different minibatch size on Detector training FAQ • reception field 的理解，以及作用 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-30 16:58:32 "},"Papers/Causal_Analysis.html":{"url":"Papers/Causal_Analysis.html","title":"Causal_Analysis","keywords":"","body":"1. Causal Analysis1.1. 现阶段机器学习存在的问题1. Causal Analysis @Aiken 2021.04.07 这篇文档目前就是一个motivation，暂时不去看综述，分析一下为什么我们需要Causal Analysis，后续进行系统性的学习。 1.1. 现阶段机器学习存在的问题 参考文章链接 受限于iid基本假设：现阶段的机器学习，严重受限于训练数据的分布情况，（因为我们把复杂的问题转化为i.i.d的分布，但是在现实应用场景下实际上很多时候并不是这个分布。 而现实的应用场景往往不满足这种独立同分布的假设，那么：「要在i.i.d.环境之外对对象进行很好的概括，不仅需要学习变量之间的统计关联，还需要学习一个潜在的因果模型。」 「当学习一个因果模型时，我们应该需要更少的例子来适应大多数知识，比如创造一个模块，这样这个模型可以在不需要进一步训练的情况下重用。」 当在外界的干预下改变一个问题的统计分布的时候，因果的模型仍然是稳健的，但是普通机器学习就并不是，所以用因果来解决机器学习泛化性能的是一个比较好的思路 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 09:56:06 "},"Experiment/Hyper_Resolution.html":{"url":"Experiment/Hyper_Resolution.html","title":"Survey: Hyper Resolution","keywords":"","body":"1. 图像恢复技术在表情识别中的应用1.1. 超分辨率在表情识别中的应用1.2. 图像恢复技术、图像增强技术、人脸增强技术在表情识别中的应用1.3. 针对光，姿势变化，噪声和遮挡的人脸识别？AND 其他1. 图像恢复技术在表情识别中的应用 说明：重点针对超分辨率技术日期：2020-03-06 备注： 超分辨率在人脸识别上的多，但是表情识别上的确实不多，不过很多都会引用一波 1.1. 超分辨率在表情识别中的应用 KEY WORDs ： 1. (\"super resolution\" OR \"image restore\") AND (\"facial expression recognition\" OR \"emotion recognition\") 2. (\"super resolution\") AND (\"expression recognition\") 针对于低带宽传输的分辨率不足和比率低的应用场景 基于facial expression recognition 的 emotion recognition 在解码器进行视频下采样的时候，联合SR和识别 通过层次卷积神经网络(HCNN)来实现有校的SR 在facial expression recognition 中案例研究发现增强后的图像有助于提高识别性能 有点擦边吧，就是基于超分辨率算法的多分辨率图像，对面部进行识别从而判断疼痛程度 也可能妹啥用，你可以考虑一下 摘要中没有明确的提到Super-Resolution， 但是感觉低分辨率这个问题前缀，可能和SR有关系来着 好像是什么比赛，过程中有一部分是面部表情检测 在识别之前采取了超分辨率的查询增强 针对分辨率低和部分遮挡的面部表情识别 GAN IGCN RRMB 修复和超分辨率面部表情 1.2. 图像恢复技术、图像增强技术、人脸增强技术在表情识别中的应用 KEY WORDs： 1. (\"super resolution\" OR \"image restore\") AND (\"facial expression recognition\" OR \"emotion recognition\") 2. (\"image restore\") AND (\"expression recognition\") ——NONE 3. (\"Image enhancement\") AND (\"facial expression recognition\") 4. (\"face enhancement\") AND (\"facial expression recognition\") 5. (\"Image restoration\") AND (\"facial expression recognition\") 离散小波变换正则化和快速盲恢复模型来重建红外光谱。。。。来帮助面部表情识别 1.3. 针对光，姿势变化，噪声和遮挡的人脸识别？AND 其他 没有提到超分辨率或是图像重建 但是有提到标题那几个，结合局部和全局特征... 是对表情识别的增强但是好像不是图像增强..... 基于特征提取的增强把 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-29 23:56:41 "},"Experiment/并行训练的理解.html":{"url":"Experiment/并行训练的理解.html","title":"1.1 Parallel Compute Theory","keywords":"","body":"1. 大模型并行化训练1.1. 数据并行策略：1.2. 模型并行范式：1.3. Pipeline并行策略：1.4. Tensor并行范式1.5. Mixture of Experts1.6. Other Memory Saving Designs1. 大模型并行化训练 How to Train Really Large Models on Many GPUs? (lilianweng.github.io) 对于浮点运算，模型参数的存储和中间计算输出（梯度和优化器状态）的存储的在GPU内存上的大量需求使得我们需要并行化，下面我们参考一些常用的并行化范式： 1.1. 数据并行策略： 在Multi-GPUs上Duplicate模型，然后分别feed数据，进行运算，每个batch同步或者异步的进行多卡间的梯度传递和模型同步。 同步可能会导致每个batch需要停止训练，异步则是可能会使用陈旧的梯度进行一段时间的训练，增加了计算时间。 而在PT1.5以来，使用一种中间的方式：每隔x次迭代，进行多卡间的全局同步梯度一次，使用这种梯度积累的策略，根据计算图来进行计算和通信调度的优化，提高吞吐量。 1.2. 模型并行范式： 是为了解决单模型过大无法存储在单一的Node上的问题，但是这样会有GPU间的顺序依赖，虽然减少了内存的占用和计算量，但是这种IO的需求导致计算资源的利用率严重不足。 在这种pipeline中，就存在利用率的bubble，也就是空白的部分 1.3. Pipeline并行策略： 混合模型和数据并行的策略，来减少低效时间的泡沫，也就是，将一个batch切分成多个小batch，然后分发到每个node上，减少相应的等待时间，只要我们对计算量和运行速度有合理的把握，就能极大的降低这个inefficient time bubbles. 多个mini-batch的梯度聚集起来最后同步更新. 最有情况下甚至可以忽略气泡的开销 1- \\frac{2md}{(2m+2(d-1))d} = \\frac{d-1}{m+d-1} m个mini-batch和d个分布, bubble的比例将如上述所示 这种方式能实现吞吐量和设备数量的几乎线性加速, 但是如果参数模型不能均匀的分布的话, 她并不总是能保证. pipedream下面的这种方法, 虽然没有在最后进行全局的梯度同步, 可能会有之前提到的异步并行的问题, 使用不同的模型权重, 但作者也提出了一些相应的解决方案, 包括权重存储和垂直同步等等方法. 详细的需要的时候对原文进行参考. 后续还有两个对其的不同设置的改进*PipeDream-flush* | *PipeDream-2BW* 1.4. Tensor并行范式 前面都是纵向的对模型进行切割, Tensor并行是横向的对Tenosr以及相应的矩阵运算进行切割. 1.5. Mixture of Experts 结合弱监督学习器来得到一个强学习器, 采用一个专家门控机制来实现, 用一个门控网络G来学习n个专家的概率分布, 将网络的流量导向对应的专家 1.6. Other Memory Saving Designs CPU offloading: 异步的将一些GPU存不下的放到cpu上, 但以不影响Training Process为基准 Mixed Precision Training: 混合精度训练, 实际上应该就是APEX采用的策略. © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 12:54:33 "},"Experiment/NerualNetworkTraining.html":{"url":"Experiment/NerualNetworkTraining.html","title":"2. Training","keywords":"","body":"1. 训练策略调整1.1. 优化器1.2. 学习率1.2.1. 学习率的基本设置1.2.2. 学习率变化方法1.2.3. 分析学习率的大小1.3. 过拟合欠拟合现象1.3.1. 过拟合问题定义和分析1.3.2. 收敛过快泛化能力差1.3.3. 产生的原因分析1.3.4. 数据的复杂度分析:1.3.5. 常见的解决方式1.3.6. 图像增强1.4. 早停法1.5. 效率优化1.5.1. rapidAI1.5.2. torch.Cuda.AMP1.5.3. APEX_显存优化1.6. 限制网络的输出范围1. 训练策略调整 @Aiken 2020， 主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。 参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。 1.1. 优化器 Some Material 实际上虽然有很多的优化算法，但是到最后最常用的还是 SGD+Mon 和 Adam两种： Adam主要的有事在于自适应学习率，他对我们设计的学习率实际上没有那么敏感，但是在具体实验中往往不会有调的好的SGD那么好，只是在SGD的参数调整中会比较费劲。 但是有了根据patient调整lr的scheduler后，我们基本上可以使用SGD做一个较为简单的调整，只要设计好初始的lr的实验以及用来调整学习率的参数值。 1.2. 学习率 $\\omega^{n} \\leftarrow \\omega^{n}-\\eta \\frac{\\partial L}{\\partial \\omega^{n}}$ 其中的权重就是学习率lr， ==Basic== 学习率大 学习率小 学习速度 快 慢 使用情景 刚开始训练时 一定的次数过后 副作用 1. Loss爆炸 2.振荡 1.过拟合 2.收敛速度慢 1.2.1. 学习率的基本设置 在训练过程中，一般根据训练轮数设置动态变化的学习率。 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。 一定轮数过后：逐渐减缓。 接近训练结束：学习速率的衰减应该在100倍以上。 Note： 如果是 迁移学习 ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4) 在新数据上进行 微调 。 1.2.2. 学习率变化方法 ==warm up== warm up为什么有用 warm up衰减策略与上述的策略有些不同，它是先从一个极低的学习率开始增加，增加到某一个值后再逐渐减少, 这点上倒是和Cosine Anneal LR有一定的相似之处，将这两种结合起来是一种常见的训练策略： 这样训练模型更加稳定，因为在刚开始时模型的参数都是随机初始化的，此时如果学习率应该取小一点，这样就不会使模型一下子跑偏。 而这样的跑偏对于大模型而言，可能是导致很严重的影响，后面收敛了也可能不会达到最佳的效果，一开始的跑偏，可能会造成准确率在后面的严重结果。 # MultiStepLR without warm up scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \\ milestones=args.milestones, gamma=0.1) # warm_up_with_multistep_lr warm_up_with_multistep_lr = lambda epoch: epoch / args.warm_up_epochs if \\ epoch ==Scheduler Setting：== 分组的学习率也能通过scheduler进行学习率的更新，可以放心使用。 轮数减缓 指数减缓 分数减缓 step decay exponential decay 1/t1/t decay 每N轮学习率减半 学习率按训练轮数增长指数插值递减 lrt=lr0/(1+kt)，k 控制减缓幅度，t 为训练轮数 Pytorch的Scheduler pytorch中提供了很多scheduler的方法，其中用的最多的可能还是multistep，考虑到后续可能会用到基于指标调整的学习率，这里特别提一个cosine的学习率调整策略，它的学习率呈现的是一种周期变化的样子。 ==Custom Scheduler== Pytorch为可能的自定义提供了一个方便的Scheduler接口，ReduceLROnPlateau，通过step 调用指标的变化，进行学习率的调整，极其方便。 scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=False, threshold=1e-4, threshold_model='rel', cooldown=0, min_lr=1e-8) scheduler.step(acc) 基本的参数包括： mode 很好理解，max（acc），min（loss）值 factor 学习率下降的参数 patience 多少次没有变化就调整 cooldown 调整后多久的冷却期 threshold，threshold_model 调整我们的动态上下限 threshold (float) – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4. threshold_mode (str) – One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in ‘max’ mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: ‘rel’. 1.2.3. 分析学习率的大小 在训练过程中可视化Loss下降曲线是相当重要的，那么针对Loss出现异常的情况我们应该怎么样去调整使得Loss逐步趋于正常呢？ 曲线 初始时 上扬 [红线]：（直接起飞梯度爆炸） 初始 学习率过大 导致 振荡，应减小学习率，并从头开始训练 。 曲线 初始时 强势下降 没多久 归于水平 [紫线]： Solution：后期学习率过大导致无法拟合，应减小学习率，并重新训练后几轮 。 曲线 全程缓慢 [黄线]： Solution：初始 学习率过小 导致收敛慢，应增大学习率，并从头开始训练 。 1.3. 过拟合欠拟合现象 过拟合->各种泛化能力差的现象在这里我个人对这个现象的定义为以下的几种： 训练阶段的准确率和验证/测试阶段的准确率相差大 训练过程和验证过程中的损失下降不一致，验证集中的准确率没有随着训练提升 典型的过拟合导致这样的现象 下面整理一下李沐对该部分的讲解 bug部分可能是由于增强做的过高或者问题太难, 但是在正常的表现下也不应该出现这种问题, 误差应该是差不多的. 上面的这张图片也说明了, 我们模型和问题的难度是需要相互匹配的, 如果不匹配就会出现各种各样的问题, 模型的复杂度, 通常可以从可学习参数的数来进行简单的判断的. 1.3.1. 过拟合问题定义和分析 定义：模型对于训练集的假设过度严格，导致对训练集的数据拟合的“很好”，但是在测试验证集中效果不理想。可能会出现的典型现象如下： 验证损失先下降后上升 训练集和测试集稳定后的准确率相差很大 下面这张图, 显示的是模型的复杂度和相应的泛化和训练误差之间的关系, 在训练的时候复杂度还是需要自我调整. 1.3.2. 收敛过快泛化能力差 过拟合的一种衍生问题，当模型在训练集中快速收敛，在这种情况下可能会陷入极小值，由于损失太小，模型参数难以跳出极小值点，这种情况下，如果不加以约束会影响泛化能力，可以考虑使用， flood 方法来设计我们的loss（效果未知，作为一种策略把，保证模型能够有一定量的损失，同时希望验证集上的损失能够下降到一个平缓的地方，来保证泛化能力） 1.3.3. 产生的原因分析 训练数据样本单一，数据量不足 噪声干扰过大：失去了真实的输入输出之间的关系 模型的复杂度太高，足够死记硬背所有训练集的数据，导致不知道变通 1.3.4. 数据的复杂度分析: 大部分情况下进行数据的对比还是一个比较直观的情况, 其实可以从这几个方面进行比较 数据集的样本数, 类别 数据集的分辨率 数据的时空结构和多样性 1.3.5. 常见的解决方式 :zap:添加正则化L1，L2（weight decay）， weight decay等权重下降的方法，需要熟练掌握在pytorch上的设置 :zap:降低模型的复杂度，对应模型的设计和问题的规模需要更好的分析。 :zap:数据增强，使得数据的多样化指标进一步上升 :zap:Dropout，Early Stop BatchNormalization 集成学习方法，通过对多个模型进行集成来降低单一模型的过拟合风险 1.3.6. 图像增强 这里我们为图像增强另外开一个文档，图像增强的内容实际上可以考虑《数字图像处理》的这样一门课。 自监督学习和对比学习 (qq.com) 文中提到对准确率提升最多的一些增强方式是如下的三种： Crop，Resize ，Flip Colour Distortion Gaussian Blur from torchvision import transforms # Size used in SimCLR size = 224 crop_resize_flip = transforms.Compose([transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(3/4, 4/3)), transforms.RandomHorizontalFlip(p=0.5)]) # Higher means stronger s = 1.0 # 0.8*s and 0.2*s are from the paper colour_jitter = transforms.ColorJitter(brightness=0.8*s, contrast=0.8*s, saturation=0.8*s, hue=0.2*s) colour_jitter = transforms.RandomApply([colour_jitter], p=0.8) colour_distortion = transforms.Compose([colour_jitter, transforms.RandomGrayscale(p=0.2)]) kernel_size = int(0.1*size) # The size of the kernel must be odd kernel_size = kernel_size if kernel_size%2 == 1 else kernel_size+1 gaussian_blur = transforms.GaussianBlur(kernel_size, sigma=(0.1, 2.0)) gaussian_blur = transforms.RandomApply([gaussian_blur], p=0.5) augment = transforms.Compose([crop_resize_flip, colour_distortion, gaussian_blur]) 1.4. 早停法 MicroSoft Ai 教程 ES 因为准确率都不再提高了，损失值反而上升了，再继续训练也是无益的，只会浪费训练的时间。那么该做法的一个重点便是怎样才认为验证集不再提高了呢？并不是说准确率一降下来便认为不再提高了，因为可能在这个Epoch上，准确率降低了，但是随后的Epoch准确率又升高了，所以不能根据一两次的连续降低就判断不再提高。 对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。 更好的一个方式应该是使用一个类来进行计数 class TrainingTrace(): def __init__(self, need_earlystop=False, patience=10, mode='max'): self.early_stop = need_earlystop self.patience = patience self.patience_count = 0 self.last_vid_metrric = float('inf') if model =='min' else float('-inf') self.compare = new_min if model == 'min' else new_max def step(self, value): 在得到早停的迭代次数和权重矩阵参数后，后续有几种方法可以选择。 彻底停止 就是啥也不做了，最多再重复几次早停的试验，看看是不是稳定，然后就使用做为训练结果。 再次训练 由于第一次早停是通过验证集计算loss值来实现的，所以这次不再分训练集和验证集，记住了早停时的迭代次数，可以重新初始化权重矩阵参数，使用所有数据再次训练，然后到达第一次的时停止。 但是由于样本多了，更新批次也会变多，所以可以比较两种策略： 1) 总迭代次数epoch保持不变 2) 总更新梯度的次数保持不变 优点：使用更多的样本可以达到更好的泛化能力。 缺点：需要重新花时间训练。 继续训练 得到后，用全部训练数据（不再分训练集和验证集），在此基础上继续训练若干轮，并且继续用以前的验证集来监控损失函数值，如果能得到比以前更低的损失值，将会是比较理想的情况。 优点：可以避免重新训练的成本。 缺点：有可能不能达到目的，损失值降不到理想位置，从而不能终止训练。 1.5. 效率优化 and there are some tips in this article, we should read and learn about it 这一部分希望通过trick或者对应的一些代码技巧，优化训练过程中带来的资源占用和损耗，进一步提升训练时效性和资源上的有效利用 # making relu inplace will save memory def inplace_relu(m): classname = m.__class__.__name__ if classname.find('ReLU') != -1: m.inplace=True # we need to learn this function model.apply(inplace_relu) relu(inplace = True) 1.5.1. rapidAI Thanks to Nvidia, we could using np, spicy, pandas, sklearn on CUDA, which is much more faster. Achieve this by those repo: cuml for sklearn, cupy for numpy and spicy, cudf for dataframe and so on. 借助这几个仓库的文档, 我们可以学习如何调用这些库去加速和实现我们的代码. 在这里要注意的是, 使用这几个仓库的同时会引入更多的数据类型, 以及设备存储情况, 我们要在必要的时候对数据的存储位置进行分析和迁移. 过于频繁的数据移动可能反而会减慢运行速度, 但是如果是后续不需要的数据我们可以进行迁移. Install 如果版本和torch的匹配(old version) 10.2 可以通过以下的命令安装cuml, 但是要注意panda版本 == 1.3.0, 首先对panda版本进行修改, 这种时候可能使用pip结合conda是一个更好的选择 如果版本不匹配, 我们可以首先配置rapidai的环境, 在安装pytorch即可, 或者使用nvidia发布的相同cuda版本的pytorch. 1.5.2. torch.Cuda.AMP 使用Torch自带的AMP取代APEX的AMP进行优化，在>=1.6的情况下，Torch已经自动支持了AMP混合, 而且事实证明在大多数情况下, Torch对amp的支持相比APEX来说要更加稳定和性能友好。 使用方法： 较为简单，只需要在训练的主流程中进行如下的嵌入 from torch.cuda.amp import autocast, GradScaler # 在训练最开始的阶段实例化一个GradScaler对象 scaler = GradScaler() for i in epochs: for j in iterators: ... # model and loss with autocast(): out = model(input) loss = loss_fn(output, target) # and change the update and backward phas # 放大loss scaler.scale(loss).backward() # 对inf和nan进行判断，没有问题的话就进行step scaler.step(optimizer) # 是否对scaler进行更新 scaler.update() 1.5.3. APEX_显存优化 this session is write for the nvidia module APEX which can save a lot of memory and accelerate the training speed. we should learn how to use it . 通过APEX好像能优化接近50%的显存，而且在修改原框架代码中的要求很小，所以在这里有必要通过APEX去优化我们的框架 理论参考：基于Apex的混合精度加速； 其中opt_level分别表示：O0纯FP32，O1混合精度训练，O2几乎FP16除了BN，O3纯FP16很不稳定，但是速度最快 安装： 验证cuda版本，验证torch的cuda版本 nvcc -V # nvcc 很可能会找不到命令，去如下路径搜索是否cuda正确安装 cd /usr/local/cuda*/bin # 其中若有nvcc命令的话可以直接执行 nvcc -V import torch print(torch.version.cuda) 安装apex git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./ import验证安装成功 import apex 使用： 参考官方示例，我们可以知道APEX的使用场景主要集中在几个部分： model,optimizer,loss upgrade and parallel 故而我们对原始代码修改或添加如下： from apex import amp from apex.parallel import DistributedDataParallel model = resnet() optimizer = torch.optim.SGD(model.parameters(),lr=1e-3) # MODEL PART: after model and optimizer design model, optimizer = amp.initialize(model, optimizer, opt_level = \"O1\") # DISTRIBUTION PART: # replace nn.parallel.DistributedDataParallel() model = DistributedDataParallel(model) # LOSS PART: # replace the loss BP process # loss.backward() with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() optimizer.step() 此外，如果我们希望使用APEX在训练过程中执行resume的话，我们还需要对代码做如下的添加 Note that we recommend restoring the model using the same opt_level. Also note that we recommend calling the load_state_dict methods after amp.initialize. # Save checkpoint checkpoint = { 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'amp': amp.state_dict() } torch.save(checkpoint, 'amp_checkpoint.pt') ... # Restore model = ... optimizer = ... checkpoint = torch.load('amp_checkpoint.pt') model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level) model.load_state_dict(checkpoint['model']) optimizer.load_state_dict(checkpoint['optimizer']) amp.load_state_dict(checkpoint['amp']) # Continue training ... 安装过程中遇到了很多的问题： Build error \"fatal error: ATen/cuda/CUDAGraphsUtils.cuh: No such file or directory\" · Issue #1043 · NVIDIA/apex (github.com) # rollback apex to the previous commit git reset --hard 3fe10b5597ba14a748ebb271a6ab97c09c5701ac cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ pip install -U cpython # this method is not useful command 'gcc' failed with exit status 1 git checkout f3a960f80244cf9e80558ab30f7f7e8cbf03c0a0 1.6. 限制网络的输出范围 实际上，这一部分的应用就属于激活函数的数学理念问题了，我们倘若需要将网络的输出限制在一定的范围内，除了自己编写相关的数据处理手段之外，激活函数实际上有一部分原因就是为了这点设置的。 神经网络基于对非线性运算的需要，引入了激活函数，强化了网络的学习能力； 同时神经网络对于输出有所要求（很多时候是以一种概率表达的方式输出的）所以就会需要softmax（0，1同时sum==1）之类的函数，可以将分类器的原始输出映射为概率。 Sigmoid tanh之类的将输出限制在（0，1），但是并没有对加和有要求，这里可以做一个区分https://www.cnblogs.com/jins-note/p/12528412.html区分sigmoid（多分类）和Softmax（单分类） Softmax和tanh可能会出现梯度消失的问题，ReLU将输出限制在（0，1） 一部分激活函数的特点 所以很显然，我们可以通过对于相应的激活函数的应用，来限制我们的网络输出范围。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-12-16 16:34:42 "},"Experiment/DataAugmentation.html":{"url":"Experiment/DataAugmentation.html","title":"2.1 Data Augmentation","keywords":"","body":"1. Data Augmentation1.1. Principle1.2. PIL，CV2，SkImage的选择1.2.1. 数据读写1.3. 简单图像增强1.3.1. TorchVision1.3.2. OpenCV1.3.3. Pillow1.3.4. SkImage1.4. 混合图像增强（For ML）1.5. 特殊图像增强1. Data Augmentation @AikenHong 2021 intergrate with those augmentation method. this doc will Record those theory and the effect after transformation Show the codes for ez use And the complete .py will be intergrate in my classification pipeline reference below:arrow_down_small:, if use them, start it for respect for his work. aleju/imgaug :star:albumentations-team/albumentations: torchvision PIL/ImageEnhance CCBS opencv 1.1. Principle Principle 1 of coding: Don’t reinvent the wheel unless it’s needed 具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。 Principle 2 of coding 图像增强的两种使用方式： 做全集的增强后存储在本地，然后通过随机载入或者按一定batch的载入来实现我们增强的作用，（or contrasive），这种方式实际上是使用空间来换时间，由于处理是一次性的，所以如果空间充足的话，是更为充足的方式。 动态的在线增强：这种方式比较消耗io和cpu，不推荐，但是如果本地的空间不够，就只能采用这种方式了。 Principle 3 of saving 如果我们要存储本地副本的话，推荐的存储格式和方式 文件格式：npz 由于多种增强，实际上这种方式还蛮适合使用npz格式作为我们的存储，这种既保留了对应的np还可以保留对应的字典信息，此外这种方式的存取速度也不算慢，（相较之下好像没有特别突出的一种格式） 路径格式：imagenet也就是对应的train-class-data的层级关系，通过这种约定俗称的存储关系我们得以在我们框架中的dataset格式方便读入 1.2. PIL，CV2，SkImage的选择 pytorch图像的加载/读取方式 | cv2、PIL、matplotlib 在这里讲解三个模块之间的基本区别和其中的选择，目前希望将已有的算法从PIL转向CV2，后续也会添加一下三个模块对于Torch的适配性等等 1.2.1. 数据读写 基于下列的代码和注释给出相应的之间的区别，通过这些区别我们可以知道几乎所有的格式在使用的时候都是需要对应的转换的， import cv2 img = cv2.imread('lena.png') # numpy格式，HWC，【0，255】，BGR img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # 除了最下方的转换方式外也可以通过这种方式进行转换 cv2.imwrite('lena.jpg', img) # img 需为BGR格式的numpy array import PIL from PIL import Image img = Image.open('lena.png') # PIL格式，RGB，【0，255】 img = np.array(img) # 转换为numpy格式, HWC Img.fromarray(img).save('lena.png') # x需要转换为PIL.Image格式才能保存 from skimage import io img = io.imread('lena.png') # numpy 格式， RGB，【0，1】 HWC io.imsave('lena.jpg') import matplotlib.pyplot as plt img = plt.imread('lena.png') # numpy 格式，HWC [0,255] RGB plt.imsave('lena.jpg') # BGR 转换 RGB, 转回来也是一样的操作 img_rgb = img_bgr[:,:,::-1] 基于上述的代码，我们可以总结出： 当我们使用matplotlib的时候 当我们用cv2读取图像的时候若要进行matplotlib的展示，我们需要对其做bgr2rgb的转换，PIL则需要做到numpy的转换，skimage考虑归一化方面的问题 当我们要使用tensror格式的时候 我们需要对cv2转换成RGB 对三个方法都是用transpose或者类似的方法转换到CHW import torch import numpy as np torch_img = torch.from_numpy(np.transpose(img,(2,0,1))) 1.3. 简单图像增强 在这里由于opencv更为强大且全面，我们将框架转移到opencv中进行图像增强处理，于是本章节会主要介绍opencv, torchvision中的图像增强，同时也会对pillow,skimage`进行简单的介绍。 1.3.1. TorchVision torchvision的使用实际上是最简单便捷的，为了协调统一，该类变换我们在totensor后进行使用，接在其他所有变换的后面，实际上有一些变换是可以获取参数的，要调用的对应函数我们可以在对应文档中查询 随机机制：现已向后兼容torch # 随机种子采用torch import torch torch.manual_seed(17) # KEY FUNCTION: 给一个transformers加上概率，以一定的概率执行该操作 transformers.RandomApply(transforms,p=0.5) # Key Function：从给定的一系列transforms中选一个进行操作 transformers.RandomChoice(transforms) 由于存在randomapply这个函数，所以实际上我们在调用变换的时候，我们可以用prefix random去搜补全，如果没有的话，也可以使用RandomApply来手动赋予随机性。 组合机制：同时使用多种变换，这种方法将一组强关联的变换进行组合，简化后续的使用，但是对于我们如果需要做多种增强的话，实际上并不是一个合适的方式。 # a simple example for torchvision's transformer transform = transforms.Compose([ transforms.CenterCrop(10), transforms.PILtoTensor(), transforms.ConvertImageDtype(torch.float) ]) 常见的一些增强：列在下面 裁剪系列： # cental crop transforms.CenterCrop(size), # five corners and the cental crop transforms.FiveCrop(size), lambda(lambda crops: torch.stack[ToTensor()(crop) for crop in crops]) # 随机裁剪到对应的尺寸， transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant') # 随机裁剪后resize到指定的尺寸 transforms.RandomResizedCropsize, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=) # Padding 无需多言 transformers.Pad(padding,fill=0,padding_mode='constant') 色彩变换增强 # 色彩抖动,随机改变亮度对比度饱和度和色调 transforms.Colorjitter(brightness=0, contrast=0, saturation=0, hue=0) # 图像转换为灰度 transforms.grayscale(num_output_channel=1) 几何变换增强 # 仿射变换 transformers.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=, fill=0, fillcolor=None, resample=None) # 可以通过以下函数获取变换矩阵 get_params(degrees: List[float], translate: Optional[List[float]], scale_ranges: Optional[List[float]], shears: Optional[List[float]], img_size: List[int]) → Tuple[float, Tuple[int, int], float, Tuple[float, float]] transforms.RandomHorizontalFlip(p=0.5) transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=, fill=0) RandomResizedCrop 1.3.2. OpenCV Install Ubuntu：安装opencv的python版本 apt-get -y python3-opencv pip install opencv-python Usage 这一部分可能主要还是对OpenCV基础使用方式的介绍，至于数据增强方面，上面有一个库已经集成了很大一部分图像增强的操作，且在效率上也有了很高的优化和证实，简单的使用方式如下。 try: import albumentations as A except ImportError: os.system(\"pip install - U albumentations\") import albumentations as A import cv2 # Declare an augmentation pipeline transform = A.Compose([ A.RandomCrop(width=256, height=256), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2), ]) # Read an image with OpenCV and convert it to the RGB colorspace image = cv2.imread(\"image.jpg\") image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Augment an image transformed = transform(image=image) transformed_image = transformed[\"image\"] 下面我们会讲一些opencv中的简单使用方式： (一)OpenCV-Python学习—基础知识 - silence_cho - 博客园 (cnblogs.com) 1.3.3. Pillow Using PIL do some augmentation CCBS： color，contrast，brightness，shapen # way1：PIL from PIL import ImageEnhance ccbs_img = ImageEnhance.Color(img).enhance(factor1) ... Blur，Detail，Edge_Enhance，Smooth，Sharpen… 自行在文档中找到对应的方法，实际上不是很多 from PIL import Image from PIL.ImageFillter import SMOOTH,BLUR img = Image.open(image_pah) img_1 = img.filtter(BLUR) img_2 = img.filtter(SMOOTH) 1.3.4. SkImage 1.4. 混合图像增强（For ML） ==ALL in one 不是什么好点子== 为了对Machine Learning中的任务进行图像增强任务，我们在过程中可能会使用一些Github Repo，主要可能就是albumation，在进行图像增强和数据混合的过程中，我们会遇到的问题包括： PIL和NP，Tensor的三者格式不对应的关系 Channel混杂的关系 调用transformer的形式不统一的问题 为此我们特地开了这个专题，介绍一下使用的方式，以及使我们在后续的使用过程中注意最终转换到datalist中最好采用统一的存储形式 path（load by cv or pil，st np） data （np（else），tensor（cuda）for instant use） 以此来规范我们的dataset，and sampler or mixer 我们可以尝试使用如下的方式来进行transformer的管理操作，此外如果我们要用的是纯粹的数据增强而不是用来dataloader的transformer，我们就不要用这个逻辑去做，我们直接集成Augmentation就好了 ch class mytransformer(): # if augs is not none we need to add ToPIL to get the right data type def get_transformers(): self.transformer = { 'augs': Augs.Compose([]), 'trans': transformers.Compose([ tranformers.ToPIL(), ... ]), } # then we using k-v pair to do all my transformer, def __call__(img): for k,v in transformer: img = transformer['augs'](image=img)['image'] img = transformer['trans'](img) return img 1.5. 特殊图像增强 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2022-01-19 15:54:40 "},"Experiment/MLFlow.html":{"url":"Experiment/MLFlow.html","title":"4.1 ML Flow","keywords":"","body":"1. MLFlow 机器学习系统的使用1.1. 基本部署1.2. Tracking 实验版本跟踪1.3. Reference1. MLFlow 机器学习系统的使用 @Aiken 2020 基于Python开发的DAG数据工作流系统，面向机器学习，支持Spark并行环境和K8S容器集群； MLFlow主要解决了三个问题，也就是三个我们可能会需要使用的功能： Tracking：跟踪实验训练结果，记录算法参数，模型结果和运行效果等等； Projects：对所有的算法项目有一套标准的projects概念，记录下代码版本，参数和运行环境这些东西，并且projects是可以拟合所有的算法框架的； Models：解决的是打包和部署模型的这样一个行为，提供json接口给后续的flsk框架等等进行使用 1.1. 基本部署 INSTALL： DEPLOY： 1.2. Tracking 实验版本跟踪 Tracking为本次描述的重点，来做一个训练过程中的版本管理，记录每一次训练的参数和变量信息等等，这样便于后续的恢复和实验信息的整理。便于统计和管理。使用的时候好像也是需要代码嵌入的部分，就是需要在代码中调用MLFlow的API。 但是在Tracking的时候有一个比较重要的点在于，这个方法和Tensorboard对原模型的参数的嵌入和Logging记录中会不会产生冲突，同时两个方法之间是不是有什么overlap；关键的问题： 这两个API能不能进行混合使用 怎么统一和区分两个方法的应用情景 1.3. Reference https://mlflow.org/docs/latest/tracking.html https://mlflow.org/docs/latest/projects.html https://github.com/mlflow/mlflow https://blog.csdn.net/chenhuipin1173/article/details/100913909 https://my.oschina.net/u/2306127/blog/1825638 https://www.zhihu.com/question/280162556 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 12:53:29 "},"JobHunting/IntView_笔试题型和框架总结.html":{"url":"JobHunting/IntView_笔试题型和框架总结.html","title":"1. 笔试题型和框架总结","keywords":"","body":"1. 题型和框架代码总结1.1. C++笔试读取输入的操作1.2. 常见数据类型和特殊专题1.2.1. 随机数问题1.2.2. 环形问题：1.2.3. 单调栈模板1.2.4. 链表问题1.2.5. 二叉树问题1.2.6. 动态规划问题1.2.7. 博弈问题1.2.8. 背包问题1.2.9. 贪心算法1.2.10. 回溯算法（backtrack）1.2.11. BFS算法：1.2.12. 分治算法1.2.13. 双指针&&二分查找&&滑动窗口1.2.14. Union-Find 并查算法1.2.15. 排序算法总结：1.2.16. 各种数据结构的模拟编写：1.2.17. 一些其他的问题1.3. 一些常用操作的复习1. 题型和框架代码总结 @Aiken 2021 简明的描述相应的框架和提醒分析，不输出冗余内容，结合笔记和跳转链接进行具体的复习，该文只作为大纲使用。 适用的情况分析 算法的具体框架和思路 特殊情况描述 怎么读取命令行的输入，在笔试的时候可能会需要 1.1. C++笔试读取输入的操作 待整理： Link1；Link2 输入通常使用while+cin>>来进行，这种方式也可以直接读入整行的string； 需要注意的是，这种方式的话，如果使用的是getline，当遇到换行符的时候cin会直接停止继续输入； while(cin>>a>>b); // 主要是按照类型来进行cin，操作应该是通过空格来分割的。 // 按照我们执行的次数来进行指定次数的读取操作 以指定符号分割的字符串输入 char str[3][11]; cin.getline(str[0], 11, ','); //接收最多10个字符 ,以‘，’作为结束符 cin.getline(str[1], 11, ','); cin.getline(str[2], 11); //默认结束符 enter 1.2. 常见数据类型和特殊专题 经常在那种预设值出问题，导致不会更新后续的max or min 1.2.1. 随机数问题 470：用rand7 实现rand10： 拒绝采样 实际上就是使用7进制来实现10，拒绝多余的选项，继续进行采样 class Solution { public: int rand10() { int col, row, idx; do { row = rand7(); col = rand7(); idx = (row-1) * 7 + col; } while (idx > 40); return 1 + idx % 10; } }; 1.2.2. 环形问题： 涉及到环形的问题的时候，如果我们是需要进行遍历操作的话，可以考虑通过取余的方式去解决； 如果是进行是否是环的判断，我们可以考虑双指针的问题（快慢指针），然后让一个先行之类的策略。 1.2.3. 单调栈模板 下一个更大的xxx的问题；同样，单调的队列也是一个意思(可能就是有窗口的长度约束之类的) 还有像什么字典序的排列，这一题值得一做，是一个比较灵活的情况 是一种保持栈中的数据从大到小的结构，当下一个比当前的数据更大的值出现的时候，我们就需要对数据进行记录，然后进行重排。 设计到环形的数据，也就是更大的下一个最终可能在前面的情况，我们考虑通过原长度的双倍长度去遍历，然后通过取余去取值即可 index % n = new index; 需要注意的是index和取值之间的关系也就是我们到底应该存index还是存值； 窗口是K，那么index到k-1就需要开始判断了，也就是第k个，可能这个时候就需要进行弹出之类的（这种情况是否应该使用双端队列） 特殊取值问题，我们要考虑输入如果只有一个数字的情况下，还有两个size不等的情况下到底怎么做 1.2.4. 链表问题 这里有两类题需要我自己再揣摩一下，一个是K个一组反转（使用迭代+递归的方式），还有一个是求回文链表的两种技巧（快慢指针同时能判断奇偶的特性） 其中典型的技巧就是双指针技巧和前序后续遍历技巧，以及快慢技巧 类通用的额外head来实现正向和后序遍历同时进行的过程。(比如判断回文链表) 1.2.5. 二叉树问题 二叉树问题很多情况下表现为一个遍历和递归的问题，同时其中最重要的是3种遍历方式的特性：前序、中序、后序，典型的就是重建二叉树；下面这一题是比较有启发性的： 二叉树中的最大路径和（142） return的值和中间进行判断，以及最终结果的统计值是不同的 所以维护一个全局的ans，来区分这个过程。 经典难题 序列化和反序列化 这种用字符串来表示树的，一定要记得间隔和结尾是两个不同的符号来表征； 1.2.6. 动态规划问题 WORKFLOW：明确状态（DP表各个index对应的子状态含义）、明确状态转移的过程、通过状态转移判断迭代的方向、根据方向和题解确定base cases、根据状态的转移过程优化压缩dp表的空间 题型方面实际上应该主要是下面的几类问题： 能分成子问题的最优解问题 序列匹配问题 “逐步”的过程：博弈 难点：实际上就是动态转移方程的构建和dp表的建立，我们要明确这样一个状态转移的过程。 思路：和递归是一样的，假设子问题解决了，到新问题， 压缩：要注意我们在删除维度的时候是不是会导致新的数据覆盖掉了需要的数据（决定好迭代的方向） 闫式DP分析法： 状态表示（把集合表现为数） & 状态计算 （化整为零，划分成若干个子集去做，） 属性：max，min，count；（题目问的是什么） 划分子集依据：寻找最后一个不同点 LINK1 ； Link2 选择问题： 每一个维度是一个条件 第一个维度：我只考虑前i个问题（物品），第二个维度一般都是限制，存放属性（也就是最大的xxx） 最后一个不同点：对最后一个物品i，做出的不同选择，来进行状态转移 正则表达式问题： 永远的痛，我们怎么把他归化到小问题，也就是为什么我们最多只需要往回退一次 考虑当前组合：匹配并丢弃（往前规划的时候j-2） j-1 匹配不丢弃（往前归化的时候j不变） i-1 不匹配。j-2 class Solution { public: bool isMatch(string s, string p) { // 考虑两种情况：普通的匹配和*号匹配 // 建立DP-table 考虑初始情况 int m = s.size() + 1; int n = p.size() + 1; vector> DP(m, vector(n, false)); DP[0][0] = true; // 下面这个可以转换成&&语句可能会更好看点 for (int i = 2; i 编辑距离 这一题的二维很好写，也是一个帮助理解DP方法的一个很好的例子，但是在压缩成一维的时候有很多的细节这里要注意 首先是初始化的问题，还有在内层循环要将第一个数的初始化在这里进行赋值，（第一行，和第n行的第一个需要被初始化） 这里i-1，j-1的形式就比较老生常谈了； 这里的if else 就变成必要的了！（所以能表达的更清楚的时候，大多数时候我们还是不要省略else，这样的话也能减少不必要的运算）（暂时还没搞清楚），同时这里的和自己进行min的方式就不能用了，因为我们原本是初始化成一个很大的数，但是在这里的象征意义就已经被改变了。 class Solution { public: int minDistance(string word1, string word2) { // 实际上这题已经很熟悉了，这里需要做的就是构建一个DP table，以及状态转移 // 切分的状态就是string的index if(word1.empty()) return word2.size(); if(word2.empty()) return word1.size(); // 建立dp table size_t len1 = word1.size(); size_t len2 = word2.size(); vector dp(len2+1); // 初始化base-case，当一个是空，另一个不是空的情况 for(int i =0; i 俄罗斯套娃： 这种两个维度，然后相互嵌套的问题，为了让其中的一个等大被抵消掉，所以一个逆序，另一个正序，然后寻找最长递增子序列。（找最长子序列的时候要找对搜索方向） 子序列问题： 一维的DP数组：这种子序列问题（子序列不同于子串），需要的一般都是以i为结尾的情况下，取得的最值，这样才符合我们需要归纳 的条件。 // 基础的算法模板如下 int n = array.length; int[] dp = new int[n]; for (int i = 1; i 在子数组array[0..i]中，以\\array[i]*结尾的目标子序列（最长递增子序列）的长度是dp[i]*。 二维的DP数组:这种思路其实用的更多，尤其是涉及到数组，两个字符串这样的问题的情况下，这种思路实际上涵盖了，包含一个字符串和两个字符串的情况 int n = arr.length; int[][] dp = new dp[n][n]; for (int i = 0; i 涉及两个字符串/数组时（比如最长公共子序列），dp 数组的含义如下： 在子数组arr1[0..i]和子数组arr2[0..j]中，我们要求的子序列（最长公共子序列）长度为dp[i][j]。 可以参考的是编辑距离和最长公共子序列两个文章 只涉及一个字符串/数组时（比如本文要讲的最长回文子序列），dp 数组的含义如下： 在子数组array[i..j]中，我们要求的子序列（最长回文子序列）的长度为dp[i][j]。 最长回文子序列 假设i+1，j-1已经是最好的情况，当现在的两端不是相同，就没办法同时产生增益，也就是要么是左侧的最大值要么是右侧的最大值两种情况 最长公共子序列 实际上考虑下来和最长回文子序列是一样的问题，两边相同，同时产生增益，但是如果两边不同的话，就是两侧的其中一个max了。 四键键盘问题 实际上实现上和最长递增子序列稍微有点像，但是思路上的差别还是比较多的：要么就是一直按ａ，要么就是在某个地方开始ｃｖ所以我们存放的还是当前的最长值，然后通过一个内层循环，判断从哪开始cv就行。 股票问题 简单的直接贪婪的计算正向差就完事了，或者通过迭代来计算前向的最大和； 动态规划方法的计算框架：定义两个维度，当天我们是否持有股票，然后进行状态的转移方程；（空间复杂度未优化的计算情况如下）（可以优化成4个值，实际上3个就好） class Solution { public: int maxProfit(vector& prices) { int n = prices.size(); // 根据受伤是否持有股票来进行运算，存放第i天能获取的最大利益； vector> dp(n, vector(2, 0)); // 可以先将购买消耗掉 dp[0][1] = -prices[0]; for (int i = 1; i 限制交易次数（最通用的情况）： 使用动态规划的话，+次数约束：把pair分开，变成两个加了k约束的dptable；188题，可以再看看 资金冻结一天，就将交易时间稍微修改一下，卖出后的那个值变成i-2就行了 每次需要手续费，我们只需要在购买的时候扣除手续费就行。 状态机解法 实际上也就第三题只进行两次交易的情况好一些，也就是说，当我们只进行一次买入的时候是s1，然后我们进行一次买入一次卖出的时候是s2，以此类推，然后再上一个s的情况下进行状态的更新就性了没什么特殊的地方。 打家劫舍问题： 实际上像二叉树那样的题目可以和股票一样分成两个状态去做，也能够和之前是一摸一样的，用递归的方法加上备忘录去做，但是递归的方法实际上就是dfs，我们先递归到最底层然后加入备忘录然后进行计算，这样我们在递归的时候就要确定现在的这个值我们之前是不是已经计算过了。 最坏情况下的最好 高楼扔鸡蛋问题 : 通过碎或不碎来对问题切割成子问题，然后两个单调序列使用二分法查找； 戳气球问题 这题比较难就另外提出来说了：实际上思路就是假设i，j是戳破i，j这个开区间以后能够得到的最大的res，然后我们遍历每个k，其中的最大值就是i，j的最大值了。然后这就是个动态归化的问题，搜索方向其实也比较简单，写起来就没啥 KMS字符串匹配算法： 基本想要实现的就是让指针i不走回头路，不会进行重复的扫描，理论上这样需要一个影子指针去做。 实际上就是设计一个delay，然后同步进行状态的推进，然后base case就是当匹配到i个字符的时候进行加1，然后main遇到每个元素后要跳转到哪，全问delay，只有匹配的时候进行下一个状态的演变 1.2.7. 博弈问题 实际上是一种特殊的动态规划问题，这里的技巧是使用pair，然后将dp中的值定义为first：先手能取到的最高总分； sec：后手能取到的最值总分； 然后我们就能通过先后手和当前的状态图来计算得到一个状态传递方程了 1.2.8. 背包问题 实际上是动态规划的子问题，但是这里我们还是独立出来说一下 （核心）状态的选择：分别是重量、对应的item，存储当前的value，然后每次对item做0，1选择来做状态转移。 int knapsack(int W, int N, vector& wt, vector& val) { // vector 全填入 0，base case 已初始化 vector> dp(N + 1, vector(W + 1, 0)); for (int i = 1; i 分割等和子集： 需要注意，当我们进行bool转化和压缩的时候，为了防止被更新覆盖掉，所以我们最好还是使用反向的迭代方式 零钱兑换问题：状态转移实际上还是和分割子集是一样的，就是如何将用不用这个item（因为可以是无数个），给归化出来。 用其他硬币到j 用所有硬币到[j-coint[i]] 1.2.9. 贪心算法 实际上就是一种极端的思想，取各个最优的情况进行选择的这种思想： 最长上升子序列：让子序列的末尾尽可能的小： 当我们大于数组的结尾的时候就给数组的结尾加长，不然就找到第一个比他小的数组后面那个数字给替换掉。在这里使用二分查找的思路。 区间重叠、破气球问题：按照区间结束得早来排序，最终归纳出重叠区间即可 跳跃游戏：没啥好说的就，贪，然后找到最远的地方，然后这个时候要注意一下，我们改如何去更新步数，就好（每一次的最远的边界更新一次距离即可） 1.2.10. 回溯算法（backtrack） 解决一个回溯问题，实际上就是一个决策树的遍历过程。只需要考虑三个问题 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 具体代码框架：其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」 我觉得这里应该就是存储一个中间结果来做把。通过具体的代码来看看 result = [] def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) return for 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 全排列问题、组合问题、n皇后问题、子集问题、数独（主要是棋盘格子中的坐标转换，其他的和n皇后没什么区别）、合成括号生成（也就是分别对左右进行回溯，然后其他的也没什么区别了） class Solution { public: vector> permute(vector& nums) { vector> res; backtrack(res, nums, 0, nums.size()); return res; } void backtrack(vector>& res, vector& output, int first, int len) { if (first == len) { res.emplace_back(output); return; } for (int i = first; i 1.2.11. BFS算法： 其他的也不多说了，实际上就一个重点，那就是要使用队列这个数据结构来做，BFS也有一个特点就是，找到的路径一般是最短的， 也就是和七点的最短距离，但是相应的我们会需要更多的空间复杂度。 在BFS需要深度的时候，我们可以 同时用pair来维护一个深度信息. 每次把queue中的数据清空再来size++ // 计算从起点 start 到终点 target 的最近距离 int BFS(Node start, Node target) { Queue q; // 核心数据结构 Set visited; // 避免走回头路 q.offer(start); // 将起点加入队列 visited.add(start); int step = 0; // 记录扩散的步数 while (q not empty) { int sz = q.size(); /* 将当前队列中的所有节点向四周扩散 */ for (int i = 0; i 转盘锁问题 实际上我们可以看成每个状态有8个可能的前进方向，然后用BFS就能优先遍历到。如果是回溯的话也差不多的写，但是问题是这样的搜索时间是一定需要遍历完所有的可能性的。实际上回溯就是我们每一步的变换+4就行了，基本的答题代码还是一致的。 用HASH来避免重复的遍历 双向的bfs优化：维护两个set（用来交替进行）和一个visited。 滑动谜题：实际上也能分析成是一个树一样的决策结构，然后我们希望找到最低的深度，这样的话，关键还是遍历还有一个避免重复。 Trick：转化为维度1来节省空间和时间，也可以直接将邻居列出来 // 记录一维字符串的相邻索引 vector> neighbor = { { 1, 3 }, { 0, 4, 2 }, { 1, 5 }, { 0, 4 }, { 3, 1, 5 }, { 4, 2 } }; 1.2.12. 分治算法 实际上和回溯，动态规划都是特殊的递归： 回溯算法就一种简单粗暴的算法技巧，说白了就是一个暴力穷举算法，比如让你 用回溯算法求子集、全排列、组合，你就穷举呗，就考你会不会漏掉或者多算某些情况。 动态规划是一类算法问题，肯定是让你求最值的。因为动态规划问题拥有 最优子结构，可以通过状态转移方程从小规模的子问题最优解推导出大规模问题的最优解。 分治算法呢，可以认为是一种算法思想，通过将原问题分解成小规模的子问题，然后根据子问题的结果构造出原问题的答案。这里有点类似动态规划，所以说运用分治算法也需要满足一些条件，你的原问题结果应该可以通过合并子问题结果来计算。 基本框架： void sort(int[] nums, int lo, int hi) { int mid = (lo + hi) / 2; /****** 分 ******/ // 对数组的两部分分别排序 sort(nums, lo, mid); sort(nums, mid + 1, hi); /****** 治 ******/ // 合并两个排好序的子数组 merge(nums, lo, mid, hi); } 1.2.13. 双指针&&二分查找&&滑动窗口 一些常见的用法：（实际上要么就是两个方向，要么就是两个步调） 是否有环：相遇可以判定有环； 找到环的起始点：相遇后，把一个调到头，同速前进，再次相遇即是起始点。 链表的中点：快慢指针，快指针到达终点。 延申问题：对链表进行归并排序，通过快慢指针实现二分的操作，合并两个有序链表。 起始点偏差：先让一个指针走k步，另一个指针再出发，寻找链表的倒数第k个元素 类型题总结 快慢指针：链表操作，归并排序找中点，链表成环搞判定； 左右指针：反转数组，二分搜索 滑动窗口：字串问题，左右指针滑动，前后并进 快慢指针的常见用法： 二分查找算法，没啥好说的 子数组之和：只要数组有序，就要想到双指针技巧。通过调节left和right来调整sum的大小。找到对应的区间 反转数组：从前或从后出发，然后直接互换。 下面讲的滑动窗口 27.移除元素那一题，我原本写法的优越性，这里可以掌握一下，为啥用while的方式的话，效果反而不好呢 移除0 这个题还是有一些细节的，普通的写很容易出错 class Solution { public: void moveZeroes(vector& nums) { if (nums.empty()) return; // 直接进行值的改变是不是好一点，然后在后面进行添加就好了 int index1 = 0; int index2 = 0; while(index2 二分查找：(了解了，问题不大，晚上再来看下就好) 三种情况：找到值，左侧边界，右侧边界（） 找到值的方式就没什么好说的：我们直接当相等的时候返回就行，我们主要分析一下左侧边界和右侧边界的情况到底是什么含义： 左侧边界：满足某个条件的最左边（最小值），比如大于等于二的边界（最左边的那个2）； 右侧边界：满足某个条件的最右边（最大值），比如小于等于二的边界（最右边的那个2）； 结论1：两侧的边界是对称的：实际上画一下还是很容易分析出来的（>= / 的direction）： 左侧边界： >= = : r = mid-1 检测左侧边界是否合理 return l； 右侧边界： : r = mid-1 检测右侧边界是否合理 return r； 结论2：>= 换成> :或者小只需要换一下相等的情况即可： 左侧边界：> : r = mid-1 检测左侧边界是否合理 return l； 右侧边界：= : r = mid-1 检测右侧边界是否合理 return r； 使用二分查找来解决数组的题目 存在如下的遍历架构显然是使用二分查找的方法来优化的：koko吃香蕉，货物运输 for (int i = 0; i nsum问题 （2-sum）这种问题的关键在于，排序后指针双向而行的时候，怎么排除重复的元素（可以使用while直接跳过当前的指即可） （3-4sum）首先取第一个值，然后对剩下的部分求n sum 穷举，（还是需要sort的） 滑动窗口： 基本的框架思想： 左闭右开称为窗口； 先增大right到满足，再缩减left直到不满足，每次增加left都要更新一次结果； 重复2，直到r->end; 具体的实现框架： /* 滑动窗口算法框架 */ void slidingWindow(string s, string t) { unordered_map need, window; for (char c : t) need[c]++; // 初始化状态，便于搜索 int left = 0, right = 0; int valid = 0; // 统计满足情况的数有多少，和需要的匹配时更新答案 while (right 典型例题（76）最小覆盖字串：实现代码后续再做一次 class Solution { public: string minWindow(string s, string t) { if (s.empty() || t.empty()) return {}; // 考虑到出现重复字符的情况，所以需要有一个int进行计数 unordered_map need, windows; int l = 0, r = 0, n = s.size(); // 统计需要的每个字符的数量 for (char c : t) need[c]++; int valid = 0, needv = need.size(); int start = 0, lens = INT_MAX; while (r 567题的实现细节要看懂：因为是排列，所以长度一定要相同，所以我们每一次递进right的时候同时收紧left就行 盛水最多的容器 这一题的思路实际上还是典型的如何去缩减这样的问题规模上比较巧妙 也就是怎么去排除不可能的项，来确定我们的移动方向 1.2.14. Union-Find 并查算法 判断连通性的算法：具体实现上，我们实际上就是通过父节点是否相同去判断的；这实际上是一种反向的链表，也就是我们的指针是parent而不是指向next的； 指针指向父节点，根节点指向子集 如果要通过这种方式去判断联通的话，那么树的平衡性就是特别重要的问题，也就是我们每次进行接入的时候，我们最好都对该森林进行指向性优化： 如果我们的parent有parent，我们就直接指向parent？is that right？ 每次将小树接到大树后面，而不是反过来； 需要实现的方法： find（par == self，在find的时候进行压缩）、 connect（是否联通）、 Union（链接，需要优化） 被围绕的区域130：实际上是一个DFS的题目，我们可以展开成一维去实现，同时在这种情况下使用UF也是可以的，需要注意到的是，每次的四个方向的边界判断。（这应该是这题的难点） 1.2.15. 排序算法总结： 各类时间复杂度，思想，实现 （最后进行正合理和分析） 参考链接1很直观；参考链接2 一、插入排序： 实现：实际上就是对index前面的数据进行从后往前的遍历，遍历过程>index的情况下直接后移，找到合适的位置的时候就将index->value 放到new index中 思想：每步将一个待排序的记录，按其顺序码大小插入到前面已经排序的字序列的合适位置，直到全部插入排序完为止。 关键问题：在前面已经排好序的序列中找到合适的插入位置。 方法： 直接插入排序、二分插入排序、希尔排序 直接插入排序 插入排序的最好情况是数组已经有序，此时只需要进行n-1次比较，时间复杂度为O(n) 最坏情况是数组逆序排序，此时需要进行n(n-1)/2次比较以及n-1次赋值操作（插入） 平均来说插入排序算法的复杂度为O(n2) 空间复杂度上，直接插入法是就地排序，空间复杂度为(O(1)) 二分插入排序 ：实际上就是修改了前面的搜索过程 最坏情况：每次都在有序序列的起始位置插入，则整个有序序列的元素需要后移，时间复杂度为O(n2) 最好情况：待排序数组本身就是正序的，每个元素所在位置即为它的插入位置，此时时间复杂度仅为比较时的时间复杂度，为O(log2n) 平均情况：O(n2)，实际上就是将搜索的过程变成了logN 空间复杂度上，二分插入也是就地排序，空间复杂度为(O(1))。 希尔排序：缩小增量排序 实际上就是根据一个增量序列将原数组切分成一个个子序列然后进行插入排序，然后增量越来越小，最后增量为一进行i一次整体的排序（利用的是直接插入排序在小数组上时间效率高的特点） 增量排序的时间复杂度依赖于所取增量序列的函数，但是到目前为止还没有一个最好的增量序列.有人在大量的实验后得出结论;当n在某个特定的范围后希尔排序的比较和移动次数减少至n^1.3 不管增量序列如何取值，都应该满足最后一个增量值为1。 有文献指出，当增量序列为d[k]=2^(t-k+1)^时，希尔排序的时间复杂度为O(n^1.5), 其中t为排序趟数。 空间复杂度上，SHELL插入也是就地排序，空间复杂度为(O(1))。 // shell 排序 # include using namespace std; const int INCRGAP = 2; void shellsort(vectornums) { int insertNum = 0; // 首先计算出各个GAP，然后基于GAP进行插入排序 unsigned int gap = nums.size()/INCRGAP; // GAP>=1 while(gap){ // 实现插入排序；每一个树按照gap去找其前面的数 for(unsigned int i = gap; i=gap && nums[j-gap]>temp){ nums[j] = nums[j-gap]; j-=gap; } // 当数字到了最前面或者第一个比他小的时候，插入 nums[j] = temp; } gap /= INCRGAP; } } int main(){ vector a = {1,3,41,23,5,23,53,123,1}; shellsort(a); for(auto i:a) cout 二、选择排序： 直接选择排序： 思想：每趟从待排序的记录序列中选择关键字最小的记录放置到已排序表的最后（tail）位置，直到全部排完。 关键问题：在剩余的待排序记录序列中找到最小关键码记录。 实际上很明（显就是遍历求最值，O（n^2） 堆排序 由于堆我们知道可以通过VECTOR，然后下标*2 \\ *2+1来索引树结构，然后我们把大顶堆和末尾的数字交换位置，并重新建堆，重复这样的过程即可。 时间复杂度主要来自：1. 建堆 2. 调整堆 堆排序的时间复杂度主要由两部分组成：初始化建堆和每次弹出堆顶元素后重新建堆的过程 初始化建堆过程的时间复杂度O(n)：假设堆的高度为k，则从倒数第二层右边的节点开始，这一层的节点都要进行子节点比较然后选择是否交换，倒数第三层类似，一直到第一层(即层数从k-1到1)；那么总的时间为$(2^{(i-1)})(k-i)$，其中i表示第i层(范围是k-1到1)，2^(i-1)表示该层上有多少元素，(k-i)表示子树上要比较的次数，即$S = 2^{(k-2)}1 + 2^{(k-3)}2 + 2^{(k-4)}3 + ... + 2^1(k-2) + 2^0(k-1)$，使用错位相减法(用常数2来辅助转换，两边都乘以2再减去原等式)得到S = 2^(K-1) + 2^(K-2) + 2^(K-3) + ... + 2 - (K-1)，忽略最后一项常数项就是等比数列，即$S=2^k-2-(k-1)=2^k-k-1$，又因为k为完全二叉树的深度，所以有 2^k 弹出堆顶元素后重建堆过程的时间复杂度O(nlogn)：循环n-1次，每次都从跟节点往下循环查找所以每一次时间都是logn，总时间为(n-1)*logn = nlogn - logn 故堆排序的时间复杂度为O(n) + O(nlogn) = O(nlogn) 堆排序是接地排序，所以空间复杂度为常数O(1) void abjust_heap(vector& nums, int index, int lens) { // 根据二叉树的性质，索引到子节点的坐标 // TODO:但是还要判断越不越界(这里由于我们不是堆，而是一个排序的过程，长度在变，所以需要这个参数的输入) int n = lens; int left = (2 * index) nums[right] ? left : right; // 和其中比较大的那个值互换，如果越界了是自己，和自己换 if (nums[index] & nums) { // 通过下标索引来建立堆 // 由于二叉树的叶节点一定是总数的一半左右（len/2 or len/2+1） // 然后再根据数值和坐标的映射关系-1.可以知道非叶子节点的坐标是(len/2-1) int n = nums.size(); // 首先进行建堆的过程 for (int i = (n >> 1) - 1; i >= 0; i--) { // 从倒数第二层开始一个个的进行交换 abjust_heap(nums, i,n); } // 然后依次弹出头部元素进行测试； for (int i = n - 1; i >= 0; i--) { swap(nums[0], nums[i]); abjust_heap(nums, 0, i); } } 三、交换排序： 冒泡排序： 就是对相邻的两个数进行比较，然后将大数往后放，把小数放到前面，第一次冒泡到n-i，递减排序完就行。 最坏情况：冒泡排序要进行n-1轮排序循环，每轮排序循环中序列都是非正序的，则每轮排序循环中要进行n-i次比较(1 最好情况：待排序数组本身就是正序的，一轮扫描即可完成排序，此时时间复杂度仅为比较时的时间复杂度，为O(n) 平均情况：O(n2) 空间复杂度就是在交换元素时那个临时变量所占的内存空间，最优的空间复杂度就是开始元素顺序已经排好了，则空间复杂度为0，最差的空间复杂度就是开始元素逆序排序了，则空间复杂度为O(n)，平均的空间复杂度为O(1) 快速排序： 基本思想：选择一个基准元素,通常选择第一个元素或者最后一个元素,通过一轮扫描，将待排序列分成两部分,一部分比基准元素小,一部分大于等于基准元素,此时基准元素在其排好序后的正确位置,然后再用同样的方法递归地排序划分的两部分，直到各区间只有一个数。 最好情况：是每轮划分都将待排序列正好分为两部分，那么每部分需要的时间为上一轮的1/2。如果排序n个元素的序列，其递归树深度为[logn]+1即仅需递归logn次，需要总时间为T(n)的话，第一次需要扫描整个序列，做n次比较，然后将序列一分为二，这两部分各自还需要T(n/2)的时间，依次划分下去：T(n) = 2T(n/2)+n T(n) = 2(2(T(n/4)+n/2)+n = 4T(n/4)+2n 等等，且T(1) = 0，所以T(n) = nT(1) + nlogn = O(nlogn) 最坏情况：当待排序列为有序序列(正序或倒序)，每次划分后得到的情况是一侧有1个元素，另一侧是其余元素，则最终要进行n-1轮循环，且第i次循环要进行n-i次比较，总比较次数为n-1 + n-2 + ... + 1 = n(n-1)/2，即时间复杂度为O(n2) 空间复杂度待补充。 int quickSortPartition(vector& s, int l, int r) { //Swap(s[l], s[(l + r) / 2]); //若以中间数为基准，则先将中间的这个数和第一个数交换即可 int i = l, j = r, x = s[l]; //将最左元素记录到x中 while (i = x) j--; if (i x的数 while (i x，那么算法将终止 s[j--] = s[i]; } s[i] = x; //i的位置放了x，所以其左侧都小于x，右侧y都大于x return i; } void quickSort(vector& s, int l, int r) { //数组左界= r) { return; } // 划分，返回基准点位置 int i = quickSortPartition(s, l, r); // 递归处理左右两部分，i处为分界点，不用管i了 quickSort(s, l, i - 1); quickSort(s, i + 1, r); } def QuickSort(list_): if len(list_) my version quicksort int quickSortPartition(vector& s, int l, int r) { //Swap(s[l], s[(l + r) / 2]); //若以中间数为基准，则先将中间的这个数和第一个数交换即可 int i = l, j = r, x = s[l]; //将最左元素记录到x中 while (i = x) j--; // 从左向右找第一个>x的数 while (i & s, int l, int r) { //数组左界= r) { return; } // 划分，返回基准点位置 int i = quickSortPartition(s, l, r); // 递归处理左右两部分，i处为分界点，不用管i了 quickSort(s, l, i - 1); quickSort(s, i + 1, r); } 四 归并排序 时间复杂度：归并排序主要分为拆分和对有序数组进行排序，拆分操作的时间复杂度为logn，排序的复杂度为n，所以归并排序的时间复杂度为O(nlogn) 归并排序的空间复杂度就是那个临时数组和递归时压如栈的数据占用的空间：n + logn，所以空间复杂度为O(n) (1)基本思想:归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。归并排序中第二步，对两个有序数组排序法则非常简单，同时对两个数组的第一个位置比较大小，将小的放入一个空数组，然后被放入空数组的那个位置的指针往后移一个，然后继续和另一个数组的上一个位置进行比较，以此类推。直到最后任何一个数组先出栈完，就将另外一个数组里的所有元素追加到新数组后面。 ​ 归并排序和快速排序有那么点异曲同工之妙，快速排序：是先把数组粗略的排序成两个子数组，然后递归再粗略分两个子数组，直到子数组里面只有一个元素，那么就自然排好序了，可以总结为先排序再递归；归并排序：先什么都不管，把数组分为两个子数组，一直递归把数组划分为两个子数组，直到数组里只有一个元素，这时候才开始排序，让两个数组间排好序，依次按照递归的返回来把两个数组进行排好序，到最后就可以把整个数组排好序。 void _sort(int n,int a[],int l,int r){ int mid = (l+r)/2; int tem[n]; if(l == r) return; _sort(n,a,l,mid); _sort(n,a,mid+1,r); //归并，将l→mid 和 mid+1→r 两部分有序的数组归并成一个数组 int la = l,lb = mid+1;int k=l; while(lamid && lbr && la 五 基数排序 快排亲兄弟：快速选择算法 数组中的第k个最大的元素： 自己实现一个二叉堆，（优先队列） 使用快速查找的方法 1.2.16. 各种数据结构的模拟编写： 红黑树？平衡树？B+树 hash 冲突解决的方法 开放定址法、再hash法、链地址、公共溢出区 Stack栈，队列的实现 通过vector或者链表，维护一个头部的指针就行 队列也式类似的实现原理 二叉堆 通过vector和index*2的关系来实现，主要是一个插入和一个重排两部分算法 Hash的实现 待补充 1.2.17. 一些其他的问题 区间问题 也就是先使用排序，然后进行画图分析就行了； 其中986：区间有重叠区域的判断 b2>a1 && b1>a2 （画图看看就知道了） 计算器 乘除法就使用和栈顶元素先结合，使用栈解决最终的加减法；括号进递归。 随机算法 1/i 保留原有选择。 差分数组和前缀和 差分数组主要用于一段区域内的统一加减运算 快速求素数 从下往上搭建false表 快速幂运算 模幂运算 这一题自己推导一下，然后自己写，印象更深 判断括号的合法性： 使用STACK，在出栈的时候进行匹配 如果带通配符的：双向进行查找，左到右的时候把*当成++ 右到左的时候也把*当成++,在遍历过程中只要小于0了就直接失效 这题实际上也可以使用DP，但是怎么做呢 状态转移： 算法： 如果且仅当间隔 s[i], s[i+1], ..., s[j] 能组成有效的括号时，dp[i][j]为 true。只有在下列情况下，dp[i][j] 才为 true： s[i] 是 '*' 号, 且在 s[i+1], s[i+2], ..., s[j] 这个范围能够组成有效的括号 或者，s[i] 为 '('，并且在 [i+1，j] 中有一些 k，使得 s[k] 为 ')'，且(s[i+1:k] 和 s[k+1:j+1])截断的两个间隔可以构成有效的括号； 批量判断是子序列还是字串 先统计出现的位置，然后针对位置进行二分搜索 1.3. 一些常用操作的复习 LINUX：LINK1 GIT：Onenote Docker：Markdown © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-06-11 00:12:29 "},"JobHunting/IntView_面试补充.html":{"url":"JobHunting/IntView_面试补充.html","title":"2. 机器学习补充资料","keywords":"","body":"1. 面试的一些补充内容1.1. 需要补充的一些面试知识1.2. 深度学习部分1.2.1. DenseNet1.2.2. EfficientNet1.2.3. bottleneck1.2.4. Xavier参数初始化方法1.2.5. 模型压缩1.2.6. 超参数搜索的方法1.2.7. 激活函数的稀疏性1.2.8. 梯度消失与爆炸1.2.9. 损失函数的反向推导1.2.10. 过拟合欠拟合的判断和处理方式1.2.11. 有监督学习中涉及到的损失函数1.2.12. 卷积1.2.13. 正则化的具体函数和内在含义1.2.14. Normalization1.2.15. 解释一下attention1.2.16. Pooling的反向传播：1.2.17. Anchor-based和 Anchor-free的理解和辨析1.3. 机器学习部分1.3.1. 回归任务为什么难以训练1.3.2. Selective Search HOG SIFT1.3.3. LR怎么处理低维线性不可分问题1.3.4. 集成学习方法1.3.5. SVM细节1.3.6. 决策树1.3.7. PCA与LDA1.3.8. Boosting与Bagging1.3.9. MAP、MLE、Bayesian1.3.10. 交叉熵 KL散度 信息熵1.3.11. 无监督方法1.3.12. 数据清洗1.3.13. 图像增强策略1.3.14. 评估模块1.3.15. 训练集和测试集的划分机制1.4. 语言部分1.4.1. Python部分1.4.2. C++部分2. ifndef：依赖于宏名称不能冲突，这个能保证内容相同的不同文件不小心被同时包含，实际上是一个预编译的条件语句1. 面试的一些补充内容 LINK1 1.1. 需要补充的一些面试知识 整理一些自己不太清楚的，整理起来然后进行后续的补充复习，对这一部分提到的这一类知识重点进行复习， [x] 模型部署的方法：（MxNET和PyTorch） [x] GBDT、XGBOOST [x] Pooling和BN如何反向传播， [x] RNN和LSTM的基础门了解（复习一下Onenote笔记） [x] Embedding方法是否了解 （这一块不是很熟悉，我只对embedding的有一定的基础认知，具体的方法上gbdt我就不是特别清楚） [x] pytorch mxnet和 tensorflow之间的区别 [x] 我实验的最终的准确率的比重 [x] 项目中遇到的问题 [x] 卷积加速，转化为矩阵的方法 1.2. 深度学习部分 1.2.1. DenseNet 任意的两层之间都存在直接的链接，每一层的输入都是前面的所有层的输出的并集，而该层学习的特征图也会是后面所有层的输入，DenseBlock中需要Feature_map保持一致； Block与Block之间的降采样则使用transition layer，使用BN，1*1的 conv，Pooling来降维 优缺点： 省参数，省计算 抗过拟合，泛化能力强 需要保存Feature占显存 1.2.2. EfficientNet 针对卷积神经网络的模型拓展，可以通过：1 增加网络规模可以、 增加模型宽度，增加模型深度，增加输入图像的分辨率，但是如何去人工调整这个比例和参数，比较麻烦， 实验发现ConvNet缩放过程中平衡网络宽度、深度、和分辨率的维度是很重要的 EfficientNet就是对这些的合理组合，复合型的模型扩展技术集合神经结构搜索技术获得 关键技术复合扩张技术 所以本文提出了复合扩张方法，这也是文章核心的地方，\\alpha,\\beta,\\gamma是我们需要求解的一组参数，如下图公式，带约束的最优参数求解。\\alpha,\\beta,\\gamma分别衡量着depth, width和 resolution的比重，其中 \\beta,\\gamma在约束上会有平方，是因为如果增加宽度或分辨率两倍，其计算量是增加四倍，但是增加深度两倍，其计算量只会增加两倍。 固定公式中的φ=1，然后通过网格搜索（grid search）得出最优的α、β、γ，得出最基本的模型EfficientNet-B0. 固定α、β、γ的值，使用不同的φ，得到EfficientNet-B1, ..., EfficientNet-B7 φ的大小对应着消耗资源的大小，相当于： 当φ=1时，得出了一个最小的最优基础模型； 增大φ时，相当于对基模型三个维度同时扩展，模型变大，性能也会提升，资源消耗也变大。 1.2.3. bottleneck Bottleneck layer又称之为瓶颈层，使用的是1*1的卷积神经网络。之所以称之为瓶颈层，是因为长得比较像一个瓶颈。 中间比较细，像一个瓶颈 如上图所示，经过$11 conv$，中间那个看起来比较细。像一个瓶颈一样。使用 $11$网络的一大好处就是可以大幅减少计算量。深度可分离卷积中，也有这样的设计考虑。如果想具体了解如何大幅减少计算量的话，可以参考参考资料[2] 1.2.4. Xavier参数初始化方法 已经写在笔记上了，但是还要补充的，或者说是理清楚的是怎么推导的或者为什么这么干的，还有其他的初始化方法之间的区别什么的 1.2.5. 模型压缩 为了研究小而快的机器学习模型：1. 对复杂模型进行压缩 2. 直接设计小模型得到训练 轻量网络：MobileNet 主要的网络架构：是使用depthwise separable convolution也就是xception中提到的这种，双分离的卷积 然后使用：ReLU6激活函数（限制最大输出为6） 过程中不使用pooling直接使用stride=2进行下采样 基本的组件如图所示： 最后用avg pooling从$771024 $到$111024$ 对其进行瘦身： 按比例缩放特征图的大小 按比例缩放通道数 核心的计算量实际上在$1*1 conv$这个全通道密集卷积的操作上 V2： 引入了shortcut（残差结构） 在进行depthwise之前先用$1*1$扩张feature_map的通道数 是为了提升效果，参数量比1还是稍微增加了， 这里的1*1和resnet中的是完全相反的： 残差模块：11降维 33卷积 11升维 反残差模块：11升维，33卷积，11降维 原因：因为depthwise卷积不能改变通道数，因此特征提取受限于输入的通道数，所以将通道数先提升上去。文中的扩展因子为6。 pointwise之后的relu改成了Linear激活函数，防止relu破坏特征 原因：relu造成的低维度数据塌陷 https://www.sohu.com/a/307566264_100024677 就是当低维信息映射到高维，经过ReLU后再映射回低维时，若映射到的维度相对较高，则信息变换回去的损失较小；若映射到的维度相对较低，则信息变换回去后损失很大。因此，认为对低维度做ReLU运算，很容易造成信息的丢失。而在高维度进行ReLU运算的话，信息的丢失则会很少。 因为relu函数小于0时候，值为0，是有信息损耗的，而线性激活函数没有信息损耗 轻量网络：Shufflenet V1 带Pytorch ShuffleNet的核心是采用了两种操作：pointwise group convolution和channel shuffle，这在保持精度的同时大大降低了模型的计算量。 缓解$1*1 conv$的计算量，也使用channel sparse connection，但是这种group convolution 也有弊端：特征图之间不通信，所以我们就channel shuffle（均匀打乱） 图3.(a)是一个普通的带有残差结构的深度可分离卷积，例如，MobileNet[5], Xception[6]。ShuffleNet v1的结构如图3.(b)，3.(c)。其中3.(b)不需要降采样，3.(c)是需要降采样的情况。 V2: G1). 使用输入通道和输出通道相同的卷积操作； G2). 谨慎使用分组卷积； G3). 减少网络分支数； G4). 减少element-wise操作。 通道分割在每个单元开始前将通道的输入分成两个，不再使用group卷积，然后concat两个channel并channel shuffle 轻量网络：squeezeNET 利用Fire module 来实现了：Link Knowledge Distiall 实际上很简单，就是用小模型逼近大模型的输出，用大模型来做小模型的监督，也就是让小模型尽量去适应大模型的输出分布（原本应该只是一个类别标签），这其实已经偏向于一个无监督的驯良过程了，或者说是自监督。 训练大模型 计算大模型的soft output 训练小模型，在类别的监督信息以外再加上soft target的loss，用lambda来调节传中 使用小模型进行预测。 1.2.6. 超参数搜索的方法 网格搜索，随机搜索，贝叶斯优化方法（这种方法实际上还不是太清楚，后面可能还需要看看），AutoML 1.2.7. 激活函数的稀疏性 适应性强，针对不同的情况有不同的激活单元 简洁的模型，计算更快 更好的预测能力和更不容易过拟合 更可分 1.2.8. 梯度消失与爆炸 消失：sigmoid ，tanh， 误差的反向传播造作，我们可以看出这种连乘的形式涉及到了很多的参数和导数，这种时候无论是放大还是缩小的效应都很容易累加起来产生梯度的消失或者膨胀， 当对当前层的w进行求导的时候，我们需要先对上一层的x求导，就会得到wt+1 与梯度连乘这样的式子，这也是导致梯度爆炸的原因 sigmoid的最大输出只有1/4所以sigmoid中梯度消失实际上是更常见的时候 再RNN中BPTT这种反向传播的时候也是类似的思路，但是它是以一个n维矩阵的方式向后传播的，这样当雅可比矩阵的特征值小于1的时候，这样的梯度传播会呈现指数级的变化，要么梯度消失要么梯度爆炸 ResNet 缓解深层网络中梯度消失的问题，实际并不是真正的解决这样的问题，只是给出了一个传播的shortcut，所以掩盖了梯度消失的情况出现，实际上ResNet解决更多的是模型退化的问题，也就是 梯度爆炸也可以使用梯度裁剪的方法进行解决：当梯度的范数大于阈值的时候，我们对梯度进行等比的搜索， 1.2.9. 损失函数的反向推导 常见的几个损失函数和误差之间的关系（推一下grad的公式） Softmax CE 1.2.10. 过拟合欠拟合的判断和处理方式 过拟合的训练方法： 获取更多数据 降低模型的复杂度 正则化方法：实际上这就是权重衰减的方法这里药剂的L1（直接减少有效参数的数量），L2正则化结构风险（减少权值过大带来的过拟合的风险）的那些具体定义；dropout等等。 drop可以理解成bagging的办法的继承实现 集成学习方法，通过多个模型的集成来降低单一模型的过拟合风险 early-stop DROPOUT rescale 使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p$ 开始时随机删除，一半P的神经元 在剩下的一般中更新， recycle，然后，最后学出来的参数的存在概率只有（1-P)所以要*概率。 欠拟合降低的方法： 添加新特征：特征与标签之间的相关性太差，我们可能需要挖掘新的特征； 增加模型的复杂度：简单模型的学习能力可能太差了，比如在线性模型中添加高次项，增加网络的深度或者神经元个数 减少正则化参数 1.2.11. 有监督学习中涉及到的损失函数 LINKLINE 0，1损失函数：$max{0,1-fy}$,标签，二分类问题 hinge损失（0，1的进化）：max{0,1-fy} 1.2.12. 卷积 卷积的矩阵加速：https://www.mobibrw.com/2019/17787 转置卷积：LInk1 ；LInk2 转置卷积上采样带来的棋盘效应：https://www.cnblogs.com/hellcat/p/9707204.html 空洞卷积也会带来棋盘效应，也就是采样不均匀，有些点被重复采样，有些点没有被采样到，这样就会像棋盘一样，两种颜色分离的这种情况。 上采样 下采样： SVD：https://www.cnblogs.com/endlesscoding/p/10033527.html 1.2.13. 正则化的具体函数和内在含义 KEYPOINT：https://www.cnblogs.com/alexanderkun/p/6922428.html 实际上我们可以发现这也是一个权重衰减的操作，如果从导数去分析的话，或者我们数形结合，我们会发现这样的w和我们的函数曲线的交点实际上是偏向 Regularization：在目标函数中添加惩罚项，对复杂度高的模型进行惩罚，于是考虑从W的向量出发， 0范数：非零元素的个数，在机器学习中是个NP完全问题，很难求 1范数：绝对值之和 2范数，通常意义上的模，实际上从分布上来看 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦；所以大家比起1范数，更钟爱2范数。 1.2.14. Normalization 重要参考资料 ；参考资料2向东的回答 权重伸缩不变性，可以有效的提高反向传播的效率，也有参数正则化的效果； 数据伸缩不变性；减少梯度弥散，简化对学习率的选择 covariate shift 协方差偏移，实际上是训练集和测试集中，变量的分布的不同带来的问题，这样就会导致模型的效果收到影响，也会印象网络的迭代速度，通过BN就可以将每个mini-batch的数据都拉回到标准正态分布，把带有偏差的数据拉回标准分布，拉回模型擅长的领域中，使得梯度变大，避免梯度小时的问题出现，也能加快收敛的速度。 帮助理解covariate shift，更确切的来说，实际上是source domain和Target domain的数据分布不一致，他们的条件概率相同，但是边缘概率不同： 这样会导致不再是独立同分布： 需要不断适应输入的分布，降低学习速度 下端的变化可能趋向于变大或者变小，容易使得上层落入饱和区，学习过早的停止 会相互影响。 BN，IN，LN，WN Normalization的基本框架就是 $h=f(g·\\frac{x-\\mu}{\\sigma} + b)$ 再次进行平移和缩放是为了模型的表达能力不因规范化而下降；这两个缩放因子是可以学习的，用来对底层的学习结果有所价值，这也会将数据变换到基本在非饱和区中（线性区），然后通过再次的缩放和偏移，提供了非线性。 BNnormalization纵向 是对同一个batch的所有图的逐channel进行的 使用mini-batch来计算相关的均值和方差，element-wised；但是BN是独立的对每个维度（channel）进行统计的，这样的话，如果原始分布差距很大的话，会导致不同的数据训练，这样可能会增加模型的训练难度。 适用场景：每个mini-batch比较大，数据分布比较接近，所以我们需要进行充分的shuffle，不然效果会差很多。再运行中需要统计一阶和二阶统计量，所以不适用动态的网络架构和RNN，；相应的改造就不说了 适用于判别模型中， Instance Normalization 是对单张图片的单个通道单独进行的Normalization，适用于生成模型中，比如风格迁移，主要依赖的是单张图片的实例，所以不适宜对整个batch进行归一化，这样可以加速模型收敛和保证实例之间的独立性 Group Normalization 实际上就是一个normalization的变体： Layer Normalization 横向 是指对同一张图片的同一层的所有通道进行的normalization，所以和公式不一样的就是不需要求M的均值了，是对同一张图片的所有channel进行的 Wegiht Normalization参数规范化 CosineNormalization 1.2.15. 解释一下attention 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。 比如我们看到eating就对food有更高的attention，与此同时color的权重就会比较低。 因为传统的RNN架构，固定的context vector可能会导致序列信息的缺失，可能无法记住长句子和时序对齐的信息，所以诞生了attention. content vector和 attention weight加权，然后和上一个时刻的target和y，生成现在的y。 self-attention：https://zhuanlan.zhihu.com/p/47282410； 内部注意力，也就是内部的自我关联的获取，QKV的关系 Transformer 的基本架构 Transformer 的 Multi-Head 实际上就是有不同的QKV表示，然后我们将其表示起来。 mask屏蔽未来的信息 1.2.16. Pooling的反向传播： Pooling层的主要作用 1、特征不变性，使模型更加关注是否存在某些特征而不是特征具体的位置，对于一些旋转和平移具有不变性2、特征降维，使模型可以抽取更广泛围的特征，减小了下一层输入大小，进而减小计算量和参数个数 3、在一定程度防止过拟合，更方便优化 4、扩大感受野 avg pooling：均分后回传 MaxPooling：只传给最大的一项，其他项的反向传播值为0 1.2.17. Anchor-based和 Anchor-free的理解和辨析 实际上就是把五百问的内容再巩固一下，然后整理过来，这里还有两个比较相关的链接，到时候还不清楚就可以看下。 https://www.zhihu.com/question/356551927/answer/926659692 https://zhuanlan.zhihu.com/p/62372897 Anchor-based方法：在选练之前在训练集熵使用K-Means等聚类方法聚类出一组矩形框，代表主要的长宽，然后在推理时用这些Anchor滑动提取proposal之后在对这些Anchor进行回归和分类，比如扩张和中心点预测 Anchor-free:就是没有预先定义这样的BoundingBox，直接通过网络来预测相应的边框，比如左上角右下角，中心点这样的预测；中心点+长宽 1.3. 机器学习部分 1.3.1. 回归任务为什么难以训练 目标更难，需要适应所有的点 损失函数的角度，MSE之类的，下降与Value相关，对Outline敏感 1.3.2. Selective Search HOG SIFT HOG: 手机知乎 Selective Search：https://zhuanlan.zhihu.com/p/27467369 SIFT：概览 ； 开贴细说 1.3.3. LR怎么处理低维线性不可分问题 特征工程或者使用多项式核和高斯核，将样本映射到可分的空间去进行逻辑回归； 线性回归和逻辑回归的异同点 实际上都是一个函数映射的问题，就是离散化和连续化的区别而已，还有其中的损失函数的设计等等 线性回归和逻辑回归实际上都是线性模型，但是最终一个映射到连续域做拟合，另一个映射到离散域做分类，最主要的区别就在于目的，输出值域，以及损失函数的设计上。 本质上都是一个一个线性映射函数的选择。 LR对连续值特征是怎么处理的 离散化的作用和优势：https://notes.001.io/lian-xu-te-zheng-li-san-hua/ LR本质上属于广义线性模型，通过进行值域的划分，相当于引入了非线性，从而提升了模型的表达能力； 进行离散化后对异常值不敏感，能对轻微的扰动有一定的鲁棒性，但是这还是比较考验特征划分技术的 可以对离散的特征值进行组合，进一步引入非线性 相当于简化了逻辑回归的模型，引入模型复杂度的正则化，这样能够减少过拟合的风险。 LR为什么要归一化 LR的求解过程不就是是梯度下降的方式，那么就是归一化的作用了没什么好说的 1.3.4. 集成学习方法 Boosting 基学习器，做错的样本受到更多的关注，然后调整后的样本分布进行下一个基学习器的训练（AdaBoost） 对特定的数据分布进行学习，实际上就像是re-weighting的这样的操作；如果不能使用这种方法的模型我们就使用re-sampling的操作来处理。 过程中如果不满足比随机猜测好，这种模型就直接停止 可以看出这种方法主要关注降低偏差，可以对弱学习器构建出很强的集成 Bagging与随机森林 又放回的随机抽取样本构建包含m个的数据集，采样出T个这样的数据集，然后对这些基学习器进行结合，（简单投票做分类，简单平均做回归） 随机森林是在以决策树为基础构建的基学习器，在bagging的基础上，引入了随机的属性选择（原本是选择最优的），也就是先选择一个随机的子集，然后在子集中选择最优的用来划分。 GBDT和XGBoost GBDT实际上就是BOOSTing的一种方法 都是通过损失函数相对于模型的负梯度方向来对当前的模型进行更新，但是在梯度下降里，我们的模型是通过参数表示的，而在梯度提升，是在函数空间中直接表示的。 通俗一点的说的话，实际上就是梯度下降是在更新梯度来进行梯度下降，梯度提升通过累加弱学习器来梯度下降。 关键：实际上在梯度提升中，梯度最终会被计算为（当作）残差，也就是用损失函数的负梯度去模拟残差 参考链接，里面也有提到区别；简单的实例分析 GBDT：Gradient Boosting Descend Tree GBDT分类和回归时的基学习器都是CART分类回归LINK树，每轮迭代在上一个基学习器结果的残差至上进行训练，（弱分类器）； 这里的残差可以更改为损失函数，然后最后还是会变成对残差的一个拟合，模型的残差实际上也不是这么好弄的，用损失函数的负梯度，来拟合本轮损失的近似值。然后就是从上到下一个个基学习器过去 不同的损失函数针对的是不同的问题（分类回归） 分类问题：指数损失函数和对数损失函数 回归问题：均方差损失函数，绝对值损失函数 GBDT的正则化： 给每个模型乘上一个弱化系数，降低每个模型对拟合损失的贡献 通过按比例来随机抽取样本训练模型，bagging，减少方差但是会增加偏差，可以使用交叉验证 控制CART的复杂度，可以使用剪枝正则化 优缺点： 灵活，准确率高，使用健壮的损失函数可以处理异常值 难以并行化处理，受限于基学习器之间的依赖关系 XGBoost 详细讲解XGBoost； 和残差然后用CART拟合不同，我们是通过SCORE来确定结构，然后通过梯度的值来计算结构中应该有的值， 所以在我们计算完二阶梯度的时候，我们能同步的进行划分和复制，这样，但是还有为为什么要排序后去做，我有点没搞明白 两种方法的区别： 正则化防止过拟合，提升泛化能力 L1+L2 还可以使用线性分类器 使用了二阶导数信息来进行对代价函数的优化 采用和随机森林类似的策略，能对数据进行采样，降低过拟合和减少计算 有缩减项，类似weight decay 能计算出缺失值的分裂方向 可并行计算 划分的方式改成了一个Score（根据两阶段梯度数据） 1.3.5. SVM细节 在SVM推导部分的后面添加最终的形式以及整理一下KKT条件，通过这些特性对一些问题进行分析 为什么将原问题转化成对偶问题 对偶问题将原始问题中的不等式约束转为了对偶问题中的等式约束 方便核函数的引入,输入最终会转化从恒一种内积的形式 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。 怎么转化到多分类的场景 不是逐个二分吗？ hinge损失的多分类形式：https://www.turingtopia.com/article/details/e2492b497a144bf6b3cd1fc62df60bbd Lagrange乘数法，对偶问题 二次型函数 A是实对称矩阵 f(x)=x^TAx 在$R^N$上是凸函数和A是半正定矩阵是充要的关系； 凸规划：目标函数是凸函数，约束空间是凸集 MP是凸规划的条件：满足。。。 为什么凸规划是重要的，因为凸规划的任意局部最优值都是他的整体最优解 凸优化问题Lagrange： 引入松弛变量/kkt乘子，将不等式约束转化为等式约束， 引入拉格朗日乘子将等式约束转为无约束优化 KKT条件：MP（非线性规划）问题，可微可行点$x^*$是整体最有解的条件 实际上是在凸规划的最优值求解过程中，使用拉格朗日乘数法，其中的不等式约束（ 求解Largrange，KT条件的时候我们通常使用互补松紧条件入手来求解（分情况讨论，但是这种时候要考虑分类的完备性来进行求解。） 对偶条件的引出 在线性规划的过程中可以使用对偶问题来进行转化，将求最大转化为求最小值；如果LP问题又最优解，则对偶问题也有最优解，且解相同 SVM的具体推导以及核函数 再生希尔伯特空间， 于是我们可以选择多项式核，高斯核，拉普拉斯核之类的来做这个核函数映射 核函数的记忆 相关的面试问题 SVM和LR的联系与区别： 他们都是分类算法，都是监督学习的模型 都是判别模型，如果不考虑核函数的话，都是线性分类算法 LR采用log损失，SVM采用合页（hinge）损失 LR基于概率理论，使用sigmoid和MLE来估计出参数的值；SVM基于几何的边界最大化原理 LR对异常值敏感，SVM不 对海量数据来说LR效率高，在低纬度的时候LR的准确率高，但是维度上升就被反超 处理线性不可分：LR靠特征构造（组合交叉特征，特征离散化），SVM 还可以核函数 LR是经验风险最小化，SVM自带结构风险最小化（自带了L2正则项所以） 将数据向SVM求得的超平面进行投影后，是否仍然线性可分？（数学推导，这里的推导我放弃） 显然不，从支持向量的角度分析，最优的结论必然是两点的中垂线，那么这种情况本身并不是线性可分的，但是如果超平面不是这个中出现的话，那么就不满足SVM求解条件中的最优分界面了。 70页开始，但是我决定先推导SVM，这一部分的内容再说吧 是否一定存在一组参数使得SVM的训练误差为0？ 训练误差为0的SVM分类器一定存在吗？ 加入松弛变量的SVM训练误差可以为0吗？ 1.3.6. 决策树 3种分支计算的方法：信息增益，增益率，gini指数 预剪枝，后剪枝 是否会重复利用连续值或者离散值特征来分树？ 离散特征不行，比如用西瓜的纹理来说：就是有没有了 连续特征可以，阈值切割，我们可以不断的往下细分，比如 1.3.7. PCA与LDA 优化的目标：最大化投影方差， 在主轴上的投影方差最大，包含更多的信息量（信噪比的概念）。 通过这个思想去设计一个求解过程：中心化（为了使得投影后均值为0），然后求方差，然后推导出最大化问题，然后通过largrange乘数法，推出实际上就是特征值。 LDA投影到的是便于分类的，PCA是方差最大信息量最大的去除冗余的信息维度；无监督有监督 1.3.8. Boosting与Bagging bagging 解决的是过拟合，boosting解决的是欠拟合的方法 这里需要重新再去温习一下基本的定义 1.3.9. MAP、MLE、Bayesian https://blog.csdn.net/u011508640/article/details/72815981/ https://zhuanlan.zhihu.com/p/61593112 从数学意义上和模型上区分 显然基于bayesian公式我们可以区分后验和先验，以及使用贝叶斯公式去获得估计的基本依据是啥。（要看清楚似然的主体是啥，也就是求解的参数是那个） P(\\theta | D)(后验概率 ) = \\frac{P(D|\\theta)(似然函数)P(\\theta)(先验)}{\\sum P(D|\\theta)P(\\theta)} 这里的P实际上可以看成一个概率分布模型，一个推断模型（其中的D和θ一个是模型的数据一个是模型的参数） 前两者将 那么MLE（极大似然）：（频率学派，假设为定制） 我们把模型参数设置成θ（变量）然后，我们计算当θ等于多少的时候出现D这个数据的概率最大，（这样的话也就会引出我们对大量数据的需求，实际上是一个概率估计模型） 而MAP（最大后验概率）（和&#x1F447;一样是贝叶斯学派，θ是随机数） 是贝叶斯估计的一种近似 也就是我们考虑了参数发生或者出现的先验概率以后再进行计算，由于上式的分母P(D)是个确定的值，所以不影响最大化的过程，我们通常再计算的时候将其忽略，然后最大化分子，就是MAP了，在这里这一步的prior是非常重要的，和我们之后的模型估计息息相关。 最后Bayesian估计 其中MAP估计的是后验的最大值，而贝叶斯估计是去近似这个后验函数，基于贝叶斯公式去做这个估计。（去看看数据挖掘中的这部分的题目来帮助理解） 朴素贝叶斯问题 看看数据挖掘中的那个讲稿，就很清楚，实际上就是类关系条件独立假设； 可以使用laplace校准来避免0概率对决策造成影响 那什么是朴素贝叶斯学习呢？ 1.3.10. 交叉熵 KL散度 信息熵 https://blog.csdn.net/b1055077005/article/details/100152102 1.3.11. 无监督方法 K均值聚类的有优缺点： 受离群值影响，通常是局部最优解，类别量级和密度的问题没法解决 需要人工确定k 样本只能被划分到单一的类中 如何调优： 数据归一化，离群点处理 合理选择K值 核函数 证明K means 收敛：和EM算法实际上是一个框架，这里看一下关系 GMM：高斯混合模型 假定不同簇种的样本各自符合不同的高斯分布，这种得到的聚类算法就是高斯混合模型； P(x) = \\sum_{i=1}{K}\\pi_iN(x|\\mu_i,) 核心思想是：每个单独的分模型都是标准高斯模型，我们需要估计高斯分布的双参数，还有一个额外参数（权重或者生成数据的概率），实际上和K均值聚类是一样的操作过程，这里要记得EM。 使用极大似然（很难求解）去寻找均值方差和权重，所以最后使用EM去做 随机初始化参数， E：根据当前参数，计算每个点由某个分模型生成的概率 M：根据概率，来改进每个分模型的均值方差和权重 SOM：自组织映射神经网络 nah 聚类算法的评估： 轮廓系数；霍普金斯统计量；R方 1.3.12. 数据清洗 没用到就不说了，在数据挖掘中主要是：缺失数据，错误数据，和噪声数据 错误数据：分析更改删除和忽略 缺失数据：忽略，手工，填充（全局常量，属性或者中位数，基于贝叶斯等等方法（这个不提）） 噪声数据：分箱（均值平滑，中值平滑，边界平滑），聚类，回归，人工检查 1.3.13. 图像增强策略 torchvision.transformers的库里面有很多，还有unbalance_image中有一些经典的unbalance_image的一些策略 还有PIL中的Image.Enhance(这里可能要注意图像维度的转换) 1、对比度：白色画面(最亮时)下的亮度除以黑色画面(最暗时)下的亮度； 2、色彩饱和度：：彩度除以明度，指色彩的鲜艳程度，也称色彩的纯度； 3、色调：向负方向调节会显现红色，正方向调节则增加黄色。适合对肤色对象进行微调； 4、锐度：是反映图像平面清晰度和图像边缘锐利程度的一个指标。 MixUp的操作实际上就不要在赘述了，而Sharpen的操作实际上就是对分子分母都做一个1/T的乘方的这样一个锐化的操作，突出显著的样例，这样能够使得：？（需要加强记忆） 1.3.14. 评估模块 ROC、PR、F1 PR曲线就比较清楚是根据Precision和recall区划分的，然后根据判定为正负样本的阈值去区分这个曲线的情况。 ROC曲线是根据 TPR和FPR真阳性率（正样本中有多少被判定为真）和伪阳性率（负样本中有多少被判定为真），依据score的阈值来绘制曲线 AUC越大，说明分类器越可能把真正的正样本放在前面 还有F1，是综合反应一个排序模型的性能(调和平均值)： F1 = \\frac {2*precision * recall}{precision + recall} ROC比起PR来说，对于正负样本数据量的分布比较不敏感，所以在这种数据不均衡的大数据场景，ROC更合适，更广泛。如果是针对特定数据集上的表现的话实际上PR曲线的话能更直观的反应性能 RMSE 离群点可能导致效果一直很差，可以用归一化的百分比来看 余弦相似度 实际上就是将问题从距离转换到了角度上，用1-cos(A,B)表示余弦距离，实际上这种距离比起欧氏距离来说，能够适应更高的维度，比较相对差异的时候我们可以考虑用余弦相似度来衡量 不满足三角不等式：等腰直角三角形 在机器学习领域，还有kl距离，相对熵，也能衡量两个分布之间的距离，但是也不满足对称性和三角不等式 1.3.15. 训练集和测试集的划分机制 K次交叉验证法； 首先描述一下什么是k次交叉验证，k次交叉验证的作用是：评估模型的预测性能，训练好的模型再新数据上的表现，在一定程度上减少过拟合，从有限数据中获取尽可能多的有效信息，使得模型对于数据的划分不那么敏感 Handout检验：实际上就是7：3的随机划分的方式；其中还有留一划分的策略； 自助法（bootstrap）：有放回的随机抽样，总数为n的集合，抽样n次作为训练集，剩下的没有被抽样出来的数据作为测试集，这就是自助法的验证过程 当抽样次数趋向于无穷的时候，有多少数据没有被选择过，$(1-\\frac{1}{n})^n$ 取极限，根据重要极限$(1+\\frac{1}{n})^n$ 的极限是e，可以推得大概是1/e的概率没被选中，也就是大概百分之36%， （59页） 1.4. 语言部分 1.4.1. Python部分 深拷贝浅拷贝 概念上是一致的，但是具体实现深拷贝上，是不一样的，python 应该是自带的.copy函数 https://www.jianshu.com/p/a71c09798123) 修饰符的作用 修饰符的作用python函数修饰符@的作用是为现有函数增加额外的功能，常用于插入日志、性能测试、事务处理等等。 实际上就是讲函数作为作为输入参数，然后对函数进行包装，在执行函数之前或者之后增加一些操作，通常用来指示函数执行进程，也可以用来添加数据预处理等等 LINK REFERENCE def log(func): def wrapper(): print('log开始 ...') func() print('log结束 ...') return wrapper @log def test(): print('test ..') test() 多线程处理 一些基本的概念： 每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 每个线程都有他自己的一组CPU寄存器，称为线程的上下文，该上下文反映了线程上次运行该线程的CPU寄存器的状态。 指令指针和堆栈指针寄存器是线程上下文中两个最重要的寄存器，线程总是在进程得到上下文中运行的，这些地址都用于标志拥有线程的进程地址空间中的内存。 线程可以被抢占（中断）。 在其他线程正在运行时，线程可以暂时搁置（也称为睡眠） -- 这就是线程的退让。 使用threading作为线程处理模块： run start join([timr])：等待至线程终止；isAlive() getName() setName() append():添加到线程列表，使用for i in threads: i.join() 来等待所有线程完成。 首先通过继承threading.thread创建子类，实现run操作； 实例化之后使用start启动进程； 线程同步（Lock） Lock和Rlock 都有相应acquire和release， 防止多个线程同时对某个数据进行修改，就需要对多个线程进行同步； Python 中多线程和多进程的区别 这一点还是需要补充，好像是使用什么来着 主要从进程和线程本身的区别来谈，然后Python中的多线程是收到了Cpython的GIL约束的，所以稍微有点鸡肋，可能还是要用多进程去了解 [参考链接]( 文件IO部分 文件处理： 读取键盘输入：a = input('请输入：')； 打开和关闭文件：open()，一般需要按照指定的格式打开一个文件才能对其进行修改或者读取。 fo = open(\"foo.ext\",\"w\") fo.close() # 相应的文件的读写实际上应该是 fo.write(\"dadsad\") res = fo.read(10) # 使用tell 和seek输出和定位当前的文件位置， # 实际上经常和with命令一起使用 with open(path,\"w\",encoding='utf-8') as f : # record the dir name (depth=1) f.writelines(authorres) # record all the video for element in result: f.write(element) 补充说明： 这里with语句的解析 OS命令 chmod，chdir，chown，mkdir，path（一系列常用操作，exist之类的），getcwd（返回当前工作区），listdir（返回文件夹包含的文件或者文件夹的列表） 1.4.2. C++部分 实际上c++对于编写的顺序是有上下关系的，如果我们定义的时候遇到了一些上面和下面的差别的时候，我们可以考虑在上面先进行declaration，在后面在具体的进行definition。 文件操作： 基本读写 写文件步骤如下： 包含头文件 #include 创建流对象 ofstream ofs; 打开文件ofs.open(\"文件路径\",打开方式); 写数据ofs ; 关闭文件ofs.close(); 读文件的操作步骤如下 包含头文件 #include 创建流对象 ifstream ifs; 打开文件并判断文件是否打开成功 ifs.open(\"文件路径\",打开方式); 读数据 四种方式读取 关闭文件 ifs.close(); 处理二进制文件 二进制方式写文件主要利用流对象调用成员函数write 函数原型 ：ostream& write(const char * buffer,int len); 参数解释：字符指针buffer指向内存中一段存储空间。len是读写的字节数 打开方式要指定为 ios::binary； 二进制方式读文件主要利用流对象调用成员函数read 函数原型：istream& read(char *buffer,int len); 参数解释：字符指针buffer指向内存中一段存储空间。len是读写的字节数 预编译头 首先介绍的是如何防止在重复的include\\的时候,不会导致重复的定义和include的方式，以及其中的区别 #pragma once：自定义包含了这种情况的时候，依赖于不会将同一个文件多次编译，这个方式没办法保证内容相同的不同名文件被重复的编译，针对的是文件。 2. ifndef：依赖于宏名称不能冲突，这个能保证内容相同的不同文件不小心被同时包含，实际上是一个预编译的条件语句 首先介绍一下写法： 基本的逻辑也就是当我们第一次执行的时候，就会预先定义到这一块，这样到时候就不会导致相应部分的代码被重复的执行或者定义 #ifndef _code_block #define _code_block // code here #endif 在自己编写的时候需要注意不能重复使用宏名（_code_block），不然可能会出现以外的其他地方的代码没有被执行。 多线程 mutex互斥锁 内存分区模型 主要将内存分为四个区域: 代码区：存放函数的二进制代码，由操作系统进行管理 程序运行前进行分配 存放CPU执行的机器指令，具体而言代码区是共享的和只读的 全局区：存放全局变量、静态变量和常量 这部分空间应该是在预编译的时候事先分配的 该部分的数据再程序结束后由操作系统释放 栈区：由编译器制动分配和释放，存放函数的参数值，局部变量等 堆区：由程序员分配和释放，如果程序员不释放，程序结束后由操作系统回收； （类内的存储特点）： 在c++中，类内的成员变量和成员函数分开存储，只有非静态的成员变量才属于类的对象上的存储。（函数也是不占对象空间的，所有的函数共享一个函数实例） （static）静态成员函数在编译阶段分配内存，类内声明类外初始化，所有的对象共享同一份数据； 类： THIS指针 指向的是，当前调用的这个 对象； 友元friend： 通过friend关键词告诉编译器其他的可以访问类中私有内容的东西； 友元可以是： 函数 类 其他类别的成员函数 继承 构造函数顺序 继承的时候首先调用父类的构造函数，在构造子类的构造函数，析构是反向的。 几种继承方式 注意区分继承方式面临的主体，也就是说： 派生类能访问的元素实际上都是除了私有类都能访问 区别在于我们是否能通过派生类的实例对基类的变量进行访问，这里的继承方式就代表着这些变量在被继承之后的状态，是私有的保护的，还是公共的 父子的成员同名 无论是不是静态，都是： 访问子类同名成员 直接访问即可 访问父类同名成员 需要加作用域 多态 多态是C++面向对象三大特性之一 多态分为两类 静态多态: 函数重载 和 运算符重载属于静态多态，复用函数名 动态多态: 派生类和虚函数实现运行时多态 静态多态和动态多态区别： 静态多态的函数地址早绑定 - 编译阶段确定函数地址 动态多态的函数地址晚绑定 - 运行阶段确定函数地址 多态使用时，如果子类中有属性开辟到堆区，那么父类指针在释放时无法调用到子类的析构代码 解决方式：将父类中的析构函数改为虚析构或者纯虚析构 ​ 1. 虚析构或纯虚析构就是用来解决通过父类指针释放子类对象 ​ 2. 如果子类中没有堆区数据，可以不写为虚析构或纯虚析构 3. 拥有纯虚析构函数的类也属于抽象类 常用STL函数 for_each：遍历容器 transform：搬运容器中的数据到另一个容器中 accumulate：计算容器的元素综合 fill：向容器中添加元素 replace_if find_if set_union：求两个容器的并集 set_intersection：求两个容器的交集 通用部分： 深拷贝和浅拷贝的概念： 浅拷贝：简单的赋值拷贝操作 深拷贝：在堆区重新申请空间，进行拷贝操作 同步与异步 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-11-28 23:08:48 "},"JobHunting/IntView_刷题笔记.html":{"url":"JobHunting/IntView_刷题笔记.html","title":"3. 个人刷题完整笔记","keywords":"","body":"1. LeetCode 刷题笔记1.1. 《Fuck Algorithm》1.1.1. 数据结构的存储方式1.1.2. 二分查找专题1.1.3. 数据结构的基本操作1.1.4. 更好的理解数据类型的作用1.1.5. 链表刷题1.1.6. 二叉树刷题1.1.7. 二叉搜索树1.1.8. 动态规划方法1.1.9. 贪心算法：动态规划的特例1.1.10. KMP算法：动态规划下属1.1.11. 回溯算法详解1.1.12. BFS算法详解1.1.13. 双指针使用技巧总结1.1.14. 滑动窗口算法1.1.15. 分治算法详解1.1.16. 区间问题1.1.17. 排序算法1.1.18. 题型：数组1.1.19. Two Sum 到N Sum问题1.1.20. Union-Find并查算法1.1.21. 从LRU到LFU1.1.22. 一些其他的算法技巧1.2. 《剑指offer》1.2.1. 基本知识点1.2.2. 数据结构1.2.3. 算法和数据操作1.2.4. 高质量的代码1.2.5. 解决面试题的思路1.3. 《LeetCode》1.3.1. 经典类型题（后续归纳）1.4. 顺带GIT知识扩充（后续迁移）1.4.1. 基本的workflow1.4.2. 使用的一些KeyPoint1. LeetCode 刷题笔记 @Aiken 2021; 汇总LeetCode刷题以及刷《剑指offer》过程中遇到的一些不会做的题或者启发性很强的题目等等；内容主要以以下几个方面为主： 题目-题解-相关注释； 相关难点分析； 相关知识点索引 同时copy到数据结构或者c++的文档中） 1.1. 《Fuck Algorithm》 针对各个专题指向性的去刷一些Leetcode中的题目，通过对这些题目进行分析整合来对巩固各个知识点，这一部分的代码整合到/leecode文件夹中，但是主要可能整合在md中； 这里可以顺便把git的内容整理一下，本地的git操作流程 最近先把数据结构刷了，变刷变看后面的搜索等等的内容，一部分一部分的往后看 第一课中回溯和其他规划的题还没看，后续再看看 思考C++中多返回值的设计 1.1.1. 数据结构的存储方式 数据结构的存储方式（物理层面的存储方式）：数组（顺序存储）和链表（链式存储）。 最底层的存储架构上基本上只有这两种实现的方式，更高维的才是：栈、队列、堆、树、图这些高层结构； 而这些实现的高层实现上，分别使用量中架构有啥优缺点： 综上，数据结构种类很多，甚至你也可以发明自己的数据结构，但是底层存储无非数组或者链表，二者的优缺点如下： 数组由于是紧凑连续存储,可以随机访问，通过索引快速找到对应元素，而且相对节约存储空间。但正因为连续存储，内存空间必须一次性分配够，所以说数组如果要扩容，需要重新分配一块更大的空间，再把数据全部复制过去，时间复杂度 O(N)；而且你如果想在数组中间进行插入和删除，每次必须搬移后面的所有数据以保持连续，时间复杂度 O(N)。 链表因为元素不连续，而是靠指针指向下一个元素的位置，所以不存在数组的扩容问题；如果知道某一元素的前驱和后驱，操作指针即可删除该元素或者插入新元素，时间复杂度 O(1)。但是正因为存储空间不连续，你无法根据一个索引算出对应元素的地址，所以不能随机访问；而且由于每个元素必须存储指向前后元素位置的指针，会消耗相对更多的储存空间。 1.1.2. 二分查找专题 由于我经常写错二分查找的边界判断条件，所以这里进行一个整理操作： 二分查找总结专题 后续整理的时候在进行阅读一下，加深一下理解 其中需要注意的是： 我们使用 left+(right-left) /2 来代替 (l+r)/2 ,因为这样的话可以防止right和left太大溢出的操作； mid +- 1 以及最终的返回条件 我们分情况来讨论： 求的是特定值，求的是左右的边界值的时候， int binary_search(int[] nums, int target) { int left = 0, right = nums.length - 1; while(left target) { right = mid - 1; } else if(nums[mid] == target) { // 直接返回 return mid; } } // 直接返回 return -1; } int left_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left target) { right = mid - 1; } else if (nums[mid] == target) { // 别返回，锁定左侧边界 right = mid - 1; } } // 最后要检查 left 越界的情况 if (left >= nums.length || nums[left] != target) return -1; return left; } int right_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left target) { right = mid - 1; } else if (nums[mid] == target) { // 别返回，锁定右侧边界 left = mid + 1; } } // 最后要检查 right 越界的情况 if (right 1.1.3. 数据结构的基本操作 所有数据结构的基本操作一般都局限在 遍历+访问，更具体一点就是：增删改查； 数据结构存在的目的就在于尽可能快的增删改查： 遍历的基本操作一般来说也就两种形式：线性和非线性情况，基本的遍历框架可以总结为 线性遍历： 线性就是 for/while 迭代为代表，经典的就是数组遍历框架； void traverse(cosnt int[]& arr){ for(int i=0;i 非线性遍历： 链表遍历框架，兼具迭代和递归框架； /* 基本的单链表节点 */ class ListNode { int val; ListNode next; } void traverse(ListNode head) { for (ListNode p = head; p != null; p = p.next) { // 迭代访问 p.val：迭代需要写出我们每个的具体操作 } } // 两种不同的遍历写法&#x1F446; &#x1F447;，具体思路上的区别 void traverse(ListNode head) { // 递归访问 head.val // 递归是基于我们的n-1假设，只需要实现n-1 到n的转变就行 traverse(head.next); } 二叉树的情况的话，实际上就是链表的递归情况，然后要针对两侧进行递归就可以了，在多几个分支也是一样的；而也可以拓展成图的遍历，针对图可能出现环的情况就使用flag标记一下就可以了。 /* 基本的二叉树节点 */ class TreeNode { int val; TreeNode left, right; } void traverse(TreeNode root) { // oprtator 前序遍历 traverse(root.left); // oprtator 中序遍历 traverse(root.right); // oprtator 后序遍历 } 1.1.4. 更好的理解数据类型的作用 设计twitter 335 从题目需求出发，更好的理解各种数据结构的使用情景： 不需要时序，需要快速搜索的关注列表：Hashset，set，... 需要发表的时序，同时需要多个用户推文发表的时间顺序，也涉及到顺序的整合：有序链表 同时考虑一个全局的时间戳来进行比对。（合并k个有序链表） 合并k个有序链表：借助优先级队列，设定好优先级队列的优先关系（timestamp），它能够实现自动排序，然后我们讲k个链表插进去，就行。 面向对象设计，针对每个对象的需求来定制需要的数据类型和方法；当然也要考虑基类和子类之间的关系，还有private 和 static的关系。 具体的代码实现后面还是要看一下的，这种比较复杂的类型设计题目。 单调栈模板 实际上就是栈，利用了一些巧妙的规则，使得新元素入栈后，栈内的元素都保持有序（单增或者单减）。Purpose：如何设计这样一个数据结构，同时如何利用这样的数据结构来解题。 496 下一个更大的元素 反向写更好，不要执着了，学起来就完事了 这题我对题意的理解是错误的，下一个更大的元素，不是按照大小排列的，而是按照原本在数组中的顺序排列的，所以我们实际上可以用一个hash映射来做这样的题目，官方题解中队单调栈的讲解更清晰一点， 这个是网址中写的，这样的方法是倒着完成的，基本的概念是差不多的，也就是遍历的顺序和判定的条件稍微变换了一下： vector nextGreaterElement(vector& nums) { vector res(nums.size()); // 存放答案的数组 stack s; // 倒着往栈里放 for (int i = nums.size() - 1; i >= 0; i--) { // 判定个子高矮 while (!s.empty() && s.top() 下面这个是我写的，我是正向执行的。 class Solution { public: vector nextGreaterElement(vector& nums1, vector& nums2) { int n = nums2.size(); unordered_maporderh; // 这个没有长度初始化这种说法的 stack temps; vector res; // 只初始化长度的话，会初始化为0； temps.push(nums2[0]); // 当遇到比原本的大的时候，我们就直接弹出，直到里面的都比他大 for(int i =1; itemps.top()){ orderh[temps.top()] = nums2[i]; temps.pop(); } temps.push(nums2[i]); } // 对于剩下来的哪些元素，就赋值为-1 while(!temps.empty()){ orderh[temps.top()] = -1; temps.pop(); } // 添加进最终的结果。 for(int num: nums1){ res.push_back(orderh[num]); } return res; } }; 问题变形，1118等待多少天 这一题，求间隔，我们就讲放入stack的值变成相应的index，然后根据index去索引值来对比，然后通过，相似的操作去求解，但是我们当然也可以反向的进行操作，因为我们现在的num1和num2是相等的，我们就没必要建立hash去索引求解，只需要直接输出答案即可。 可以像上一题我的写法一样，只需要修改存入stack的值就可以； 也可以反向进行，如下： vector dailyTemperatures(vector& T) { vector res(T.size()); // 这里放元素索引，而不是元素 stack s; /* 单调栈模板 */ for (int i = T.size() - 1; i >= 0; i--) { while (!s.empty() && T[s.top()] 反正基本思想都是让stack里存放的值从大到小，如果违反了就pop到符合位置。 503 下一个更大的元素2 如何处理环形数组：也就是他能绕一圈的，进行操作的。 使用取余来得到相应的环形特性，但是我们其中已经存在的答案怎么fix呢？ 也可以使用双倍长度的解法,构建或者不构建新数组。 一般的通过取余获得环形特效的代码模板： int[] arr = {1,2,3,4,5}; int n = arr.length, index = 0; while (true) { print(arr[index % n]); index++; } 具体实现：通过取余来模拟双倍长度，但是这样的作法，实际上还是进行了重复的计算吧？正向的写也没问题 class Solution { public: vector nextGreaterElements(vector& nums) { int n = nums.size(); vector res(n); stack s; // 假装这个数组长度翻倍了 for (int i = 2 * n - 1; i >= 0; i--) { // 索引要求模，其他的和模板一样 while (!s.empty() && s.top() 删除数组中的重复元素316 1081 这一题实际上还是用单调栈的思路，让里面的顺序尽量是按照从小到大排，就是增加了约束，也就是： 里面已经有的我们就直接过； 后面没有再出现的情况我们也直接过； 如果是比里面的大就直接加进去，如果是比里面的小，我们就pop到直接过的时候再加 需要两个辅助的存放判断的辅助情况 class Solution { public: string removeDuplicateLetters(string s) { if(s.empty()) return {}; // 初始化需要的存储数据结构 vector countAl(256,0); vector countSt(256,false); stack store; // 初始化count数组 for(auto t: s){ countAl[t]++; } for(char c: s){ countAl[c]--; if(countSt[c]) continue; while(!store.empty() && store.top()>c){ // 如果top后面没有了 if(countAl[store.top()]==0) break; // 如果还有就pop countSt[store.top()] = false; store.pop(); } store.push(c); countSt[c] = true; } // 对stack中的字符进行反转然后输出 string res; stack temp; while(!store.empty()){ temp.push(store.top()); store.pop(); } while(!temp.empty()){ res.push_back(temp.top()); temp.pop(); } return res; } }; 单调队列 存进index，然后根据index取值来做判断 实际上就是和上面一样的思路，剑指offer的队列中的最大值，用一个deque双向队列实现，刚好是剑指offer59题。维护一个头部是最大值的队列，后续的加入的时候，将前边比他小的都pop出去，再push，然后每次移动要把头pop出去。 FA讲解的，结合offer理解更妙 二叉堆实现优先队列 实际上就是用数组维护的一个类似的二叉树，然后要保有最大堆或者最小堆的性质，数组的子节点可以很容易的通过*2来获取： 然后为了维护最大堆或者最小堆的操作，我们需要有一个上浮``下沉两个操作来维护最大堆的性质，实际上也比较简单。就是 上浮：当父节点小于当前节点的时候就不断向上换 下沉：下面更大的哪个和父节点换。 通过这两个操作来实现删除和添加的维护： insert：添加到底部不断上浮即可 delete：将堆顶元素和堆底元素互换，（1，N）然后将堆顶的元素不断的下沉到正确的地方即可 这些操作都是二分的时间复杂度。 hash和数组实现O(1)插入删除和随机数 通常理解的情况下我们需要依靠hash来实现O(1)的搜索和插入删除，但是，这样的话，我们没法等概率的取出随机数，我们认为需要借助index，产生一个随机的数字来索引，但是这样，我们就需要借助vector，那如何通过底层的vector来进行删除？ 使用hash来存储index，然后通过swap和pop来O(1)的删除，然后调用rand()和%来产生随机数即可。 class RandomizedSet { public: // 存储元素的值 vector nums; // 记录每个元素对应在 nums 中的索引 unordered_map valToIndex; bool insert(int val) { // 若 val 已存在，不用再插入 if (valToIndex.count(val)) { return false; } // 若 val 不存在，插入到 nums 尾部， // 并记录 val 对应的索引值 valToIndex[val] = nums.size(); nums.push_back(val); return true; } bool remove(int val) { // 若 val 不存在，不用再删除 if (!valToIndex.count(val)) { return false; } // 先拿到 val 的索引 int index = valToIndex[val]; // 将最后一个元素对应的索引修改为 index valToIndex[nums.back()] = index; // 交换 val 和最后一个元素 swap(nums[index], nums.back()); // 在数组中删除元素 val nums.pop_back(); // 删除元素 val 对应的索引 valToIndex.erase(val); return true; } int getRandom() { // 随机获取 nums 中的一个元素 return nums[rand() % nums.size()]; } }; 进阶问题 排除黑名单数字来产生随机数，这样我们只需要将黑名单里的数字移动到数组的末尾再产生随机数就可以了，但是有两个需要注意的地方： 黑名单里的数字本来就在末尾 交换的时候黑名单的数字和黑名单里的数字交换了，（实际上他通过限定次数的交换是没有问题的，按顺序还过去就好） 跳过尾部的黑名单缩影的问题 int last = N - 1; for (int b : blacklist) { // 如果 b 已经在区间 [sz, N) // 可以直接忽略 if (b >= sz) { continue; } while (mapping.count(last)) { last--; } mapping[b] = last; last--; } 1.1.5. 链表刷题 主要还是和二叉树一样，熟悉一个递归实现的问题； 一些总结：双边约束的情况下好像使用迭代写起来比递归好写多了； 反转链表（206）： :stadium:迭代的分析思路：基于n-1的假设，我们可以将n-1已完成的情况，当前在n的情况画出来，或者想象出来来分析怎么解题。 注意对head非空的判断要在head next的前面 不要临时临时变量，先把题做出来在做简化，（双指针指示法，一个指向前一个一个指向当前一个） 进阶一点的问题： 递归反转链表的一部分（92） 铺垫任务：反转链表的前N个节点 具体的区别： 1 base case 变为 n == 1，反转一个元素，就是它本身，同时要记录后驱节点。 刚才我们直接把 head.next 设置为 null，因为整个链表反转后原来的 head 变成了整个链表的最后一个节点。但现在 head 节点在递归反转之后不一定是最后一个节点了，所以要记录后驱 successor（第 n + 1 个节点），反转之后将 head 连接上 注意这里tail的设置，理解透 ListNode successor = null; // 后驱节点 // 反转以 head 为起点的 n 个节点，返回新的头结点 ListNode reverseN(ListNode head, int n) { if (n == 1) { // 记录第 n + 1 个节点 successor = head.next; return head; // 这里也很重要！只有一个的时候return啥。 } // 以 head.next 为起点，需要反转前 n - 1 个节点 ListNode last = reverseN(head.next, n - 1); head.next.next = head; // 让反转之后的 head 节点和后面的节点连起来 head.next = successor; return last; } 最终实现部分： :question: 但是这样的方法最终的实现效率并不高有时间的话可以去看看题解中的其他的迭代思想方式；时不时回来刷一刷这两三道题，来加强一下对于递归思想的理解。 */ class Solution { private: ListNode* last = nullptr; public: ListNode* reverseBetween(ListNode* head, int m, int n) { if (m == 1) { return reverseN(head, n); } head->next = reverseBetween(head->next, m - 1, n - 1); // 这里，return head和递归之间的关系要掌握好，从变换的阈值开始分析，比较传入值和return值就知道了 return head; } ListNode* reverseN(ListNode* head, int n) { if (n == 1) { last = head->next; return head; } ListNode* tail = reverseN(head->next, n - 1); head->next->next = head; head->next = last; return tail; } }; 如何k个一组反转链表（25） 使用迭代+递归的方式编写，迭代进行反转，递归进行组合排序（外层架构）,这种双边约束的好像使用迭代的方式比递归更好写一些，而且这样的时间复杂度好像甚至更低把。和上面的对比一下就知道了。 class Solution { public: ListNode* reverseKGroup(ListNode* head, int k) { ListNode* start, *end; start = end = head; for (int i = 0; inext; } ListNode* newH = reversek(start,end); start->next = reverseKGroup(end,k); return newH; } ListNode* reversek(ListNode* start, ListNode* end) { ListNode* curr = start; ListNode* prev = nullptr; //最后的赋值再下一行，哪个start.net while(curr!=end) { ListNode* temp = curr; curr = curr->next; temp->next= prev; // curr.next= temp; 多做了一部，我们只做到当前curr指向的点就行 prev = temp; } return prev; } }; 判断回文链表（234） 判断是不是回文数的话，首先需要考虑两种基本情况： 数值的回文数考虑奇数偶数长度的问题（中心节点不统一）； string类型的回文数的情况，考虑的是正着读和反着读是一样的，不考虑中心节点好像；（使用双指针技巧，正向和反向遍历，这实际上也算是一种reverse的问题把） 不考虑中心节点实际上是从两侧同时逼近的话，只要在left中心逼近的思想应该还更好一点 寻找回文数的基本中心思想是：从中心向两端拓展（反过来就是递归思想） 而对于链表问题：链表是一个单向索引的数据结构，这种情况下怎么使用双指针的办法？ 遍历的同时存储一个反转副本，然后判断两个链表是否相同？ 使用二叉树的后序遍历的思想，也能倒序的遍历链表，来进行回文数判断 实际上就是基本的递归思想把；同时我们知道树结构其实也就是依托于LISTNode的高层实现，每个树的节点都是链表的节点来着。但是这样的方法目前来看算法的效率不是特别的高。 时间和空间复杂度都是O（n） 后续需要对这个方法进行优化 使用后续遍历的迭代思想进行的例子; 实现上的核心问题：我们虽然可以通过后序遍历来首先取到链表的tail; 但是我们如何让最底层的取到的也是最前面的head呢？（需要另一一个共有head）; class Solution { public: ListNode* head; bool isPalindrome(ListNode* head) { this->head = head; return reverseJ(head); } bool reverseJ(ListNode* tail){ // 我们虽然可以通过后序遍历来首先取到链表的tail // 但是我们如何让最底层的取到的也是最前面的head呢？（需要另一一个共有head） if (tail == nullptr) return true; bool ans = reverseJ(tail->next); ans = ans && (this->head->val == tail->val); this->head = this->head->next; return ans; } }; 如何优化上面的这个算法，减少这个不必要的入栈的空间复杂度，这就涉及到了如何用两个指针来模拟反向遍历的问题：这是一个特别的算法，我们放到下面一个小专题中来详细讲： 快慢指针技巧优化空间复杂度 双指针技巧1：快慢指针找到链表的中点，原理如图所示， 这个方法的时间复杂度是O（n），空间复杂度是O（1）； Keypoint->找到链表的中点。 ListNode* slow, *fast; slow = fast = head; while (fast != nullptr && fast->next != nullptr) { slow = slow->next; fast = fast->next->next; } if (fast != nullptr) slow = slow->next; // slow 指针现在指向链表中点 如果fast不是nullptr，说明链表的长度为奇数，slow还需要往后进行一步，现在是卡在中间的地方 if(fast != nullptr) slow = slow->next; 接下来就不需要多说了把，直接递归进行后续的链表反转，然后正向运行JUDGE就可以了 class Solution { public: ListNode* reverseList(ListNode* head) { ListNode* prev = nullptr; ListNode* curr = head; while (curr) { ListNode* temp = curr; curr = curr->next; temp->next = prev; prev = temp; } return prev; } }; JUDGE: ListNode* left = head; ListNode* right = reverse(slow); while (right != nullptr) { if (left->val != right->val) return false; left = left->next; right = right->next; } return true; 1.1.6. 二叉树刷题 很多二叉树的问题实际上就是上述总结的二叉树遍历的问题，可以套用以上的框架解决。 而且二叉树实际上和很多重要的算法都有关系：比如说快速排序就是二叉树的前序遍历；归并排序就是二叉树的后续遍历。 二叉树中的最大路径和（142） 路径 被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。该路径 至少包含一个 节点，且不一定经过根节点。 路径和 是路径中各节点值的总和。 给你一个二叉树的根节点 root ，返回其 最大路径和 。 解题的思路： 注意区分return和最终结果值的区别 由于考虑到节点为negative的情况，这种情况下就需要设定两个值： 一个是经过当前节点的话，最多能得到多少（动态规划，从后往前） 用max来考虑当前节点接不接如（用0来代替） 另一个是最终的最大值 对于每个节点 return：这个节点，后面能取得得最大值； ans：结合这个节点的左右child，能取得的最大值？，为什么一定要加入当前节点的值？因为不加入当前节点的值的话，就是看哪个子节点最大了（子节点已经考虑了0的情况，也就是用max做了处理。） 这实际上还是递归遍历的框架，最重要的在于问题归纳，怎么对左右节点进行处理和怎么对中间节点进行是否引入的判断； //* Definition for a binary tree node. class Solution { private: int ans = INT_MIN; public: int maxPathSum(TreeNode* root) { helpSum(root); return ans; } int helpSum(TreeNode* root) { if (root == nullptr) return 0; // 考虑negative的情况；用0来判断是否要输入 int rightG = max(helpSum(root->right), 0); int leftG = max(helpSum(root->left), 0); // 不经过该节点的情况已经在子节点的地方输出了，不需要我自己画蛇添足的在这里进行政府的判断； // 从最低层节点开始分析你就知道了，已经包含在ans中了 int temp = root->val + rightG + leftG; ans = max(temp,ans); // 最终都是复数的情况？可能还要考虑temp和val哪个更小的问题 return max(rightG,leftG) + root->val; } }; 基于前序和中序重建二叉树（105） 和剑指的题目冲突了，06，看书即可；后续和前序的应该关系差不多 基于前序找到中间切分点，然后根据中间接分店找到左子树和右子树的数目，从前序和中序中抠出子树来。 class Solution { public: TreeNode* buildTree(vector& preorder, vector& inorder) { // int tempre[preorder.size()]; // 函数，返回一个迭代器 return helpbuild(preorder.begin(),preorder.end(),inorder.begin(),inorder.end()); } // 学，给老子学，怎么在c++中实现动态数组的划分传入，如果是python可太爽了； TreeNode* helpbuild(vector::iterator preb, vector:: prend, \\ vector::iterator inob, vector::iterator inoe) { if(inob == inoe) return nullptr; TreeNode* cur = new TreeNode(*preb); // 记住这个函数find，返回的是一个迭代器，迭代器本身就是一个指针，指针的+1会随着类型的不同而变化 auto root = std::find(inob,inoe,*preb); cur->left = helpbuild(preb+1,preb+(root-inob)+1,inob,root); cur->right = helpbuild(preb+(root-inob)+1,prend,root+1,inoe); return cur; } }; 基于FA中的算法复原一下？等下思想好像是一样的就是一个前序遍历的过程。 基于中序和后序重建二叉树（106） 和上一题基本的实现思想采用了一样的思路，但是这样的方法的空间复杂度好像和网友们查了很多我也不知道具体是为啥，看看fA中间的解法把，以下先post我的思路； class Solution { public: TreeNode* buildTree(vector& inorder, vector& postorder) { return helpbuild(inorder.begin(),inorder.end(),postorder.begin(),postorder.end()); } TreeNode* helpbuild(vector::iterator ins,vector::iterator ine, vector::iterator pos, vector::iterator poe) { if(poe == pos) return nullptr; // 这里要注意的是end是有值的下一项 // iterator 之间的加减和index之间的加减的关系的转化也要清楚到底是怎么回事 poe--; TreeNode* cur = new TreeNode(*poe); auto root = find(ins,ine,*poe); cur->left = helpbuild(ins,root,pos,poe-(ine-root-1)); cur->right = helpbuild(root+1,ine,poe-(ine-root-1),poe); return cur; } }; 下面是FA的实现思路：(need to change cpp version)实际上没什么区别，但是就是在++--这块好像确实直接用下标索引会好一点，在找找把。 TreeNode build(int[] inorder, int inStart, int inEnd, int[] postorder, int postStart, int postEnd) { if (inStart > inEnd) { return null; } // root 节点对应的值就是后序遍历数组的最后一个元素 int rootVal = postorder[postEnd]; // rootVal 在中序遍历数组中的索引 int index = 0; for (int i = inStart; i 翻转二叉树（226） 这题白送的，都不需要再多说什么。 class Solution { public: TreeNode* invertTree(TreeNode* root) { if(root == nullptr) return nullptr; TreeNode* Temp = root->right; root->right = root-> left; root->left = Temp; invertTree(root->right); invertTree(root->left); return root; } }; 填充二叉树节点的右侧指针（116） 这一题也是比较考虑迭代和递归思想的，同时也考研完全二叉树节点的构造特点知识（从左到右建立起来，全满的） 我的解决方法：每次从最左侧节点开始，给下一层赋予连接，然后通过这样的设定迭代的完成这样的任务。 需要注意的地方：几个设置为空的判断，包括对于left为空的判断是需要的。 class Solution { public: Node* connect(Node* root) { if(!root) return nullptr; else { Node* temptr = root; while(root != nullptr){ // 这一步特别重要也容易忽略 if(!root->left) return temptr; root->left->next = root->right; root->right->next = root->next?root->next->left:nullptr; root = root->next; } connect(temptr->left); return temptr; } } }; FA写的方法：没有我写的快，但是其实更好理解一点，他是通过辅助函数把传入的两个节点串起来。但是这样调用的消耗也太大了。 模拟的就是第一个节点的情况，把分开的分开处理，然后跨树的节点相连。 这样其实理解起来还难一点，但是主要是一个无死角覆盖的问题，和一个跨树的处理的问题，全部归化成第二到第三层的问题。 class Solution { public: // 主函数 Node* connect(Node* root) { if (root == nullptr) return nullptr; connectTwoNode(root->left, root->right); return root; } // 辅助函数 void connectTwoNode(Node* node1, Node* node2) { if (node1 == nullptr || node2 == nullptr) { return; } /**** 前序遍历位置 ****/ // 将传入的两个节点连接 node1->next = node2; // 连接相同父节点的两个子节点 connectTwoNode(node1->left, node1->right); connectTwoNode(node2->left, node2->right); // 连接跨越父节点的两个子节点 connectTwoNode(node1->right, node2->left); } }; 二叉树展开为链表（114） 我自己的解法beat 100 99 要注意的是要全部收敛到右侧，解题思路写在下面的代码中，拜读一下自己。 class Solution { public: void flatten(TreeNode* root) { helpflat(root); } TreeNode* helpflat(TreeNode* root){ // 递归的终点以及空值判断 if(!root) return nullptr; //n-1假设：flaten后续的节点并变到左侧，同时由于后续的接入需求，我们需要return最后一个有值的节点 TreeNode* lefte = helpflat(root->left); TreeNode* righte = helpflat(root->right); //加入最后左右都是0的话，我们就return当前节点而不是下一个节点（因为我们需要最后一个元素的索引），这其实也是终值判断 if(!lefte && !righte) return root; // 如果只有右边无序处理，算是已经摊开好了 else if(righte && !lefte) { } //两边都有或者只有左边的情况下，就是把左边的最后一个的下一个接到当前节点的右侧那一路，然后将改节点转移到右侧，最后将左节点清空。返回尾巴，无论是左边还是右边。 else { lefte->right = root->right; root->right = root->left; } root->left = nullptr; // 返回尾巴，如果有右侧尾巴的话，他就在最后，否则就是左侧尾巴是最后 return righte?righte:lefte; } }; FA的解法和思想：(实际上基本是差不多的) // 定义：将以 root 为根的树拉平为链表 void flatten(TreeNode root) { // base case if (root == null) return; flatten(root.left); flatten(root.right); /**** 后序遍历位置 ****/ // 1、左右子树已经被拉平成一条链表 TreeNode left = root.left; TreeNode right = root.right; // 2、将左子树作为右子树 root.left = null; root.right = left; // 3、将原先的右子树接到当前右子树的末端 TreeNode p = root; while (p.right != null) { p = p.right; } p.right = right; } 构建最大二叉树（654） 这里理解上没啥问题，但是实现上有一些奇怪的问题需要分析，后续解决把 一个就是最后那个+1，没有弄的话会导致溢出等很严重的问题 第二是lvalue的问题还有一个就是为什么使用iterator在这里不太行，后续修改一下试试。 还有一个要注意的就是记得用new关键词来构造新的节点，不然return的那个东西最后本身都不存在了还return个几把。 // FIXME:为什么在这里使用迭代器的方法会出现很多问题，无法进行实现，正确的写法应该是怎么杨的？ // lvalue来初始化一个node我知道不行，但是为什么会是左值呢。 // class Solution { // public: // TreeNode* constructMaximumBinaryTree(vector& nums) { // TreeNode* root = helpbuild(nums.begin(),nums.end()); // return root; // } // TreeNode* helpbuild(vector){ // int max= *begin; // for(vector::iterator it=begin; it!=end; it++){ // if(*it>max) // max = *it; // } // coutval = max; // auto next = find(begin,end,max); // inner->left = helpbuild(begin,next); // inner->right = helpbuild(next+1,end); // return inner; // } // }; class Solution{ public: TreeNode* constructMaximumBinaryTree(vector& nums) { return helpbuild(nums,0,nums.size()); } TreeNode* helpbuild(vector& nums, int begin, int end){ if(nums.empty() || begin==end) return nullptr; int maxindex = -1, maxval = INT_MIN; for(int i=begin;i=maxval) { maxindex = i; maxval = nums[i]; } } // inner的生存周期问题 TreeNode* inner = new TreeNode(maxval); inner->left = helpbuild(nums,begin,maxindex); // 下面这里没有+1的画，会导致一个机器严重的问题，但是我不知道为啥，是溢出了把，永远无法到达终点？ inner->right = helpbuild(nums,maxindex+1,end); return inner; } }; :star: 寻找重复的子树（652） LINK FA参考链接， 这一题的解题思路还是比较有意思的，解题过程中也出现了比较多的问题，还有一些有待解决的问题需要分析。 用后序遍历的序列来表征子树:可以观察特点，就知道只有后续遍历保留了子树的结构，其他的方式都有一部分是Top-Down的，就不符合子树的要求 String的方式来寻找重复子树 int变量如何转换到string，为啥出现了很多问题，还有网友的解决方法对比 现在的时空复杂度结果都不太好到网上找一下更好的解决思路和解决的方案； 下面给出一个基本的解法，后续需要进行优化和补充。 // TODO:这题可以讨论一下python的解法，应该会更简单一点。 // FIXME：这题目前这样的结果十分的差，后序看看其他方法的改进，但是这题的解题思路还是很不错 #include #include class Solution { public: unordered_map memo; vector res; vector findDuplicateSubtrees(TreeNode* root) { if(!root) return {}; // 这个返回值要记得 traverse(root); return res; } string traverse(TreeNode* root){ if(!root) return \"#\"; string left = traverse(root->left); string right = traverse(root->right); // 得到一个后序遍历的序列，（但是基于这样的序列怎么判断子树一致呢？） // FIXME int到string的转换到底怎么做，好疑惑啊。 char temp = root->val +'0'; string resstr = left + \",\" + right+\",\" + temp; // 压入hashmap，通过数值判断重复的root；通过数值判断就知道有没有重复了 if(memo.count(resstr)) memo[resstr]++; if (memo[resstr]==1) res.push_back(root); return resstr; } }; :question: :star: 二叉树的序列化和反序列化（297） 这题在解题过程中出现了很多问题和值得探讨的点，后续一定要进行归纳总结以及二刷。 二刷TODO: 层级遍历的思路设计总结 各种遍历方式的可行性分析(实现) 遍历与数据结构的相对应分析 问题归纳： 需要总结一下各种数据类型的空值return方式（也就是空值的表达） string : \"\" {} 相关的各种常见类型之间的转化； string to int : stoi() int to string : to_string() (需要include) 各个类别中的迭代器实现和类别; 连续append的实现: (.append().append()); 切分字符串split方法的实现(思路),以及为什么这里在前序遍历的时候需要选用Queue; 什么类别,什么情况下需要先调用new再执行后续的复制操作(为啥定义的时候不需要,但是这里要new) 有时候append不行但是push_back可以,这是为什么?这个区别是string特有的还是通用的? 编程的format实现,要全部写在类里还是类外.(分析易读性) 前序遍历的代码实现 class Codec { public: // 实际上该问题还是突出一个三种遍历方式的问题； // Encodes a tree to a single string. string serialize(TreeNode* root) { string BTString; string SEP = \",\"; string ENDS = \"#\"; if(!root){ BTString.append(ENDS).append(SEP); return BTString; } // 前序遍历位置 // TODO：连续append的表示形式 BTString.append(to_string(root->val)).append(SEP); BTString.append(serialize(root->left)); BTString.append(serialize(root->right)); return BTString; } // Decodes your encoded data to tree. TreeNode* deserialize(string data) { // 切分字符串方法 find & substr? // 我们在这里需要的是一个先进先出的情况，所以实际上是一个Quene的类别 // 但是好像实际上for循环也能满足这个问题，但是如果我试图使用递归的话，那我应该还是要用队列 // 长度不固定的问题 // FIXME:String的空值的情况 if(data==\"\") return nullptr; // NOTE: 通过循环将布不恒等的数值压入队列 queue q; string Tmpstr; for(int i = 0; i& que); }; TreeNode* Codec::helprebuild(queue& que) { TreeNode* root; // if (que.empty()) // return nullptr; string str = que.front(); if(str == \"#\") { que.pop(); return nullptr; } // FIXME：还没有给这个类别建立一个存储空间 root = new TreeNode(); root->val = stoi(str); que.pop(); root->left = helprebuild(que); root->right = helprebuild(que); return root; } 完全二叉树的节点计算 完全二叉树和满二叉树有很多不同的定义方式，本文中针对的完全二叉树计算是如下的这种情况： 每一层都是紧凑靠左排列的 首先这种节点情况进行计算的话，最优的时间复杂度应该是$O(logN * logN)$ ? 首先普通二叉树的话直接就是O（N）遍历就完事了； 如果是全部填满的满完全二叉树架构的话，就直接$O(logN)$指数计算就好了。 那么完全二叉树的话，应该是前两者结合，也就是当左右的深度相同的话，就不需要计算，只需要在左右深度不同的情况下进行遍历的操作就可以了，但是这个思想的实现，对于计算复杂度的实现是相当巧妙的，好好分析一下。 public int countNodes(TreeNode root) { TreeNode l = root, r = root; // 记录左、右子树的高度 int hl = 0, hr = 0; while (l != null) { l = l.left; hl++; } while (r != null) { r = r.right; hr++; } // 如果左右子树的高度相同，则是一棵满二叉树 if (hl == hr) { return (int)Math.pow(2, hl) - 1; } // 如果左右高度不同，则按照普通二叉树的逻辑计算 // 这里两边都是null的情况就会回归1，不需要额外的判断 return 1 + countNodes(root.left) + countNodes(root.right); } 复杂度分析 一棵完全二叉树的两棵子树，至少有一棵是满二叉树：所以不断切分迭代的话，我们就知道每次的while是$O(logN)$ 需要迭代$O(logN)$的深度，所以就是上面分析的复杂度。 :star: 二叉树的最近公共祖先（236）二刷 这个设计思路还挺有意思 ，虽然代码不长但是思路还比较复杂；可以参考一下这个代码的设计思路，实际上还是逃不脱二叉树的几种框架： 根据我们需要首先访问的值来决定我们的遍历框架。 这题首先需要我们找到最底部的值（后序遍历），然后一层一层的往外找，然后找到最底层的哪个公共root（再往上肯定就都是公共的了）； 如果只有其中一个值就返回那个值的指针，如果root的两侧包含了两个，就返回root。 由于如果Node1的左右包含了p，q；那么Node的父节点的左右肯定另一侧是无效值，可以用这个来设计内层覆盖外层返回值的逻辑。 class Solution { public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { // 用来判断是否两个点都存在在以如今的root所在的树中，如果存在的话就返回True // 设计的主要原则： // 1. 出现在了后面节点的左边和右边的话，就不可能出现在父节点的左边和右边 // 2. 再叶节点找到相应的两个指，需要从后面遍历起来，先找到两个节点的位置，然后一级一级的并上来。 if(!root) return nullptr; if(root == q || root == p) return root; TreeNode* left = lowestCommonAncestor(root->left,p,q); TreeNode* right = lowestCommonAncestor(root->right,p,q); if (left && right) return root; if (!left && !right) return nullptr; return left?left:right; } // bool comexit(TreeNode* root, TreeNode* p, TreeNode* q); }; 扁平化嵌套列表迭代器（341） 这一题主要要好好的看提示和题目， 这题的关键在于辅助空间的建立和相应的（迭代和循环的）嵌套，还有就是理解题目，但是这种简单的实现方式，实际上并不能达到一个比较好的时间和空间复杂度。 我的解法（初始）：我的方法也可以改成用栈来实现，原理是一样的。 class NestedIterator { private: int lens; int index; vector NestIt; // 关键点在于额外辅助数组的构建，但是是否能够预先建立一个比较长的数组 // FIXME：通过初始化函数，建立一个比较合适的数组长度，从而减少需要额外分配内存的时间代价。 // Vector的重新初始化方式。 public: // TODO:迭代器的表示形式应该怎么写 // NOTE：这种题目的阅读逻辑应该怎么分析 NestedIterator(vector &nestedList):index(0) { helpBuildVec(nestedList); } void helpBuildVec(vector & nestedList){ for(int i =0;i 实际上迭代器应该根据需要来进行数值的导入，没有必要一开始就将全部的数据读取出来，这样的效率在一些特殊的情况下可能是不好的 改进1：惰性存放 Vector反向迭代 rbegin rend 利用栈，其实用队列然后正向迭代也是可以的 初步就是先把外层存进去，在实际调用的时候再解包的方法。 实际上需要关注的地方就是hasnext用来解包的过程。 class NestedIterator { private: stack st; public: NestedIterator(vector &nestedList) { for (auto iter = nestedList.rbegin(); iter != nestedList.rend(); iter++) { st.push(*iter); } } int next() { auto t = st.top(); st.pop(); return t.getInteger(); } bool hasNext() { while (!st.empty()) { auto cur = st.top(); if (cur.isInteger()) return true; st.pop(); auto curList = cur.getList(); for (auto iter = curList.rbegin(); iter != curList.rend(); iter++) { st.push(*iter); } } return false; } }; 1.1.7. 二叉搜索树 BST中的搜索（700） 利用大小的特性进行遍历就好了 class Solution { public: TreeNode* searchBST(TreeNode* root, int val) { if(!root) return nullptr; if(root->val == val) return root; if(val > root->val) return searchBST(root->right,val); if(val val) return searchBST(root->left,val); return nullptr; } }; BST第k小的元素（230） “从做算法题的角度来看 BST，除了它的定义，还有一个重要的性质：BST 的中序遍历结果是有序的（升序）”。 上面这个是最关键的一点，同时我们也很容易理解这点，根据BST本身的性质来说，所以这道题实际上很简单，我们只需要进行一个中序遍历就可以完成这样的问题了，然后在后续的位置进行累计。 还有就是熟悉迭代到底会从头开始输出还是从尾开始输出，这点逻辑要搞清楚，好像除了前序遍历的话，其他的都是会从尾巴开始的把。 文中提到的优化思路，其实是如何将算法优化到（logn），那就需要知道k和一个数是第几的这个关系，这样的话，实际上是需要树本身存放额外信息的（以自己为root的树有多少节点，这样就能分析出来自己顺位，从而和k对比）。 红黑树这种改良的自平衡BST，增删查改都是O（logN）的复杂度。（后续掌握） class Solution { private: int count = 0; int res = INT_MIN; public: int kthSmallest(TreeNode* root, int k) { traverr(root,k); return res; } void traverr(TreeNode* root, int k){ if (!root) return; traverr(root->left,k); count++; if(count==k) res = root->val; traverr(root->right,k); } }; BST转化为累加树（538，1038） 分析题目，实际上也是一个遍历的问题，由于找的是比自己大的所有数 :star: 所以累加的方向：是从大数累计到小数，而同时BST的中序遍历是有序的，所以我们改变中序遍历的方式，进行一个从大到小的遍历，用一个默认值来进行累计值的统计。 :question: 为什么NULL的方法会失效 由于Int的上限的问题，需要使用long不然会出问题 // FIXME:为什么使用NULL的方法会失效。 // LONG的使用情景， // 试着用迭代的方式去写（中序遍历） class Solution { public: bool isValidBST(TreeNode* root) { return helpJudge(root,LONG_MAX,LONG_MIN); } bool helpJudge(TreeNode* root, long int max,long int min){ if(!root) return true; // 这个大于等于等于号不能丢掉。 if(root->val >= max) return false; if(root->val right,max,root->val) && helpJudge(root->left,root->val,min); } }; BST的节点有效性（98） 也就是左侧的子树都要小于中间然后小于右侧的子树，这点，如果简单的对于所有的节点对左右判断的话，没办法维持子树全体的大小关系的特性，所以需要额外的记录一个值来保持这个特性： root的值是左边的最大值，是右边的最小值； 或者从有序规则来统计一个最大最小值来进行判断，（swap==0） boolean isValidBST(TreeNode root) { return isValidBST(root, null, null); } /* 限定以 root 为根的子树节点必须满足 max.val > root.val > min.val */ boolean isValidBST(TreeNode root, TreeNode min, TreeNode max) { // base case if (root == null) return true; // 若 root.val 不符合 max 和 min 的限制，说明不是合法 BST if (min != null && root.val = max.val) return false; // 限定左子树的最大值是 root.val，右子树的最小值是 root.val return isValidBST(root.left, min, root) && isValidBST(root.right, root, max); } BST插入数值（701） 对比判断左右就行了，就是加入了判断的遍历。下面是FA给出的框架 TreeNode insertIntoBST(TreeNode root, int val) { // 找到空位置插入新节点 if (root == null) return new TreeNode(val); // if (root.val == val) // BST 中一般不会插入已存在元素 if (root.val val) root.left = insertIntoBST(root.left, val); return root; } 我的解题方法：实际上就是基于遍历的思想，然后在需要的地方执行完操作再往后继续遍历就好了。实际上实现的效果非常的好。 class Solution { public: TreeNode* insertIntoBST(TreeNode* root, int val) { if(!root){ TreeNode* curr = new TreeNode(val); return curr; } if (val > root->val) { if (!root->right) { TreeNode* curr = new TreeNode(val); root->right = curr; return root; } insertIntoBST(root->right,val); } else { if(!root->left) { TreeNode* curr = new TreeNode(val); root->left = curr; return root; } insertIntoBST(root->left,val); } return root; } }; :star: BST删除（450） 下面这个的思路其实和我们想的是一样的，但是这种书写的方式留意一下，用delete来做就好。 class Solution { public: TreeNode* deleteNode(TreeNode* root, int key) { if(root == nullptr) return root;//第一种情况：没找到删除的节点，遍历到空节点直接返回 if(root->val == key) { //第二种情况：左右孩子都为空（叶子节点），直接删除节点，返回NULL为根节点 //第三种情况：其左孩子为空，右孩子不为空，删除节点，右孩子补位，返回右孩子为根节点 if(root->left == nullptr) return root->right; //第四种情况：其右孩子为空，左孩子不为空，删除节点，左孩子补位，返回左孩子为根节点 else if(root->right == nullptr) return root->left; //第五种情况：左右孩子节点都不为空，则将删除节点的左子树放到删除节点的右子树的最左面节点的左孩子的位置 //并返回删除节点右孩子为新的根节点 else{ TreeNode* cur = root->right;//找右子树最左面的节点 while(cur->left != NULL) { cur = cur->left; } cur->left = root->left;//把要删除的节点左子树放在cur的左孩子的位置 TreeNode* tmp = root; //把root节点保存一下，下面来删除 root = root->right; //返回旧root的右孩子作为新root delete tmp; //释放节点内存 return root; } } if(root->val > key) root->left = deleteNode(root->left, key); if(root->val right = deleteNode(root->right, key); return root; } }; 先找到，然后改，主要是不能破坏BST的数值结构.先写出基本的框架 TreeNode deleteNode(TreeNode root, int key) { if (root.val == key) { // 找到啦，进行删除 } else if (root.val > key) { // 去左子树找 root.left = deleteNode(root.left, key); } else if (root.val 情况 1：A恰好是末端节点，两个子节点都为空，那么它可以当场去世了。 if (root.left == null && root.right == null) return null; 情况 2：A只有一个非空子节点，那么它要让这个孩子接替自己的位置。 // 排除了情况 1 之后 if (root.left == null) return root.right; if (root.right == null) return root.left; 情况 3：A有两个子节点，麻烦了，为了不破坏 BST 的性质，A必须找到左子树中最大的那个节点，或者右子树中最小的那个节点来接替自己。我们以第二种方式讲解。 if (root.left != null && root.right != null) { // 找到右子树的最小节点 TreeNode minNode = getMin(root.right); // 把 root 改成 minNode root.val = minNode.val; // 转而去删除 minNode root.right = deleteNode(root.right, minNode.val); } 综上所述：但是我们通常不会通过val交换值来交换节点，而是通过链表操作来处理，暂时把框架和思路描写成这样，后续进行修改。 TreeNode deleteNode(TreeNode root, int key) { if (root == null) return null; if (root.val == key) { // 这两个 if 把情况 1 和 2 都正确处理了 if (root.left == null) return root.right; if (root.right == null) return root.left; // 处理情况 3 TreeNode minNode = getMin(root.right); root.val = minNode.val; root.right = deleteNode(root.right, minNode.val); } else if (root.val > key) { root.left = deleteNode(root.left, key); } else if (root.val 我的解题过程 遇到的问题： 删除的话不是之前的链表那种用nullptr替代，好像是直接用delete去做的逻辑，这样的话，编写的难度其实就不太一样了，我们可以找一个替代指针来进行删除。 分析清楚总共有几种情况，一些特殊的情况下直接进行一个值的替换可以吗。（不行） 这里有好几种解题思路，主要用的是一个替换的思想，这里后面要重新看看多捋捋。（这些算法实际上都涉及到内存泄露把，就很离谱） 但是还有一个典型的就是只要我们最后return的ptr的树，是我们的目标树来着。这里给出的方法实际上是直接整个把左子树接到右子树的后面去了，如果删除的是中间节点的话，学习一下。 1.1.8. 动态规划方法 首先是动态规划的基本思路（适用条件）（剑指有提到讲的还挺好）：本质上是一种穷举的搜索方法 求解的是最值问题 最优解可以依赖于子序列的最优解（最优子结构） 大问题可以分解为小问题，小问题还有重叠的更小的子问题； 从上到下分析问题，从下到上的求解问题（避免重复计算）（需要额外的存储空间） 最关键的点在于正确的状态转移方程。（实际上也就是二叉树中多种遍历之间要执行的那个操作）： 明确[状态] -> 定义dp动态规划表中的数组或者函数的含义（子最优状态）-》明确选择-》明确base case就是起始状态 可以看一下其中的硬币问题 实际上很多情况下可以使用这样的解题策略： Bottom-up （Vector，或者用hashtab之类的D-P table 存储） 双指针Bottom-up 其中设计迭代的一些准则： 遍历的过程中需要的状态必须是已经算出来的 遍历的重点是我们要存储结果的那个位置。 存储表和DP-TABLE不是一个意思 动态规划的状态压缩方法 主要用于我们需要有二维的存储空间的时候，怎么压缩成一维的这种情况。 理解如下： 初始状态直接向下压成1维度，去掉i维度 通过内外层循环的不同更新特性，来逐渐的覆盖之前的值。 用一个pre和temp来保留上一层的i 和上一层的j 然后基于先后的更新顺序取更新问题。 实际上就是利用更新的延后性去压缩空间，实际上只要保留一个temp和一个pre就可以了、 TODO： 二刷的时候考虑更新之前的动态规划算法 在做后续题目的时候也考虑执行压缩的策略。 :question:正则表达式问题： 永远的苦主，事实上我应该意识到，这样移动序列的问题，完全可以转化成，递归或者说是动态规划的问题来做，通过一个设想的匹配函数和一个相应的状态转移方程来进行，由于其中的*号带来的多种重复可能性，所以可能需要像动态规划那样建立一个索引表来防止重复计算，这个我们也要重新进行分析一下看看。（我觉得好像是不用的把。） 实我还没get为什么这一个问题属于正则表达式的问题。研究一下FA中的说法把， 通过剑指offer中的递归的思想，倒是能够解决这道题，但是问题就在于，这样的话，虽然空间复杂度好了，但是时间复杂度拉跨的不行，这就说明是存在着重复运算的动态规划的情况把，所以我们使用FA中的思路建立一个memo表 class Solution { public: bool isMatch(string& s, string& p) { // 空值测试 if(p.empty()) return s.empty(); return findMatch(s,p,0,0); } bool findMatch(const string& s , const string& p,int si, int pi); }; bool Solution::findMatch(const string& s, const string& p, int si, int pi) { // if(p.empty()) return s.empty(); int s_ize = s.size(), p_size = p.size(); if (pi>=p_size && si=p_size && si>=s_ize) return true; // 第二种，匹配到了*号的情况 // 由于关键的*的个数，实际上是一种不确定的情况，而只要有一种情况符合就只要当True即可 // 所以用递归的方法去做还是挺合适的 if(p[pi+1]=='*') { if(p[pi]==s[si] || (p[pi] == '.'&&si 建立递归表的方式，这里是参考的官方的减法，但是这里的加和减，和上面的区别就是， 这里建表的假设是前面的几项是否相同（True or False），以及我们是往后面迭代，他是从前面开始迭代，我们跌打到后边界，他迭代到前边界。所以实际上还是和我们的方法一样的，然后就是通过dp的框架就行。 这个思路我还是写不好，DP表建立的不好，改天找一天耍一天动态规划 class Solution { public: bool isMatch(string s, string p) { int m = s.size() + 1, n = p.size() + 1; vector> dp(m, vector(n, false)); dp[0][0] = true; for(int j = 2; j :star: 编辑距离（着重用于思路理解） 解决两个字符串的动态规划问题，一般都是用两个指针i,j分别指向两个字符串的最后，然后一步步往前走，缩小问题的规模 这一题的关键在于，如何将这样的问题抽象成状态转移方程，如何抽象成一个动态的规划问题。Fuck Algorithm解析。 后续通过第一串代码和最后的正确代码进行分析，但是这样的方法实际上效率也还不够高 我们也可以用new生成普通数组的方式去做，此外，如果我们要存储具体的操作，我们可以定义一个简单的NODE structure去实现这个功能，存放val和opp； TODO：空间效率优化：将二维空间压缩成一维的情况分析。 结合之前的硬币问题，我们可以把三种操作当成三种状态转移操作。然后将两个字符串的长度，看成矩阵的两个维度，然后通过状态转移操作进行坐标上的变换，由于我们需要的是最短距离；我们就假设我们的函数是从A->B的最短距离的转移函数； 分析问题的时候给定两个确定的case去分析：1.起始状态 2.状态转移； 此外：我们不要去分析最优应该是什么样的，遍历求最值，就是能做的操作都做，使用动态规划的方式降维而已。 基本的实现思路如下（需要集成存储思路） ✔：动态规划的方法实际上还是Bottom-up更好，无论是从空间还是时间上来说 (但是实际上实现效率还是没有提高，为什么呢？) class Solution { public: int minDistance(string word1, string word2) { // 实现将word1 变成 word2 // auto w1_it = word1.rbegin(); // auto w2_it = word2.rbegin(); // 首先使用递归的方式实现一下这个问题，然后再用迭代的方式做 int w2_idx = word2.size(); int w1_idx = word1.size(); // 存储表实现：vector 长度初始化学起来(主要需要存储一个空值所以记得后面的index要加1) vector> memo(w1_idx+1,vector(w2_idx+1,0)); // 初始状态初始化 for(int i =0; i 子序列问题 关键的解题思想： 一维的DP数组：这种子序列问题（子序列不同于子串），需要的一般都是以i为结尾的情况下，取得的最值，这样才符合我们需要归纳 的条件。 // 基础的算法模板如下 int n = array.length; int[] dp = new int[n]; for (int i = 1; i 在子数组array[0..i]中，以\\array[i]*结尾的目标子序列（最长递增子序列）的长度是dp[i]*。 二维的DP数组:这种思路其实用的更多，尤其是涉及到数组，两个字符串这样的问题的情况下，这种思路实际上涵盖了，包含一个字符串和两个字符串的情况 int n = arr.length; int[][] dp = new dp[n][n]; for (int i = 0; i 涉及两个字符串/数组时（比如最长公共子序列），dp 数组的含义如下： 在子数组arr1[0..i]和子数组arr2[0..j]中，我们要求的子序列（最长公共子序列）长度为dp[i][j]。 可以参考的是编辑距离和最长公共子序列两个文章 只涉及一个字符串/数组时（比如本文要讲的最长回文子序列），dp 数组的含义如下： 在子数组array[i..j]中，我们要求的子序列（最长回文子序列）的长度为dp[i][j]。 最长递增子序列（300） 我的思路（排雷）： 首先找到长长度为一的所有子序列，然后从这个子序列的尾巴出发，找到后续的长度+1的子序列 覆盖，清楚，（O(2N)的空间） （O（n^3））太差了，正常动态规划应该怎么去做。 实际上正确的修改能得到二分查找N*logN的最佳时间复杂度的方法。 为什么不能用以每个结尾的子串中的最长子序列来做动态规划 因为最大值不可控，而且主要是这个玩意没办法重复利用。还是需要从重复寻找子序列。 :star: N*logN的改进二分查找加贪心算法 修改前面的假设的方法：我们要以当前值作为前面那个最长子序列的结尾（不是一次搜索完，而是只搜索到当前元素），然后维护一个长度的结尾值最小的算法。官方解答 这里要学习一下二分查找的思路理念 这题实际上不就是单调栈模板的二分优化嘛，这种情况 通过一个-1来错开同时并减少多余计算。 用二分查找来找到第一个大于的值的位置（的思想）分析这种情况，从最后的区间开始分析。（我们要找的是第一个比他小的数，所以最后要加一） class Solution { public: int lengthOfLIS(vector& nums) { // 基于二分查找和贪心的算法 vectorDpT(nums.size()+1,0); // 这里好像没考虑到负数的情况吧,便于比较和加入所以+1 int lens = 1; DpT[lens] = nums[0]; for(int i=1;iDpT[lens]){ DpT[++lens] = nums[i]; } else{ // 二分查找大型现场。 int left = 1, cur=0, right = lens; while(left >1; if(nums[i]>DpT[mid]){ // 这里为什么要+1，避免重复搜索同时做种错开吗？ left = mid+1; cur = mid; } else{ right = mid-1; } } DpT[cur+1] = nums[i]; } } return lens; } }; 动态规划 dp[i] 表示以 nums[i] 这个数结尾的最长递增子序列的长度 在迭代搜索的过程中，只需要找比当前值更小的前序子序列+1取最大值就行，最终返回值是dp中的最大值，复杂度O(n^2). 空间复杂度还行，时间复杂度依旧拉跨。如何将算法的复杂度降低到O(n*long(n)) class Solution { public: int lengthOfLIS(vector& nums) { // 建立dptable，每个存放以当前值为结尾的最长子序列长度 // 在迭代搜索的过程中，只需要找比当前值更小的前序子序列+1取最大值就行 // 最终返回值是dp中的最大值，复杂度O(n^2) vector DPtable(nums.size(),1); // bottom-up 循环 for (int i=1;inums[j]) DPtable[i] = max(DPtable[i],DPtable[j]+1); } } // 找到最大值返回 int res =1; for(int& v_dp: DPtable) { if(v_dp>res) res = v_dp; } return res; } }; 这里降低到n*logn的方法还比较猎奇。通过新建立堆的方法来实现，但是这种方法为什么的 俄罗斯套娃信封问题(354) 这个问题是个很有意思的问题，实际上关键在于通过合理的排序操作来给问题降维 同个宽度的信封无法互相嵌套，那么如何在通过最长递增子序列的搜寻来排除掉同个高度的多个选择呢？（高度之间逆序排列，那么其中的两个无论如何都无法是递增的这样就能达到我们的唯一性和递增的目的了） 一个维度升序一个维度降序，然后进行最终的最长子序列的搜索即可。 通过排序（一正一逆来给问题降维成一个最长递增子序列的问题）主要的实现难点应该就在快速排序和嵌套排序中。（这里肯定要使用快速排序把，这也是时间复杂度的重要标准来着。） 二分查找的change位置放错了，找了半天 /* * @lc app=leetcode.cn id=354 lang=cpp * * [354] 俄罗斯套娃信封问题 */ // @lc code=start #include #include using namespace std; class Solution { public: int maxEnvelopes(vector>& envelopes) { if(envelopes.empty()) return 0; // 使用匿名函数编写排序算法，sort采用的是快排的基本方法 sort(envelopes.begin(), envelopes.end(), [](const vector & A , const vector & B){ return A[0]B[1]); }); // 接着使用二分查找结合动态规划来搜索最长递增子序列 vector dpt (envelopes.size()+1, 0); // HYPER int len = 1; dpt[len] = envelopes[0][1]; for (int i = 1;idpt[len]){ len = len + 1; dpt[len] = envelopes[i][1]; } else{ //TODO:lower_boundry的使用 // FIXME:不知道这个二分查找的问题在哪，非常疑惑 change的位置放错了 int l = 1, r = len, cur = 0; while(l>1; if (envelopes[i][1]>dpt[mid]){ l = mid+1; cur = mid; // 最终就会是小于的最后一个 } else{ r = mid ; } } dpt[cur+1] = envelopes[i][1]; } } return len; } }; // @lc code=end 最长回文子序列（516） 这一题的分析里面主要可以通过上面的假设和绘2维图来辅助分析，考虑到常规情况下的回文判断是对两侧的拓展实现的，如果不是两侧相同的画，两边不可能同时对子串发生回文增益；这句话中就隐含了一个操作和两个状态变换。 代码实现。 class Solution { public: int longestPalindromeSubseq(string s) { // 画那种二维图能够帮助分析是真的， // 考虑到我们常规情况下的回文判断是通过两侧拓展实现的 // 我们在这里也执行增加的两侧相等时进行判断，其他时候借助于状态转移 // if (s.size()==0) return 0; int n = s.size(); int res = 1; // 初始化DP Table 显然子序列的长度应该是大于0的，至少得有一个，所以 vector> DpTable(n,vector(n,0)); for (int i=0;i=0;i--) { for (int j=i+1;j 最大子数组和（53） 实际上也是动态规划的问题，每个表存储着以当前节点为结尾的最大子数组和，这个思路其实很简单，因为需要是连续的，所以只有两种选择，要么时自身开始，要么和前面合并；那就做一个max就性了，这样的画实际上只需要3个值来做中间变量就可以了。 这可能就是压缩的思路，后需要好好看看怎么压缩的， class Solution { public: int maxSubArray(vector& nums) { int res = nums[0]; int pre = nums[0]; int cur = nums[0]; for (int i = 1; i 0 无脑加上 if (pre > 0) cur = pre + nums[i]; else cur = nums[i]; // 修改到上个数字为止的总和 pre = cur; // 验证一下最大值 res = max(res, cur); } return res; } }; 最长公共子序列（1143） 这题实际上和编辑距离的思考逻辑有点像，从上面的框架出发，我们很容易考虑到，实际上就是存放到，ij的最最长公共子序列。 那么我在思考的时候出现了一定的盲区（实际上画图很容易考虑到），在这里分析一下： 因为如果他们不等，他们是没办法共同产生增益的，也就是一个在末尾的话，另一个就不能在末尾了。所以实际上也是两种转移状态中的max情况。那么就可以开始写了。 class Solution { public: int longestCommonSubsequence(string text1, string text2) { // 分析一下basecase // bottom-up 等下写一下递归的框架 int m = text1.size(), n = text2.size(); // 我们分析可以知道，我们按照3个方向进行搜索的话，一开始会存在溢出，所以 // 我们不妨牺牲一点点空间来换取每一步都需要判断的 不必要的运算时间 vector> dpTable (m+1,vector(n+1,0)); // TODO: 存储空间压缩 for (int i = 1;i dp(n + 1, 0); // dp和text的index 对应关系有1的offset记得 for (int i = 1; i > dpTable (m,vector(n,-1)); return dp(text1,m-1,text2,n-1,dpTable); } int dp(string s1, int i, string s2, int j, vector>& dpTable ){ if (i == -1 ||j == -1) { return 0; } if(dpTable[i][j] != -1) return dpTable[i][j]; if(s1[i] == s2[j]) dpTable[i][j] = dp(s1,i-1,s2,j-1,dpTable) +1; else dpTable[i][j] = max(dp(s1,i,s2,j-1,dpTable),dp(s1,i-1,s2,j,dpTable)); return dpTable[i][j]; } }; 实际上 两个字符串的删除操作（583）也是公共子序列的问题，稍微修改一下就好了 return -2*dpTable[m][n] + m + n ; 实际上两个字符串的最小ASCII删除和（712）也是公共子序列的问题，也是稍微修改一下： char直接赋值给int就是ASCII 把中间存储的是相等的（个数）改成 Ascii和；（毕竟求的是最小的ASCII）,求得最大的重叠ascii码就行了、 class Solution { public: int minimumDeleteSum(string s1, string s2) { int m = s1.length(), n = s2.length(); vector> dpTable (m+1, vector(n+1, 0)); for (int i =1;i :star:背包问题 首先阐述一下《0-1背包问题》的题目： 给你一个可装载重量为W的背包和N个物品，每个物品有重量和价值两个属性。其中第i个物品的重量为wt[i]，价值为val[i]，现在让你用这个背包装物品，最多能装的价值是多少？ 类似动态规划问题的实现框架： for 状态1 in 状态1的所有取值： for 状态2 in 状态2的所有取值： for ... dp[状态1][状态2][...] = 择优(选择1，选择2...) 题目具体分析： 我们很容易基于动态规划的思想画出一个简单的转移图（VAL=价值），但是问题在于转移图中的两个索引我们打算如何去定义他。实际上最根本的一个想法就是重量和次序，但是这样就有几种设计的情况了，重量：1. 剩余空间，2，已装入的空间；次序：1. 已装入的文件个数 2. 可选择的物品个数（也就是逐步的遍历所有物品）（第二中实际上我不太想得到，和我的思路不太一样，但是我们需要学习这种思考的的方式。） 那么如何选择呢？ 首先重量的话，我觉得应该是都可以实现的，最后也不会用剩余重量来进行索引，那我们如果这样的话，我们如果用已经装入文件的个数来进行索引的话，我们没办法去选择出一个最后的状态，但是如果我们使用可选的物品个数，对其选择装入与否的话，就是一个最终的状态了。 其实换句话来理解的话： 重量是State，价值是Vale，而每一步是一个0-1选择的问题：很显然这样做的话，我们每步的传递判断，状态转移判断，都会有一个比较底的遍历选择范围，同时最终也有一个清晰的结尾。而且，最关键的是：已装入的个数，实际上很难作为一个指导的状态，因为我们不知道后面的选择范围到底会变成什么样子的。 最终的框架就会是： int dp[N+1][W+1] dp[0][..] = 0 dp[..][0] = 0 for i in [1..N]: for w in [1..W]: dp[i][w] = max( 把物品 i 装进背包, 不把物品 i 装进背包 ) return dp[N][W] 而如果实现的 int knapsack(int W, int N, vector& wt, vector& val) { // vector 全填入 0，base case 已初始化 vector> dp(N + 1, vector(W + 1, 0)); for (int i = 1; i 分割等和子集（416） 通过求和将问题转化为背包问题，基本思想完全不变，j为当前的总状态，i为考虑的第i-1g个物品，然后也是求最大的值和，这样的话，最后判断值和是否能传递到最后（是否相等即可） 但是我们还是需要灵活一点，这题实际上可以转化为bool类型去做，实际上状态的传递，用与或非即可实现，bool的转移 同时这里涉及到2维度模型的压缩，基于上面的思路，我们可以通过循环的参数去优化，（最好还是从概念上理解这个优化，这样写起来才能一步到位。） class Solution { public: // 首先进行求和，然后将sum/2作为背包的容量，那么实际上就是一个背包问题 // 只要对最终的情况进行判断是否为相等就行。 // FIXME：但是这一题的最终重要的问题在于模型压缩，我们如何实现状态转移中的压缩。 bool canPartition(vector& nums) { // 求和 int sum = 0; for (int& num: nums) sum += num; // 背包问题求解 if(sum%2 != 0) return false; sum = sum/2; vector> bapack (nums.size()+1,vector(sum+1,0)); // i 对第i个物体进行判断 j 当前已装入的重量总数；好好分析 for (int i = 1;i 基于bool方式的改进 bool canPartition(vector& nums) { // 求和 int sum = 0; for (int& num: nums) sum += num; // 背包问题求解 if(sum%2 != 0) return false; sum = sum/2; vector> bapack (nums.size()+1,vector(sum+1,false)); for(int i =0;i 将算法压缩到一维的情况 class Solution { public: // 首先进行求和，然后将sum/2作为背包的容量，那么实际上就是一个背包问题 // 只要对最终的情况进行判断是否为相等就行。 // FIXME：但是这一题的最终重要的问题在于模型压缩，我们如何实现状态转移中的压缩。 // 还有运算时间的优化，这题可以换成bool来做。思考一下。 // 用bool的方式去做实际上就是一个状态的集成，所有的j=0都是true。,然后逐步进行状态的转移和传递即可 bool canPartition(vector& nums) { // 求和 int sum = 0; for (int& num: nums) sum += num; // 背包问题求解 if(sum%2 != 0) return false; sum = sum/2; vector bapack (sum+1,false); bapack[0] = true; // i 对第i个物体进行判断 j 当前已装入的重量总数；好好分析 for (int i = 1;i=0;j--){ // 很明显需要保存的就是上一轮的结果 i-1的情况，别的没啥好说的了 if (j :star:完全背包问题 基于零钱兑换问题解决一下所完全背包的问题，找一下有没有别的完全背包问题需要做。基本的思路看零钱兑换问题上的实现。 零钱兑换Ⅱ（518） 这一题的主要思路在于状态转移的情况，我们怎么样排除重复，同时针对这种对于排列不重要只看重组合的情况下（每个items有无数种的完全背包问题，我们应该如何去做） 实际上基本的思路应该是对所有的items作为一个维度来考量，这个items用没用到作为一个维度来把握，这样的话，然后用一个状态转移值取代替这样的解答。 注意该算法在压缩时候的特殊性 i的区别(未压缩的已经注释掉了 ) 时间复杂度为啥高也没搞清楚，看看更快的方法把，但实际上这个顶多也就N*amount感觉不高了 class Solution { public: // 完全背包问题，看看官方的解说，对于单题来说解决的挺好的，但是我们要分析这样分析的原因 // 这题是真的狗，到底怎么去构建这个动态增长的最优过程，也就是状态和状态转移，是最难的地方 // 最难的思考点在于到底是怎么排除掉重复计算可能性的情况？ // FIXME：主要在于第二个分支，他一定用到了新增进来的这个值，所以和上面的情况不可能出现重复 // 而基于假设就是所有的情况这种情况，就能将所有的列举额出来，给老子细细品味这个 int change(int amount, vector& coins) { if(!amount) return 1; int n = coins.size(); // 初始化 // vector> DP (n+1, vector(amount+1,0)); // for (int i = 0;i DP (amount+1,0); DP[0] =1; // 进行迭代操作（后续进行压缩简化） for (int i = 1; i=coins[i-1]) DP[j] = DP[j] + DP[j-coins[i-1]]; //if(j :star:高楼扔鸡蛋问题 这一题就连看懂题目都很折磨了，他要的是我们求出最优方案最少需要几步能解决这样的问题，他没有一个实际的解，要求的是这样一种，最坏情况下的最优。 [ ] 后续可以参考FA->官方题解，学习进阶的思想和思路， 基本假设：Value：K（State）和楼层N（state）得到的最少次数 怎么定义这种情况下的状态转移方程 这个理解起来比较容易，但是我们要知道，实现的时候，由于表是不断递归减小的，也就是上线N，所以实际上是一个三重循环，我卡牛角尖卡在这里。 怎么定义最坏情况。 由于没给出实际的结果，所以我们并不知道到底鸡蛋在哪一层碎掉，所以这个鸡蛋碎不碎的状态转移条件就通过MAX函数来模拟这个最差的情况。 然后用最内层MIN循环得到最好的解。 切分块 但是时间复杂度肉眼可见的高，所以我们最好分析一下，最坏情况下的最好是什么情况，我们可以知道二分法能解决这样的问题。 知道是三重循环的话，就能写出Bottom-Up的写法了。 基本状态转移方程思路 class Solution { public: int superEggDrop(int K, int N) { if(!K || !N) return 0; if(K==1) return N; if(N==1) return 1; int minstep = N+1; // 这个思想我是理解了，如果不使用dp tabble的思路的话，还是很清晰的，但是时间复杂度无法减小。 // 主要就是，他的那个外层在不断的变小，也就是那个表的上界N在不断的迭代，如果我们bottom-up的话， for (int i =0;i 使用DP-TABLE和BOTTOM-UP的方法进行求解 实现三重循环进行迭代。但是还是存在runtime error的问题，超出了时间的限制。 class Solution { private: // bool isdefined = false; // vector> mindrop; public: int superEggDrop(int K, int N) { if(!N) return 1; if(K==1) return N; vector> DP (K, vector (N+1,N+1)); // 要记住鸡蛋的个数要和序号有-1的关系 // 状态初始化，如果只有一个鸡蛋的话，就需要从头到尾遍历 for(int i =0;i 基于二分假设进行进一步的优化 我们很容易知道随着楼层的递增，需要的步数一定是增加的，这点毫无疑问，根据这样的特性，我们怎么找出最坏情况下的最好呢？实际上可以转化成上述代码中的 内层循环中MAX的最小值 可以发现其中的索引是反向的关系，那么就能给出如下图所示的搜索策略结论 这样就可以简单的基于二分法来求解这样的优化过程，需要注意的是： 递增递减，但是不是线性的递增递减，所以不能直接找中间序号要用二分查找傻逼 class Solution { private: // bool isdefined = false; // vector> mindrop; public: int superEggDrop(int K, int N) { if(!N) return 1; if(K==1) return N; vector> DP (K, vector (N+1,N+1)); // 要记住鸡蛋的个数要和序号有-1的关系 // 状态初始化，如果只有一个鸡蛋的话，就需要从头到尾遍历 for(int i =0;i>1; if (DP[i-1][iner-1] 戳气球问题（312） 也是个动态规划的问题，遍历所有情况选取最优，但是我觉得其实还能有别的解法，也就是从小到大选数，但是边界值还是要特殊处理，后续看看这种想法能不能写吧。动态规划的方法在我的GoodNote中写了，时间效率一般，看看别人的解答 首先给出动态规划情况下的思路和解答 class Solution { public: int maxCoins(vector& nums) { if(nums.empty()) return 0; // 首尾不包含 nums.insert(nums.begin(),1); nums.push_back(1); int n = nums.size(); // 加了两个之后的size vector> DP(n,vector(n,0)); // 从下往上遍历，从左往右遍历 for(int i = n-2; i>=0;i--) { for(int j =i+1; j 博奕问题 博弈类问题的套路都差不多，下文举例讲解，其核心思路是在二维 dp 的基础上使用元组分别存储两个人的博弈结果。下面引入一个例题 PAIR 将石头问题改的根据被一般性： 石头的堆数可以是任意正整数，石头的总数也可以是任意正整数，这样就能打破先手必胜的局面了。比如有三堆石头 piles = [1,100,3]，先手不管拿 1 还是 3，能够决定胜负的 100 都会被后手拿走，后手会获胜。 假设两人都很聪明，请你设计一个算法，返回先手和后手的最后得分（石头总数）之差。比如上面那个例子，先手能获得 4 分，后手会获得 100 分，你的算法应该返回 -96。 实际上还是和上面的一样，没什么区别，就是，要用元组（Pair）in CPP，然后选取单步最优吧，后面的最优交给后面的去搜索得到。动态规划吗唔。实际上就还是遍历所有的解法。所以我们不用考虑搜索的策略。 四键键盘问题 第二种思路很有参考意义，第一种方法比较常规但是实际上反而没那么容易想到，效果也比较拉跨，不推荐学习。 :star:股票问题 根据FA中的讲解，这一部分我们对股票问题的分析分为两步，第一步是实现基本的动态规划解题框架；第二步是学习一下针对这类问题进阶的状态机解法的问题； 首先LeetCode中的第一题股票问题就很简单，没什么好多说的，实际上分析问题是一个单次遍历求解最优值的过程；（问题分析能大大的减少复杂度）； 然后我们可以从第二题引出我们的动态规划解法的框架： 第二题实际上也给出了我们对于动态规划应用情景的更好理解: 也就是那种分段式的结构，只是把i,j从i到j修改成了买入和卖出而已。这就是股票问题的一个框架把。 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 框架的具体实现（伪代码）： def maxProfit(prices): res = 0; for buy in range(len(prices)): for sell in range(buy+1, len(prices)): res = max(res, maxprofit(prices[sell+1:])+prices[sell]-prices[buy]) return res 第二题算法问题具体实现（第一题在代码中） 这题使用这样递归的动态规划方法的话，确实是很简单，但是如果想要Bottom-Up去写好像还是比较麻烦的，也可能是这个定义不够好。 初始的动态规划的思路，一维数组，存储的是从今天开始的买入卖出的最优解，但是这样的话，状态转移方程实际上是不好列的;（我们无法确定状态转移房方程）（可能还要多加一层循环把。） 同样如果我们设置为是从今天开始买入的最优解，这样我写的状态转移方程会导致一个问题，就是后续的sell和buy绑定了，就会没有遍历到所有情况。（:x:）（可能还要多加一层循环才能实现） :star:官方的具体解法(考虑到现在手上是否持有股票)这应该就是C++情况下最合理的动态框架了 考虑到「不能同时参与多笔交易」，因此每天交易结束后只可能存在手里有一支股票或者没有股票的状态。然后根据有没有股票来进行四种状态转移。 定义状态 表示第 ii 天交易完后手里没有股票的最大利润， 表示第 ii 天交易完后手里持有一支股票的最大利润（ii 从 00 开始）。 class Solution { public: int maxProfit(vector& prices) { int n = prices.size(); int dp[n][2]; dp[0][0] = 0, dp[0][1] = -prices[0]; for (int i = 1; i 可以将其中的空间复杂度优化为如下形式： class Solution { public: int maxProfit(vector& prices) { int n = prices.size(); int dp0 = 0, dp1 = -prices[0]; for (int i = 1; i 还有一种解法就是下面的贪心算法，只要是正数我们就加的办法，下一个比上一个大，我们就卖，很简单。 int res = 0; for(int i =0; iprices[i]) res += (prices[i+1]- prices[i]); } return res; 问题的变体： 第三题，和第四题，都是限定了交易次数：如果使用的是递归的框架的话，就直接添加一个次数约束就可以了。上面的动态规划解法的话，给Dp添加一个次数的约束，然后在进行传递就可以了，后面自己修改一下写上来。 第五题，资金要冻结一天，也就是要加一天才开始交易，稍微改一下就行了； 第六题，每次卖出需要手续费，我们只需要在+price的时候把手续费扣除就可以了。 :small_red_triangle: 状态机解法： 状态机解法实际上就是基于官方解法的一种写法，也就是通过这个题目中的状态的转移来列DP方程把。然后基于这种分析方式的话，对于这道题来说，是一个通用的列方程的思路把。 实际上总结一下这些状态转移就是 base case: dp[-1][k][0] = dp[i][0][0] = 0; dp[-1][k][1] = dp[i][0][1] = -INT_MAX; Transfer: dp[i][k][1] = max(dp[i-1][k][1], dp[i-1][k][0] - prices[i]); dp[i][k][0] = max(dp[i-1][k][0], dp[i-1][k][1] + prices[i]); 按照这个框架去进行遍历就完事了，6题都可以按照这个框架去写，真的就是通用解，但是时间效率好像也不是特别高，但是需要注意的是，当k是无穷我们就不用特别的去约束，但是当k是有限值的时候，我们就记得要对k进行遍历，内层循环。 还有更为关键的点是（唯一的Hard，买卖的最佳时期的第三题）也就是只有两种k的时候，我们实际上可以重新分析这个状态转移，也就是，重新构筑这个状态表，改变一下原本的迭代形式。 另一点就是，我实在是不想用new，怎么用vector来建立这样一个3元组呢？到时候看看有没有示例代码；（pair？tuple？or something else？）官方直接用两个向量来表示了 由于我们最多可以完成两笔交易，因此在任意一天结束之后，我们会处于以下五个状态中的一种： 1.未进行过任何操作； 2.只进行过一次买操作； 3.进行了一次买操作和一次卖操作，即完成了一笔交易； 4.在完成了一笔交易的前提下，进行了第二次买操作； 5.完成了全部两笔交易。 class Solution { public: int maxProfit(vector& prices) { if(prices.empty()) return 0; int n = prices.size(); int s1 = -prices[0]; int s2 = 0; //我可以不买，所以一定是0不是int——min int s3 = INT_MIN; int s4 = 0; for(int i =1;i 限制了K次的情况,这里实现的时候有很多的细节，但是我觉得基于k的哪个东西有点不合理，所以我们试着用FA中的思路后面重写一下这个框架，其实要改的地方也不是特别多，稍微改几个象征值就可以了。 class Solution { public: int maxProfit(int k, vector& prices) { if(prices.empty()) return 0; int n = prices.size(); // BUY 表示已经持有股票的情况 // SELL 表示手上啥都没有的情况 k = min(k,n/2); vector> DPBuy(n,vector(k+1)); vector> DPSell(n,vector(k+1,0)); // basecase; DPBuy[0][0] = -prices[0]; for(int i = 1;i 加税的方法的话，就是要记得如果是在buy的时候扣税，记得在初始化的那次也要扣。在购买的时候扣就不用了。 打家劫舍问题 实际上是动态规划的问题，以及一些约束情况下的变体，我们要掌握到其中的精髓，进行分析； 首先对动态规划的问题进行分析的时候我们都要好好的想一下到底是要用一维的表还是用二维的表；（不要使用冗余的操作） 然后还是一样的进行存储空间的压缩就行； 打家劫舍1（198） 这一题的分析很容易可以知道是这样的情况：（我们可以反向分析，这样的话，就不需要修改Loop的方向了） 具体的代码实现和相应的空间优化后的结果如下：（100 98） int rob(vector& nums) { if(nums.empty()) return 0; // 两个相邻的房屋无法取到实际上就是一个简单的状态转移的问题； int n = nums.size(); // vector DP(n+1,0); // 实际上就是一个单项的偷盗图 // BaseCases // DP[0] = 0; // DP[1] =nums[0]; // for(int i=2;i 打家劫舍Ⅱ 第二题和第一题的区别就在于，这个房子是围成一圈的，所以就是，首尾相连的序列，（这应该会使得边界条件更加的复杂，或者引入新的约束） 问题分析：这题仔细分析以后实际上可以发现就是要么是不包含头，要么是不包含尾（为0），两个取最大值就好了。稍微修改 打家劫舍Ⅲ 这一题的特点在于房屋的分布是二叉树，最基本的思想还是这样就是，不买就跳下一级，买就跳两级，DP存储就行； 但是这一题还有一个更加精妙的解法，就是像股票那样，存储该节点购买和不购买的两种情况，通过这种情况进行转移函数就行，这题的话参考官方解法的写法更清楚，而且还有后续优化的结果。 常规解法（my） class Solution { private: unordered_map DP; public: int rob(TreeNode* root) { if(!root) return 0; if(DP.find(root)!= DP.end()) return DP[root]; // if(DP.contains(root)) return DP[root]; int robres = rob(root->left)+ rob(root->right); int unrobres = root->val; unrobres += !root->left? 0: (rob(root->left->left) +rob(root->left->right)); unrobres += !root->right? 0: (rob(root->right->left) + rob(root->right->right)); int res = max(robres, unrobres); DP[root] = res; return res; } }; 1. 结合了后续遍历框架：因为我们要知道后续的值才能对前序的值进行处理，所以我们需要先遍历后面的 我们可以用 f(o) 表示选择 o 节点的情况下，o 节点的子树上被选择的节点的最大权值和；g(o) 表示不选择 o 节点的情况下，o 节点的子树上被选择的节点的最大权值和；l 和 r 代表 o 的左右孩子。 class Solution { public: unordered_map f, g; void dfs(TreeNode* node) { if (!node) { return; } dfs(node->left); dfs(node->right); f[node] = node->val + g[node->left] + g[node->right]; g[node] = max(f[node->left], g[node->left]) + max(f[node->right], g[node->right]); } int rob(TreeNode* root) { dfs(root); return max(f[root], g[root]); } }; 这题的优化写法十分的值得参考，我们如何利用我们自己创造的数据结构,实际上和股票的是一样的也就是传输之前的买和没买的问题 struct SubtreeStatus { int selected; int notSelected; }; class Solution { public: SubtreeStatus dfs(TreeNode* node) { if (!node) { return {0, 0}; } auto l = dfs(node->left); auto r = dfs(node->right); int selected = node->val + l.notSelected + r.notSelected; int notSelected = max(l.selected, l.notSelected) + max(r.selected, r.notSelected); return {selected, notSelected}; } int rob(TreeNode* root) { auto rootStatus = dfs(root); return max(rootStatus.selected, rootStatus.notSelected); } }; 回文问题终结版：最小代价构造回文串（1312） 是一个非常典型的动态规划的问题，这种子串的问题通常就是基于二维的DP Table去做，那么实现上就是，存储的就是从i,j的字符，构造成回文串的最少次数。 但是这一题有个陷阱，通常来说，我们分析回文串的问题都是从中间向两端拓展的，但是如果我们每次拓展都直接判断两端的拓展是否相等的话，（+2）这样对于数组完全相等，只需要加1的情况就缺乏了考虑。 解决的方式 我们只需要一边一边的加就可以了 状态转移方程 = max 左或右 +1； 基本的解决方法(可以在算法的基础上进行空间压缩，只需要一个向量 class Solution { public: int minInsertions(string s) { if(s.empty()) return 0; int n = s.size(); // 建立存储表和初始化参数（单个或者是边界都是0） vector> DP(n,vector(n,0)); // 遍历方向，从从下到上，从左到右 for(int i=n-2; i>=0; i--){ for(int j=i+1; j 数据压缩后的结果，这个压缩方案实际上比较常见，用代码取解读的话也比较好解读 if(s.empty()) return 0; int n = s.size(); vector DP(n,0); for(int i=n-2; i>=0; i--){ int pre = 0; for(int j=i+1; j 1.1.9. 贪心算法：动态规划的特例 FA中对贪心算法的简单讲解 贪心算法实际上是动态规划中每一步都取最优结果的特例，实际上满足这种条件的问题并不是太多，但是这种情况下的效率是更高的。但是想博弈问题这种就不能使用贪心算法。 举个例子，典型的贪心算法： 本文解决一个很经典的贪心算法问题 Interval Scheduling（区间调度问题）。给你很多形如[start,end]的闭区间，请你设计一个算法，算出这些区间中最多有几个互不相交的区间。 也许我们可以每次选择可选区间中开始最早的那个？但是可能存在某些区间开始很早，但是很长，使得我们错误地错过了一些短的区间。 或者我们每次选择可选区间中最短的那个？或者选择出现冲突最少的那个区间？这些方案都能很容易举出反例，不是正确的方案。 正确的思路（怎么去贪）其实很简单，可以分为以下三步： 从区间集合 intvs 中选择一个区间 x，这个 x 是在当前所有区间中结束最早的（end 最小）。 把所有与 x 区间相交的区间从区间集合 intvs 中删除。 重复步骤 1 和 2，直到 intvs 为空为止。之前选出的那些 x 就是最大不相交子集。 // 代码的具体实现大致如下： sort (array1, [](const int& A, const int& B){return B[1]>A[1];}); //然后遍历通过尾巴来选就行了。 无重叠区间（435） 实际上和上面的分析是一模一样的情况，就是return的计数值不一样而已。 无需多言 class Solution { public: int eraseOverlapIntervals(vector>& intervals) { if(intervals.empty()) return 0; sort(intervals.begin(),intervals.end(),\\ [](const auto& A,const auto& B){return A[1]=eol){ eol = list[1]; count++; } } return intervals.size() - count; } }; 用最少数量的箭社保气球（452） 实际上和上一题一摸一样，就是题目描述不一样罢了，同时边界擦伤的条件也不一样，改个下小于等于就行了。 class Solution { public: int findMinArrowShots(vector>& points) { if(points.empty()) return 0; sort(points.begin(),points.end(),\\ [](const auto& A,const auto& B){return A[1]eol){ eol = list[1]; count++; } } return count; } }; JUMP GAME(55) 很简单的动态规划的思路，需要注意的只要那个break不能直接return，考虑[0]的情况。 1.这题主要需要我们脑子清醒，很明晰那，《最远距离的就是我们的所有可达点（就算有0也会被绕过。）， 2.除非我们可达点无法再前进了（我们能走到的地方《=当前战力的位置），我们才false，不然我们就能一直前进到终点 class Solution { public: bool canJump(vector& nums) { int n = nums.size(); int farthest = 0; for (int i=0;i=(n-1); } }; 这实际上还是动态规划，实际的贪心算法在后面的进阶版。 JUMP GAME2（45） 这一题如果用基本的动态规划思想的话，自底向上和自顶向下都比较好些，就是存储，到当前格子需要的最少步数就可以了。 我的自底向上的思路： class Solution { public: int jump(vector& nums) { // 后续可以尝试一下自顶向下的写法，这种写法比较傻逼，但是也可以尝试掌握 // 这题不用贪心算法的话，后面有个很恶心的东西不让做 // 下面这是基本的自底向上的方法， vector DP (nums.size(),INT_MAX); DP[0]=0; // DP 存储到到这里需要多少步 for(int i =0; i 自顶向下的思路： 从尾到头，走到尾巴需要多少步。深度有限遍历把，实际上是一种递归的解法、传统的递归分析方法，回顾一下递归的思路。 class Solution { public: vector memo; int jump(vector& nums) { int n = nums.size(); // 备忘录都初始化为 n，相当于 INT_MAX // 因为从 0 调到 n - 1 最多 n - 1 步 memo = vector(n, n); return dp(nums, 0); } int dp(vector& nums, int p) { int n = nums.size(); // base case if (p >= n - 1) { return 0; } // 子问题已经计算过 if (memo[p] != n) { return memo[p]; } int steps = nums[p]; // 你可以选择跳 1 步，2 步... for (int i = 1; i 上面两种方法虽然都可以事项，但是时间复杂度上出了问题，没办法再有效的时间内解决这个，往往会超出时间闲置。所以实际上是需要贪心算法的。 怎么实现贪心的思路呢？ 但是，真的需要「递归地」计算出每一个子问题的结果，然后求最值吗？直观地想一想，似乎不需要递归，只需要判断哪一个选择最具有「潜力」即可： 我们完全可以跳过那些被包含的情况，所以YOU KNOW 具体实现：（学） 这里主要是我们怎么去递增那个end可以学一下，其他的没啥，很容易想到这个贪心的思路 class Solution { public: int jump(vector& nums) { // 后续可以尝试一下自顶向下的写法，这种写法比较傻逼，但是也可以尝试掌握 // 这题不用贪心算法的话，后面有个很恶心的东西不让做 // 下面这是基本的自底向上的方法， int n = nums.size(); int end = 0, farthest = 0; int jumps = 0; for (int i = 0; i 1.1.10. KMP算法：动态规划下属 著名的字符串匹配算法 效率很高，但是确实比较复杂； 先在开头约定，本文用pat表示模式串，长度为M，txt表示文本串，长度为N。KMP 算法是在txt中查找子串pat，如果存在，返回这个子串的起始索引，否则返回 -1。 这个题目时要匹配完全一致的，也就是顺序不能打乱或者跳过的那种子串 遍历的解法如下所示：（伪代码） int search(String pat, String txt) { int M = pat.length; int N = txt.length; for (int i = 0; i 但是这样就有很多完全不需要考虑的不可能的情况的计算无法跳过了，所以我们希望使用一些存储空间来辅助算法的进行。 KMP特点永不回退指针i，不走回头路，也就是不会对txt进行重复的多次扫描，会利用DP数组中的信息将pat移到正确的位置来继续匹配。 那么这个数组如何构建呢？（确定有限状态自动机） 这个DP只与pat相关，与Txt没有任何关系 实际上就是构建状态转移图，然后根据状态转移图来跳转：（这里省略了到0） 这个DP数组的定义方式： dp[j][c] = next 0 根据上面的数组可以构建出这样的状态转移过程 public int search(String txt) { int M = pat.length(); int N = txt.length(); // pat 的初始态为 0 int j = 0; for (int i = 0; i 所以整个DP数组的构建状态： for 0 实际上就是一种匹配和另一种回退的状态变迁，但是这种状态回退该怎么设置，影子状态的思想： 和当前的状态具有相同的前缀的状态就是影子状态（类似双指针算法用来辅助）那么具体怎么实现呢？前缀的长度？ 就可以改进上面的代码如下： int X # 影子状态 for 0 完整的最终代码如下 public class KMP { private int[][] dp; private String pat; public KMP(String pat) { this.pat = pat; int M = pat.length(); // dp[状态][字符] = 下个状态 dp = new int[M][256]; // base case，也就是遇到了第一个字符才能转移到1 dp[0][pat.charAt(0)] = 1; // 影子状态 X 初始为 0 int X = 0; // 当前状态 j 从 1 开始 for (int j = 1; j 这里解释一下影子的更新具体是如何做到，这里从文章中抄一下 这里的状态如下： int X = 0; for (int j = 1; j 其中的原理非常微妙，注意代码中 for 循环的变量初始值，可以这样理解：后者是在txt中匹配pat，前者是在pat中匹配pat[1:]，状态X总是落后状态j一个状态，与j具有最长的相同前缀。所以我把X比喻为影子状态，似乎也有一点贴切。 另外，构建 dp 数组是根据 base casedp[0][..]向后推演。这就是我认为 KMP 算法就是一种动态规划算法的原因。 :star: 下面来看一下状态转移图的完整构造过程，你就能理解状态X作用之精妙了： public class KMP { private int[][] dp; private String pat; public KMP(String pat) { this.pat = pat; int M = pat.length(); // dp[状态][字符] = 下个状态 dp = new int[M][256]; // base case dp[0][pat.charAt(0)] = 1; // 影子状态 X 初始为 0 int X = 0; // 构建状态转移图（稍改的更紧凑了） for (int j = 1; j 1.1.11. 回溯算法详解 我们的目标是“没有蛀牙”，框架化回溯算法：实际上我好像也做了很多和回溯相关的题目了，和这里的思路对照一下； 解决一个回溯问题，实际上就是一个决策树的遍历过程。只需要考虑三个问题 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 具体代码框架：其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」 我觉得这里应该就是存储一个中间结果来做把。通过具体的代码来看看 result = [] def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) return for 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 全排列问题： 我们只要在递归之前做出选择，在递归之后撤销刚才的选择，就能正确得到每个节点的选择列表和路径。（实际上这种通常和DFS并在一起用，先遍历了某个选择下的所有可能，然后回到之前的这个节点） 但是vector没有find之类的函数，所以我们可以通过index 和 swap操作区是实现这样的假装删除， class Solution { public: void backtrack(vector>& res, vector& output, int first, int len){ // 所有数都填完了 if (first == len) { res.emplace_back(output); return; } for (int i = first; i > permute(vector& nums) { vector > res; backtrack(res, nums, 0, (int)nums.size()); return res; } }; N皇后问题： 实际上是一种特殊的全排列问题，我采取的方式是从上到下进行构建，这样的话判断valid以及遍历过程会清晰一些，同时我们每次只需要做一个单行的loop就行了，实际上就是一个典型的回溯问题，其中有一些需要注意的点 数据类型的使用，边界条件的约束，还有就是，数据变量的生存周期； class Solution { private: vector> res; public: vector> solveNQueens(int n) { // 实际上就是全排列啊，不满足再取消就行了。 // 问题在于怎么设置判断和返回条件。 // 棋盘初始化，我们一列一列的开始遍历。 vectorboard(n, string(n,'.')); backtrack(board,0); return res; } void backtrack(vector& board, int row){ int n = board.size(); if(row>=n) return; for(int j=0;j& board, int row, int col){ /* 对三种情况进行判断，上方，左前方和右前方 由于我们是从上往下添加的，所以下方的情况暂时不用考虑 */ int n = board.size(); // situation1 for(int i = row; i>=0; i--){ if(board[i][col] == 'Q') return false; } // situation2 左上角 int offset = 0; for(int i = row; i>=0; i--){ if(col-offset=0; i--){ if(col+offset>=n) break; if(board[i][col+offset] == 'Q') return false; offset++; } return true; } }; 子集问题 通过回溯或者迭代的方式来搜索子集，是一个典型但是实际上难度不是特别大的题目，可以用作基本的思路回顾。 class Solution { private: vector> res; public: vector> subsets(vector& nums) { /* 实际上这题可以使用2.5种不同的思路去解决 1. 递归法：由于重叠子问题的存在和特殊性，（子问题的每一项加上新加入的一项） 2. 回溯法：前序，也就是不断后移动start，然后逐个加入 后续，使用pop出来的值来用（不是特别贴切）还是前一种好*/ // 递归法 // if(nums.empty()) return {{}}; // int n = nums.back(); // nums.pop_back(); // vector> res = subsets(nums); // int size = res.size(); // for(int i = 0; i tempv; int start = 0; backtrack(nums,tempv,0); return res; } void backtrack(vector& nums,vector& tempv, int start){ res.push_back(tempv); int n = nums.size(); for(int i=start; i 组合问题 实际上也就是典型的回溯框架，由于我们的[1,2]和[2,1]算是重复的两种情况，所以我们和上面一样用start来进行约束就可以了。就能避免重复组合的情况发生，还是在上面的框架上稍微改改. 这题实际上和上面的排列问题是一对，可以对照着看，看看两个方法编写上的不同之处和相同之处，实际上就是同个框架下的不同写法。 #include using namespace std; class Solution { private: vector> res; public: vector> combine(int n, int k) { if (k tempv; backtrack(n,k,0 ,tempv); return res; } void backtrack(int n, int k, int start,vector tempv){ if(tempv.size() == k) { res.emplace_back(tempv); return; } for(int i =start; i 解数独： 整体上，我非常的不同意这个作者写的代码，因为我觉得她的两个循环实际上是多余的，他的状态转移是有问题的。 由于一开始有是否有值的判断，所以那个for循环勉强还是可以的把，但是我觉得我的写法更好一些，这一题的主要问题在于。 如何终止搜索过程！，如何在找到一个解后就安然离去！！仔细思考。 还有那个else为什么很重要！！，没有这个else我们可能会讲原值给改了。 #include using namespace std; class Solution { public: void solveSudoku(vector>& board) { backtrack(board,0,0); } bool backtrack(vector>& board, int r, int c){ // 这里的这个逻辑我有点没搞清楚，为什么要全进行m-n的遍历，我觉得是多余的 // 首先继续宁测试，如果不对再说 if(r==9) return true; // 判断下一个位置： int nextr = r, nextc =c; nextc++; if(nextc>=9) { nextr++; nextc=0;} // 判断返回值 if(board[r][c]!='.') { // if(backtrack(board,nextr,nextc)) return true; else return false; } for(char ch = '1'; ch>& board, int r, int c, char ch){ // 从上到下填，但是我们还是需要搜索整张棋盘， for (int i = 0; i 合法括号生成：（22）回溯算法最佳实践 实际上的关键还是在于问题分析的部分： 1、一个「合法」括号组合的左括号数量一定等于右括号数量，这个显而易见。 2、对于一个「合法」的括号字符串组合p，必然对于任何0 都有：子串p[0..i]中左括号的数量都大于或等于右括号的数量。 然后就可以使用 回溯的基本框架来完成代码了。 结束的技巧和之前稍微有点不同可以学一下。 class Solution { public: vector generateParenthesis(int n) { if (n == 0) return {}; // 记录所有合法的括号组合 vector res; // 回溯过程中的路径 string track; // 可用的左括号和右括号数量初始化为 n backtrack(n, n, track, res); return res; } // 可用的左括号数量为 left 个，可用的右括号数量为 rgiht 个 void backtrack(int left, int right, string& track, vector& res) { // 若左括号剩下的多，说明不合法 if (right 1.1.12. BFS算法详解 BFS和DFS框架，实际上DFS在回溯算法中写了很多了，这实际上就是一回事，NOW我们来学习一下BFS。 BFS 的核心思想应该不难理解的，就是把一些问题抽象成图，从一个点开始，向四周开始扩散。一般来说，我们写 BFS 算法都是用「队列」这种数据结构，每次将一个节点周围的所有节点加入队列。 *BFS 相对 DFS 的最主要的区别是：BFS 找到的路径一定是最短的，但代价就是空间复杂度比 DFS 大很多*。 问题的本质（广义描述）是： 在一幅「图」中找到从起点start到终点target的最近距离，这个例子听起来很枯燥，但是 BFS 算法问题其实都是在干这个事儿。 比如走迷宫，有的格子是围墙不能走，从起点到终点的最短距离是多少？如果这个迷宫带「传送门」可以瞬间传送呢？ 再比如说两个单词，要求你通过某些替换，把其中一个变成另一个，每次只能替换一个字符，最少要替换几次？ 再比如说连连看游戏，两个方块消除的条件不仅仅是图案相同，还得保证两个方块之间的最短连线不能多于两个拐点。你玩连连看，点击两个坐标，游戏是如何判断它俩的最短连线有几个拐点的？ 实现框架伪代码 这个框架后面我们后面需要修正一下，便于贴切现在的版本，用我们自己的形式重新总结一下体系架构。 // 计算从起点 start 到终点 target 的最近距离 int BFS(Node start, Node target) { Queue q; // 核心数据结构 Set visited; // 避免走回头路 q.offer(start); // 将起点加入队列 visited.add(start); int step = 0; // 记录扩散的步数 while (q not empty) { int sz = q.size(); /* 将当前队列中的所有节点向四周扩散 */ for (int i = 0; i 其中adj：相邻节点；visited：不会走回头路，比如说二叉树之类的，就不需要这种结构。 二叉树的最小高度（111） 首先这题实际上使用二叉树的普通遍历框架就已经能解决了，我们先给出一个常规解法的答案在这,然后我们便开始正经探讨到底要怎么解决这个问题，使用BFS的框架，我们现在使用的实际上仍然是DFS。 用这题来对DFS和BFS两种框架的解题思路进行比对和分析。 class Solution { private: int res = INT_MAX; public: int minDepth(TreeNode* root) { if(!root) return 0; helpFind(root,0); return res; } void helpFind(TreeNode* root, int step){ // 是没有子节点的节点，也就是左右节点都没有要注意区分 if(!root){return;} step++; if(!root->left && !root->right) { if(stepright,step); helpFind(root->left,step); } }; BFS解题框架 深度优先搜索，就是我之前希望的搜索思路，这样能够更快的结束这样的搜索过程，就能取得更高的时间和空间效率，在这里我们也对原本的解题框架，进行了一个修正。 利用patr 来取代depth的设置。 原本框架中的循环好像是么必要的把。（必要的情况分析：如果需要一层一层的做的话，也就是深度+1在层次之外？）oh 不，就是利用pair就能解决这个问题，不需要这样一个循环了，也就是我们要明确深度+1的过程在哪里执行，当然他那样好像会更节省空间吧， class Solution { private: // int res = INT_MAX; public: int minDepth(TreeNode* root) { if(!root) return 0; queue> q; // 初始化队列起点 q.emplace(root,1); while(!q.empty()){ // int sz = q.size(); // for(int i=0;iright && !temp->left) return depth; if(temp->left != nullptr) q.emplace(temp->left, depth+1); if(temp->right != nullptr) q.emplace(temp->right,depth+1); q.pop(); // } } return 0; } }; 打开转盘锁（752） 这一题我觉得是可以使用回溯算法来做，但是好像标准的解法就是使用BFS的方法去做，这样做的话，我按照原本的框架写出了如下的代码，但是这样的代码的时间复杂度超过了题目要求的水准，所以是算法的优化还能提升还是说是其他写法的BFS问题呢？ 可以优化：通过Unorder_set 来优化搜索的过程。具体的改进已经写进代码里了。 实际上就是看成一张图，然后同个时刻能做出的选择有8个，4个转盘分别从两边转，然后我们就看成是图传播，在解决一下重复和deadends就行了。 class Solution { public: int openLock(vector& deadends, string target) { // 使用回溯法遍历所有的可能性,同时做出特殊的终结判断 // 同时维护一个最小值,然后最终返回. // 正确理解题目的意思(每次只能转动单个转盘,而且只能移动一格) // 为什么使用BFS,最短路径,找到了就提前终止,同时是路径问题 if (target.empty()) return -1; // 非法情况约束 //初始化搜索队列 // FIXME: 不走回头路这点怎么实现。hash_set? queue> que; unordered_set visited; unordered_set dead(deadends.begin(),deadends.end()); que.emplace(\"0000\", 0); // visited.insert(\"0000\"); while (!que.empty()) { //基于多个方向搜索。 // FIXME: deadends 还没有建立排除的机制。 string temp = que.front().first; int depth = que.front().second; if (temp == target) return depth; que.pop(); if (visited.find(temp) != visited.end()) continue; //if (containsDeadend(temp, deadends)) continue; if(dead.find(temp) != dead.end()) continue; visited.insert(temp); for (int i = -4; i 0 && posandPN = -4) { if (s1[-posandPN-1] == '0') s1[-posandPN-1] = '9'; else s1[-posandPN-1] -= 1; } return s1; } // bool containsDeadend(const string& s1, vector& deadends) { // for (string temp : deadends) { // if (s1 == temp) return true; // } // return false; // } }; 双向BFS优化 当我们知道target所在的位置(基本的前提)的时候，我们可以使用双向的BFS策略，也就是target搜索begin，begin搜索starget同时进行，这样的方式在时间效率上会快的，原因如下： 传统的 BFS 框架就是从起点开始向四周扩散，遇到终点时停止；而双向 BFS 则是从起点和终点同时开始扩散，当两边有交集的时候停止。 综上所述，这样的算法只能在打开转盘锁的时候使用，因为我们知道终点在哪里，但是这样就设计到一个问题，就是我们好像不能再使用队列了，我们需要用set来判断我们是否产生了交集？ 使用unorder_set结构 交替更新，但是这里的方法没有delete，我感觉还是多少有泄露的可能性，感觉还是要修改后再使用，但是基本思路还是比较清楚的。应该把new改成clear，然后再换把。 基本的框架如下： int openLock(String[] deadends, String target) { Set deads = new HashSet<>(); for (String s : deadends) deads.add(s); // 用集合不用队列，可以快速判断元素是否存在 Set q1 = new HashSet<>(); Set q2 = new HashSet<>(); Set visited = new HashSet<>(); int step = 0; q1.add(\"0000\"); q2.add(target); while (!q1.isEmpty() && !q2.isEmpty()) { // 哈希集合在遍历的过程中不能修改，用 temp 存储扩散结果 Set temp = new HashSet<>(); /* 将 q1 中的所有节点向周围扩散 */ for (String cur : q1) { /* 判断是否到达终点 */ if (deads.contains(cur)) continue; if (q2.contains(cur)) return step; visited.add(cur); // 实际上原节点都可以不用加，第一次重叠一定是在扩散之后的值和另一个集有重叠的时候把。 /* 将一个节点的未遍历相邻节点加入集合 */ for (int j = 0; j 对于双向的BFS，还有一个优化是，我们下一次的扩散方向是根据现在集合比较小的那个来扩散的。 滑动谜题 利用BFS来进行暴力穷举，我们只需要每次找到0的位置，然后和上面一样，列出0能做的所有单步决策就行了，然后用队列和防止重复的方法来解决他，但是其中有一些小技巧值得我们学习： 直接列出所有情况下可选择的邻居来加快算法的速度 将2维降维到1维，用string或者其他的一维向量去做会更方便一点。 int slidingPuzzle(vector>& board) { int m = 2, n = 3; string start = \"\"; string target = \"123450\"; // 将 2x3 的数组转化成字符串 for (int i = 0; i > neighbor = { { 1, 3 }, { 0, 4, 2 }, { 1, 5 }, { 0, 4 }, { 3, 1, 5 }, { 4, 2 } }; /******* BFS 算法框架开始 *******/ queue q; unordered_set visited; q.push(start); visited.insert(start); int step = 0; while (!q.empty()) { int sz = q.size(); for (int i = 0; i 1.1.13. 双指针使用技巧总结 双指针技巧使用汇总帖 快慢指针的用法和用途： 是否有环：相遇可以判定有环； 找到环的起始点：相遇后，把一个调到头，同速前进，再次相遇即是起始点。（或者先停下一个，然后让另一个从头开始走） 链表的中点：快慢指针，快指针到达终点。 延申问题：对链表进行归并排序，通过快慢指针实现二分的操作，合并两个有序链表。 起始点偏差：先让一个指针走k步，另一个指针再出发，寻找链表的倒数第k个元素 快慢指针的常用算法： 二分查找算法，没啥好说的 子数组之和：只要数组有序，就要想到双指针技巧。通过调节left和right来调整sum的大小。找到对应的区间 反转数组：从前或从后出发，然后直接互换。 下面讲的滑动窗口 双指针技巧例题： 26 & 83有序数组链表去重 使用快慢指针，遇到不一样的时候往下一个赋值就可以了，链表的话就是修改一下赋值操作，没什么太大的区别。 27 移除元素 使用双指针技巧，从两端向内，找到不是val的和val交换即可。 class Solution { public: int removeElement(vector& nums, int val) { if(nums.empty()) return {}; int n = nums.size(); int be = 0, en =n-1; // 使用双指针法解决这个问题，也就是不一样的时候进行交换 while(be 移动0：283 这题要注意是进行删除，然后再后面添加0，和之前的交换不一样，交换会破坏顺序。 class Solution { public: void moveZeroes(vector& nums) { if(nums.empty()) return ; int n = nums.size(); int slow=0, fast =0; // 使用双指针法解决这个问题，也就是不一样的时候进行交换 while(fast 1.1.14. 滑动窗口算法 思路总结 链表子串数组题：就直接考虑双指针的方法去做，双指针基本可以总结成以下的3中类型。 快慢指针：链表操作，归并排序找中点，链表成环搞判定； 左右指针：反转数组，二分搜索 滑动窗口：字串问题，左右指针滑动，前后并进 滑动窗口的基本框架 1、我们在字符串S中使用双指针中的左右指针技巧，初始化left = right = 0，把索引左闭右开区间[left, right)称为一个「窗口」。 2、我们先不断地增加right指针扩大窗口[left, right)，直到窗口中的字符串符合要求（包含了T中的所有字符）。 3、此时，我们停止增加right，转而不断增加left指针缩小窗口[left, right)，直到窗口中的字符串不再符合要求（不包含T中的所有字符了）。同时，每次增加left，我们都要更新一轮结果。 4、重复第 2 和第 3 步，直到right到达字符串S的尽头。 理解向：思想很简单，但是主要是逻辑上容易出现一些bug和问题：1. 如何添加缩小，2.在哪更新结果 int left = 0, right = 0; while (right 实现向：具体实现的框架，考虑了边界问题的方法，实际上也没什么说嘛，就是更具体一点，输入操作的位置肯定是这样啊，没什么好说的。 /* 滑动窗口算法框架 */ void slidingWindow(string s, string t) { unordered_map need, window; for (char c : t) need[c]++; // 初始化状态，便于搜索 int left = 0, right = 0; int valid = 0; // 统计满足情况的数有多少，和需要的匹配时更新答案 while (right 典型例题1：76 最小覆盖子串问题，思路分析： 不断的右移扩大窗口，当满足条件以后左指针右移，优化结果，直到第一次不满足，每次移动左指针都更新答案。 直到右指针超出边缘以后结束。 实现上由于是第一题，给出代码，但是后面的题就到相应的代码文件中去找吧： class Solution { public: string minWindow(string s, string t) { if(s.empty() || s.size() need,windows; for(char temp: t) need[temp]++; //存储所有需要的字符 // 初始化双指针和判断指针 int right=0, left=0; int valid =0; int n = need.size(); // 存放结果 int len = INT_MAX; int s_index = 0; while(right 典型例题：567字符串排列 这题和上面一题的区别就在于“只包含需要的所有元素” ，并且计数一致（因为是存在排列，所以中间不能有其他的元素） 这题我的思路是直接不断移动right，然后当right遍历到不同的元素的时候，或者需要元素的count不同的时候，直接--，但是这样实际上还是有一定的问题的，比如对于windows中的元素--实际上并不好操作。但是这个思路的计算和遍历思路实际上比框架的快，也不一定，因为要遍历--； FA：右侧递进实际上还是一样的，但是左侧应该是当size不同的时候，要一直遍历到相等。因为是排列，所以长度要想等 因为这里用的是小于等于的时候都要进入循环，所以第一次发生判断的时候只可能是长度相等的时候。 典型例题：438所有的字母yiweici排列 和上一题一样，只是要存储所有的start，所以判断的热res修改一下就好了。 典型例题：3 最小不重复子串 实际上就是终止条件（left），每个字符的count都只有1，不然就left一直++； 然后更新就行maxlen就行。 1.1.15. 分治算法详解 FA的作者认为可以将回溯，分治和动态规划放到一起，实际上都是一种特殊的递归。 回溯算法就一种简单粗暴的算法技巧，说白了就是一个暴力穷举算法，比如让你 用回溯算法求子集、全排列、组合，你就穷举呗，就考你会不会漏掉或者多算某些情况。 动态规划是一类算法问题，肯定是让你求最值的。因为动态规划问题拥有 最优子结构，可以通过状态转移方程从小规模的子问题最优解推导出大规模问题的最优解。 分治算法呢，可以认为是一种算法思想，通过将原问题分解成小规模的子问题，然后根据子问题的结果构造出原问题的答案。这里有点类似动态规划，所以说运用分治算法也需要满足一些条件，你的原问题结果应该可以通过合并子问题结果来计算。 最经典的分支框架，归并排序： void sort(int[] nums, int lo, int hi) { int mid = (lo + hi) / 2; /****** 分 ******/ // 对数组的两部分分别排序 sort(nums, lo, mid); sort(nums, mid + 1, hi); /****** 治 ******/ // 合并两个排好序的子数组 merge(nums, lo, mid, hi); } 为运算表达式设计优先级241 实际上就是考虑所有可能的添加括号的方式，还要考虑括号的合法性和计算的优先级问题； 实际上没有我们考虑的那么复杂，当我们只加一个括号的时候，我们只需要针对单一的运算符号进行分割就好了，其他的情况都是可以被归化的， 实际上也没什么特殊的操作，就是通过分和治两部分进行，用分划分成子问题，然后对当前的问题进行解决，这实际上也就是递归做的事情啊，感觉没什么区别，对于特殊的问题实际上也可以使用备忘录来简化操作。 下面这个我写的代码，值得品一品好吧。 class Solution { public: vector diffWaysToCompute(string input) { vector res; int n = input.size(); vectora; vectorb; for(int i = 0; i 1.1.16. 区间问题 所谓区间问题也就是线段问题，合并所有的线段，找出线段的交集等等。主要有两个技巧 排序：常见的有，按照起点升序排序，若起点相同，则按照终点降序排序。 画图：不要偷懒。 典型例题 1288 删除被覆盖区间 这题有个问题我比较疑惑，就是区间竟然还能合并的嘛，fine。解决思路就是，通过起点升序，末端降序，然后首先排除覆盖，遇到区间合并的情况，就更新边界点（left，right），遇到完全不相交的情况，就重新（left，right）； 实际上有类似的问题涉及到贪心的算法选择。 下面这个是我的写法 class Solution { public: int removeCoveredIntervals(vector>& intervals) { if (intervals.empty()) return 0; // 按照第一维度的升序和第二个维度的降序来排列 sort(intervals.begin(), intervals.end(), [](vector& a, vector& b) { if (a[0] != b[0]) return a[0] b[1]; }); int res = 1; // 指定当前的边界 vector cur = intervals[0]; for (int i = 1; i cur[1]) { res++; cur = intervals[i]; } } return res; } }; FA中的傻逼写法？ int removeCoveredIntervals(int[][] intvs) { // 按照起点升序排列，起点相同时降序排列 Arrays.sort(intvs, (a, b) -> { if (a[0] == b[0]) { return b[1] - a[1]; } return a[0] - b[0]; }); // 记录合并区间的起点和终点 int left = intvs[0][0]; int right = intvs[0][1]; int res = 0; for (int i = 1; i = intv[1]) { res++; } // 情况二，找到相交区间，合并 if (right >= intv[0] && right 典型题 56 区间合并 和上一题是一样的 实际上也是按照开始的节点升序排列，后面其实升序降序都可以，按照start和现有的end的关系，看到底是要修改end还是push_back。即可。 我写的代码 class Solution { public: vector> merge(vector>& intervals) { if (intervals.empty()) return {}; // 按照第一维度的升序和第二个维度的降序来排列 sort(intervals.begin(), intervals.end(), [](vector& a, vector& b) { if (a[0] != b[0]) return a[0] b[1]; }); vector> res; vector temp = intervals[0]; // 考虑需要合并的情况分三种情况，覆盖（无需合并） 重叠 ， 更新 for (auto interval:intervals) { //覆盖的可以直接掠过，通过尾部来进行重叠或者更新的判断； if (interval[1] > temp[1]) { // 判断是重叠的情况还是 更新的情况 if (interval[0] > temp[1]) { res.emplace_back(temp); temp = interval; } else { temp[1] = interval[1]; } } } // 还剩下最后一个没有加入更新 res.emplace_back(temp); return res; } }; # intervals 形如 [[1,3],[2,6]...] def merge(intervals): if not intervals: return [] # 按区间的 start 升序排列 intervals.sort(key=lambda intv: intv[0]) res = [] res.append(intervals[0]) for i in range(1, len(intervals)): curr = intervals[i] # res 中最后一个元素的引用 last = res[-1] if curr[0] 典型题 986 区间交集问题 实际上画图很容易找到解决的方案，交集就是max （left） min（right），然后哪个的right小，哪个的index就++，没有别的了。 # A, B 形如 [[0,2],[5,10]...] def intervalIntersection(A, B): i, j = 0, 0 # 双指针 res = [] while i = a1 and a2 >= b1: # 计算出交集，加入 res res.append([max(a1, b1), min(a2, b2)]) # 指针前进 if b2 1.1.17. 排序算法 排序算法最少最少也要nlogn （平均和最差） 最终要对各个 排序算法都要写一下，不管是基本框架还是具体实现，找找对应的题，没有的话再说。 boolean isValidBST(TreeNode root) { return isValidBST(root, null, null); } /* 限定以 root 为根的子树节点必须满足 max.val > root.val > min.val */ boolean isValidBST(TreeNode root, TreeNode min, TreeNode max) { // base case if (root == null) return true; // 若 root.val 不符合 max 和 min 的限制，说明不是合法 BST if (min != null && root.val = max.val) return false; // 限定左子树的最大值是 root.val，右子树的最小值是 root.val return isValidBST(root.left, min, root) && isValidBST(root.right, root, max); } 复杂度分析汇总 排序算法一般在最差情况的下时间复杂度为Ω（n log n）; 书上P151页的表到时候重新扫描一下； 前三种Θ（n^2）的算法：这种算法的瓶颈就在于只比较相邻的元素，因此比较和移动只能一步步进行。交换相邻记录称为一次交换 排序算法 时间复杂度 空间复杂度 插入排序 冒泡排序 选择排序 插入排序： 一个一个输入后面的空位，然后逐步和前面的已经输入的n-1比，“冒泡”合适的位置，逐个进行比较和swap。 冒泡排序： 内层将该轮的最值冒出去，最外层就是冒泡n次就是了。 选择排序： 实际上就是冒泡排序的从最小值开始，但是不是每次都交换，而是固定每个内循环只交换一次，就是先找值后交换而已。 一些好一点的算法： Shell 排序：缩小增量排序 选择适当的增量序列可以使得Shell排序比其他的排序都更有效率；但是选择这个序列是很难的，一般来说选择（1，4，13..）增量每次÷3。Shell不加证明的认为Θ（n^1.5），确实比前面的三种都要快，当n中等规模的时候，也和下面的那些有的比。 基本思想：利用插入排序的最佳时间代价的特性，试图将待排序序列变成近似有序的，然后再利用插入排序来最后排序； 实现逻辑：把序列分成多个子序列，然后分别对子序列进行排序，最后把子序列组合起来。 快速排序：实际上就是二叉树的前序遍历 快速排序的逻辑是，若要对 nums[lo..hi] 进行排序，我们先找一个分界点 p，通过交换元素使得 nums[lo..p-1] 都小于等于 nums[p]，且 nums[p+1..hi] 都大于 nums[p]，然后递归地去 nums[lo..p-1] 和 nums[p+1..hi] 中寻找新的分界点，最后整个数组就被排序了。 执行关键在于partiton划分过程，算法效率在于怎么找到划分节点，最差n^2，平均和最佳都是nlogn // 代码框架，（不是具体实现） /* 快速排序主函数 */ void sort(int[] nums) { // 一般要在这用洗牌算法将 nums 数组打乱， // 以保证较高的效率，我们暂时省略这个细节 sort(nums, 0, nums.length - 1); } /* 快速排序核心逻辑 */ void sort(int[] nums, int lo, int hi) { /****** 前序遍历位置 ******/ // 通过交换元素构建分界点 p int p = partition(nums, lo, hi); /************************/ sort(nums, lo, p - 1); sort(nums, p + 1, hi); } 先构造分界点，然后去左右子数组构造分界点，你看这不就是一个二叉树的前序遍历吗？ int partition(int[] nums, int lo, int hi) { if (lo == hi) return lo; // 将 nums[lo] 作为默认分界点 pivot int pivot = nums[lo]; // j = hi + 1 因为 while 中会先执行 -- int i = lo, j = hi + 1; while (true) { // 保证 nums[lo..i] 都小于 pivot while (nums[++i] pivot) { if (j == lo) break; } if (i >= j) break; // 如果走到这里，一定有： // nums[i] > pivot && nums[j] 归并排序：实际上就是二叉树的后序遍历 再说说归并排序的逻辑，若要对 nums[lo..hi] 进行排序，我们先对 nums[lo..mid] 排序，再对 nums[mid+1..hi] 排序，最后把这两个有序的子数组合并，整个数组就排好序了。 归并排序的代码框架如下： void sort(int[] nums, int lo, int hi) { int mid = (lo + hi) / 2; sort(nums, lo, mid); sort(nums, mid + 1, hi); /****** 后序遍历位置 ******/ // 合并两个排好序的子数组 // 逐渐的比较两个sort的最小值就行了，应该算是O（n）把，遍历一遍就好 merge(nums, lo, mid, hi); /************************/ } 先对左右子数组排序，然后合并（类似合并有序链表的逻辑），你看这是不是二叉树的后序遍历框架？另外，这不就是传说中的分治算法嘛，不过如此呀。 堆排序： 所有情况都是nlogn. 实现的思想：建堆取中心节点，直至堆空；主要就在于建堆和removefirst； 分配排序和基数排序 看书，上网找描述，书上能理解但是描述不清楚。 1.1.18. 题型：数组 用二分查找来解决数组题目 实际上二分查找法的关键就在这一个查找，针对查找问题的这些情况，我们都可以用二分法去做，有一些题目虽然会写的比较隐晦，但是我们看到类似如下的暴力搜索框架的时候，就可以考虑使用二分查找法来优化 for (int i = 0; i 例题：koko吃香蕉；货物运输 我对于二分查找的框架的写法实际上还是没有太清楚，到底是应该+1-1还是怎么去约束，我还是要想清楚再写，看看FA的二分查找框架。 1.1.19. Two Sum 到N Sum问题 two sum实际上就是教我们使用hash-table之类的数据结构去解决这样的需要穷举的问题，或者排序后再使用双指针的问题。我们当然也可以在自定义数据结构，每次添加数字，旧纪录当前所有可能的和，然后再O1进行索引就行了。 简单的TWO-SUM就不再多说了，这里提一下如何实现到NSUM的泛化 N Sum拓展： 首先基于思路还是用sort首先排完序后再用双指针法去做的，实际上更偏向于其中的滑动窗口算法。 # two sum的基本情况 vector> twoSumTarget(vector& nums, int target) { // nums 数组必须有序 sort(nums.begin(), nums.end()); int lo = 0, hi = nums.size() - 1; vector> res; while (lo target) { while (lo 直接 3sum-4sum问题： 简单的思路：穷举，然后判断即可；结合的思路，遍历所有的第一个，然后就转化为2Sum的问题了，为了使得结果不重复，我们需要由于我们的2 sum算法中有避免重复，所以我们就只要保证第一个遍历的数字不要重复即可。 但是这样加入让我们求100 sum的话，我们可以根据上面的方式，总结出一个通用的方程： /* 注意：调用这个函数之前一定要先给 nums 排序 */ vector> nSumTarget( vector& nums, int n, int start, int target) { int sz = nums.size(); vector> res; // 至少是 2Sum，且数组大小不应该小于 n if (n target) { while (lo 2 时，递归计算 (n-1)Sum 的结果 for (int i = start; i > sub = nSumTarget(nums, n - 1, i + 1, target - nums[i]); for (vector& arr : sub) { // (n-1)Sum 加上 nums[i] 就是 nSum arr.push_back(nums[i]); res.push_back(arr); } while (i 1.1.20. Union-Find并查算法 理论基础 解决的是图论中的动态连通性问题，也就是逐渐建立图的连通关系（自反性，传递性，对称性）的时候，以下几种API的实现（参考后面提到的数据结构）。 联通性判断：父节点是否相同。（parent （x）== x） 建立连接：将其中任意一个节点的根节点指向另一个节点的根节点上； 连通分量统计：每次建立连通性的时候-- 使用的数据结构类型：森林（若干树）（每个节点指向其父节点，根节点指向自己） 通过父节点是否一致来进行判断，是否联通，如果根据这个原理的话，那么树的平衡，也就是深度就比较重要了。合理的设计能够降低树的深度，也就能降低搜索父节点的时间消耗：从而减少无论是建立联通还是连通性判断两个部分。 平衡性优化：如何避免Union中树的不平衡现象产生？每次将小树接到大树后面，而不是反过来。 那么我们在每个树种存储相应的size，也就是结点数目，这样在进行Union的时候进行判断就可以了。 class UF { // 连通分量个数 private int count; // 存储一棵树 private int[] parent; // 记录树的“重量” private int[] size; public UF(int n) { this.count = n; parent = new int[n]; size = new int[n]; for (int i = 0; i size[rootQ]) { parent[rootQ] = rootP; size[rootP] += size[rootQ]; } else { parent[rootP] = rootQ; size[rootQ] += size[rootP]; } count--; } public boolean connected(int p, int q) { int rootP = find(p); int rootQ = find(q); return rootP == rootQ; } private int find(int x) { while (parent[x] != x) { // 进行路径压缩 parent[x] = parent[parent[x]]; x = parent[x]; } return x; } } 常数级别路径压缩： 对find函数进行改进，每次在进行find的时候同时进行压缩，添加一行代码即可。所有的树高不会超过3，（union的时候树高可能达到3） :question:在这种情况下平衡判断还重要吗，毕竟find压缩的复杂度已经是O(1)了。 可以不要，基本确实是O(1)但是确实能略微提高运算的效率就是了。 ​ private int find(int x) { while (parent[x] != x) { // 进行路径压缩 parent[x] = parent[parent[x]]; x = parent[x]; } return x; } 实际应用： 考虑到把原问题转化成图的动态连通性的关系，同时有一些小技巧： 将二维数组映射到一维数组； uf.union(x * n + y, i * n + j); 使用方向代码d来简化代码量！ int[][] d = new int[][]{ {1,0}, {0,1}, {0,-1}, {-1,0} }; 很多复杂的DFS都可以使用Union-Find来进行解决 应用1：130被围绕的区域 不那么贴切，也就是130题，围棋问题，完全被围住才能换成x所以边界上是安全的，所以，我们就首先找到和边界上的O联通的所有点，然后把其他的不与这种情况联通的O全部换成X即可。 传统方法，遍历边界，然后从这些O DFS出去。标记为#，然后将其余的O换成X，然后再将#换回来即可。 应用2：判定合法算式 这题实际上就是典型的联通问题，根据等式去建立联通关系，然后根据字符翻译成是否联通即可。具体实现： boolean equationsPossible(String[] equations) { // 26 个英文字母 UF uf = new UF(26); // 先让相等的字母形成连通分量 for (String eq : equations) { if (eq.charAt(1) == '=') { char x = eq.charAt(0); char y = eq.charAt(3); uf.union(x - 'a', y - 'a'); } } // 检查不等关系是否打破相等关系的连通性 for (String eq : equations) { if (eq.charAt(1) == '!') { char x = eq.charAt(0); char y = eq.charAt(3); // 如果相等关系成立，就是逻辑冲突 if (uf.connected(x - 'a', y - 'a')) return false; } } return true; } 1.1.21. 从LRU到LFU LRU：Least recently used 最近使用的就是有用的； LFU：Least frequently used 最频繁使用的是有用的； LRU设计 我的思路如下： 怎么去设计这样一个数据结构，实际上是优先队列把？用一定的规则来设计这样的queue，但是为了要能在O(1) push 和get，我们可以使用hashmap，&存放使用的时序和val，以及一个step指向当前的操作数字，但是push中hashmap的删除涉及到find的操作，需要O（n）。所以不行 正确使用的数据结构应该是：Hash（支持快速索引链表的位置）+双向链表（支持快速的插入和删除）在CPP中使用unordered_map和自定义双向链表来实现双向哈希链表。 Push 需要判断是否超出了边界。 具体代码实现如下： // 这里的关键就在于带有额外头尾节点的双向链表，将删除，移动和乱七八糟的全部分离出来 struct Dlist{ int val, key; Dlist* prev; Dlist* next; Dlist():key(0),val(0),prev(nullptr),next(nullptr){} Dlist(int k, int v):key(k),val(v),prev(nullptr),next(nullptr){} }; class LRUCache { private: int cap,size; unordered_map loc; // 双向链表，存个头尾不过分吧 Dlist* head; Dlist* tail; // 指向最后一个的后一个 public: LRUCache(int capacity):cap(capacity),size(0) { head = new Dlist(); tail = new Dlist(); head->next = tail; tail->prev = head; } int get(int key) { if(!loc.count(key)) return -1; // 最近使用，移到头部，删除原位 Dlist* root = loc[key]; movetohead(root); return root->val; } void put(int key, int value) { // 这里需要一个add和一个remove // 当cap不满的时候我们就直接加到List和Hash中，原值不存在 if(!loc.count(key)){ // 先创建新节点，然后判断cap Dlist* root = new Dlist(key,value); // 容易忘记在hash表中添加 loc[key] = root; addHead(root); size++; if(size > cap){ Dlist* temp = removeTail(); loc.erase(temp->key); // 防止内存泄露 delete temp; --size; } }else{ Dlist* root = loc[key]; root->val = value; movetohead(root); } } void movetohead(Dlist* root){ // 实际上是删除炒作：上一个的下一个和下一个的上一个 // 实际上也是put操作：然后接在头后面 deleteNode(root); addHead(root); } // 在写移动算法的时候附加的操作 void deleteNode(Dlist* root){ root->prev->next = root->next; root->next->prev = root->prev; } // 在写移动算法的时候会归纳出来的操作 void addHead(Dlist* root){ root->next = head->next; root->prev = head; root->next->prev = root; head->next = root; } Dlist* removeTail(){ // 由于tail指向的是最后一个的后一个，实际上我们只要调用delete就行了 Dlist* temp = tail->prev; deleteNode(temp); // return是为了delete方便删除 return temp; } }; LFU设计 LFU相比于LRU来说设计上还是要复杂不少的，首先就是LFU除了维护一个优先队列以外，优先的判断和存储是比较难得，我们怎么样去存放一个决定优先级的freq的数据，然后能够很快的找到需要弹出的freq。这个freq还要能够很快的进行更新就是了。 维护一个freq的优先级，同时freq中也有时序的关系，最新最旧。 其他的和LRU还是挺像的。 这一题还是看看官方的题解把：这题的的两种解法一种set，考虑双哈希的解法； 画张图还是很容易理解的，也就是通过每个freq的一个双向链表，以及hash指向特定key的双结构去做，根据插入到尾部，就能维护到尾部。 两种方法都很有参考价值 1.1.22. 一些其他的算法技巧 接雨水 我的思路：记录变化点，然后减掉区域内的面积，就是雨水的面积，用两个flag可以实现，一个记录变化，一个记录是否成area。 FA解法 暴力解法：对于每个i找到left的最高点，right的最高点，然后选低的哪个，减掉当前坐标即可。 用备忘录优化：需要两个，从左到右的最大，和从右到左的最大，然后按图索骥就可以了 双指针解法：实际上是上一个方法的改进，我们只要知道，无论距离多远，只要一段比较高，就能把低的那边的水给锁住，那么我们只需要一直移动比较低的那一侧就好了，写写看。 判断完美矩形 原来不用自己组合，那有什么难的，面积加端点判断就好了，试着写一下： 断电判断：一个小矩形的端点，如果只有奇数个矩形接触，那就是一个额外的顶点，这样的顶点超过4个就不行 面积就是很简单了。 翻转煎饼 和我想的没什么区别，找到最大的，翻到顶上，然后翻到底下，然后递归； :star:考官调度885： 但凡遇到在动态过程中取最值的要求，肯定要使用有序数据结构，我们常用的数据结构就是二叉堆和平衡二叉搜索树了。 如果将每两个相邻的考生看做线段的两端点，新安排考生就是找最长的线段，然后让该考生在中间把这个线段「二分」，中点就是给他分配的座位。leave(p)其实就是去除端点p，使得相邻两个线段合并为一个。 也就是使用set来做 这是这题的思路，但是我们还是看看官方解答把 实现一个计算器： 通过stack实现加减乘除（遇到符号将前面的数字入栈），遇到左括号进入递归，遇到右括号跳出递归，遇到空格进行处理。 参考链接 反直觉概率： 生男生女都一样 男女这个我持保留态度，性别不应该用年龄来划分空间，这种歧义 生日问题 应该转化为计算每个人的生日都不同。就可以大概计算出来。 三门问题 应该转化为概率浓缩来理解，换门相当于选择了后面两扇门的概率。 随机算法：水塘抽样算法 如何在不知道总数的时候产生均匀的随机数？ 这篇文章的启发性很好，实际上就是我们通过将1/n 换成1/i ，然后再获取到下一个index的时候，做一个保留还是变换的决定. 可以证明，保留的概率为1/i，变换的概率是(i-1)/i; 同理，如果要随机选择k个数，只要在第i个元素处以k/i的概率选择该元素，以1 - k/i的概率保持原有选择即可。代码如下： /* 返回链表中一个随机节点的值 */ int getRandom(ListNode head) { Random r = new Random(); int i = 0, res = 0; ListNode p = head; // while 循环遍历链表 while (p != null) { // 生成一个 [0, i) 之间的整数 // 这个整数等于 0 的概率就是 1/i if (r.nextInt(++i) == 0) { res = p.val; } p = p.next; } return res; } 拓展延伸： 以上的抽样算法时间复杂度是 O(n)，但不是最优的方法，更优化的算法基于几何分布（geometric distribution），时间复杂度为 O(k + klog(n/k))。由于涉及的数学知识比较多，这里就不列出了，有兴趣的读者可以自行搜索一下。 还有一种思路是基于 Fisher–Yates 洗牌算法 的。随机抽取k个元素，等价于对所有元素洗牌，然后选取前k个。只不过，洗牌算法需要对元素的随机访问，所以只能对数组这类支持随机存储的数据结构有效。 另外有一种思路也比较有启发意义：给每一个元素关联一个随机数，然后把每个元素插入一个容量为k的二叉堆（优先级队列）按照配对的随机数进行排序，最后剩下的k个元素也是随机的。 差分数组、前缀和 1109航班预定统计 首先分别介绍一下前缀和和差分数组的定义和作用： 前缀和 简单来说定义为如下形式：便于计算区间内的累加和之类的操作 差分数组 主要使用于对区间内的一定元素进行统一的加减运算； *差分数组的主要适用场景是频繁对原始数组的某个区间的元素进行增减。* 这样构造差分数组diff，就可以快速进行区间增减的操作，如果你想对区间nums[i..j]的元素全部加 3，那么只需要让diff[i] += 3，然后再让diff[j+1] -= 3即可： 最后再又差分数组反推出最终的值就可以了。 具体应用：机票预定 vector corpFlightBookings(vector>& bookings, int n) { // 初始化结果数组 vector res(n,0); if(bookings.empty()) return res; // 构建差分数组,初始就全是0，没问题的 vector diff(n,0); // 差分求解 for(auto book: bookings){ int i = book[0]-1; int j = book[1]-1; int val = book[2]; diff[i]+= val; if(j+1 快排亲兄弟：快速选择算法215 经典问题数组中第k个最大元素 使用二叉堆（优先队列）的解法：显然就是一个针对这种数据结构的问题，我们甚至可以自己写一下这种结构，但是确实是比较麻烦来着。、 使用快速排序的解法： // 优先队列的方法 class Solution { public: int findKthLargest(vector& nums, int k) { // 使用优先队列的方法进行问题的求解 if(nums.empty() || nums.size(),greater> pq; for(auto num:nums){ pq.push(num); if(pq.size()>k) pq.pop(); } return pq.top(); } }; 快速排序的方法和思路 实际上就是不完全的快速排序，使用二分的策略叠在快速排序上，当我们排序到K的时候，就直接return就行，但是为了使得算法不是每次都取到极端情况，我们每次首先将数组进行一次随机的打乱策略。 class Solution { public: int findKthLargest(vector& nums, int k) { if(nums.empty()||nums.size()resindex){ hi = p-1; }else if(p& nums, int lo, int hi){ // 快排的划分,初始化为lo if(lo==hi) return lo; int i = lo; int j = hi+1; int privot = nums[lo]; while(true){ while(nums[++i]=hi) break; } while(nums[--j]>privot){ if(j=j) break; swap(nums[i],nums[j]); } swap(nums[lo],nums[j]); return j; } }; 快速计算素数的个数 通过是否是素数的一个个判断的效率没有我们从下网上填充false快，同时填充的时候注意内层循环和外层循环都能通过sqrt进行优化，外层是因为只需要到sqrt就可以了，内层是平方前面的都是重复的。 直接看写好的代码文件。 super pow 模幂运算 实际上计算的关键就是如下的公式：(用AK+B)之类的假设很容易证明 （a*b)\\% k = (a\\%k)(b\\%k)\\%k; 然后再根据幂运算的乘积性质就很容易了，然后用递归的去做，（分奇偶） a^{M+N} = a^M * a^N \\\\ A^{MN} = A^{M^N} 再进一步的优化得到快速幂算法,证明还是基于上面的假设 a^b \\%c == (a\\%c)^b 这里给出两种，一种是每次去最尾巴那一位，一个是快速幂算法 快速幂算法 int base = 1337; int mypow(int a, int k) { if (k == 0) return 1; a %= base; // 这里和直观的理解上是有所偏差的， // 我如果将这里注释掉，转移到下面的两个a中，还是正确的，但是效率差了点 if (k % 2 == 1) { // k 是奇数 return (a * mypow(a, k - 1)) % base; } else { // k 是偶数 int sub = mypow(a, k / 2); return (sub * sub) % base; } } 每次取基底，和其余数组的10次方相乘 int base = 1337; // 计算 a 的 k 次方然后与 base 求模的结果 int mypow(int a, int k) { // 对因子求模 a %= base; int res = 1; for (int _ = 0; _ & b) { if (b.empty()) return 1; int last = b.back(); b.pop_back(); int part1 = mypow(a, last); int part2 = mypow(superPow(a, b), 10); // 每次乘法都要求模 return (part1 * part2) % base; } 寻找缺失元素： 排序 × hash × 按index异或 等差数列求和-当前和：防止溢出边加边减 寻找缺失和重复元素（同时出现） 对于这种数组问题，关键点在于元素和索引是成对儿出现的，常用的方法是排序、异或、映射。 这里介绍的是映射的方法， val-> index -> nums[index] = -nums[index]; 这样当我们发现其中的有个数是正数的时候，对应的index就是缺失元素，发现有个数要变换的时候已经是负数的时候就是重复元素。 字符串乘法： 由于字符串做乘法，就很直观的就是大数相乘的问题，所以就是不能直接转成整形去做，我们直接模仿手乘画图就行，这里的关键在于将我们的乘法拆解的更加的底层 图不好放，看网站上的，写的特别清楚 主要的问题在一个坐标的转换，但是仔细观察的话问题也不大 判断括号的合法性： 单种括号，我们只需要（++ ）--进行判断就可以了，最终==0 多种括号，需要增加存储的信息：使用STACK，遇到左括号就入栈，右括号就出栈，出栈的时候进行匹配。 如果带通配符的：双向进行查找，左到右的时候把*当成++ 右到左的时候也把*当成++,在遍历过程中只要小于0了就直接失效 这题实际上也可以使用DP，但是怎么做呢 状态转移： 算法： 如果且仅当间隔 s[i], s[i+1], ..., s[j] 能组成有效的括号时，dp[i][j]为 true。只有在下列情况下，dp[i][j] 才为 true： s[i] 是 '*' 号, 且在 s[i+1], s[i+2], ..., s[j] 这个范围能够组成有效的括号 或者，s[i] 为 '('，并且在 [i+1，j] 中有一些 k，使得 s[k] 为 ')'，且(s[i+1:k] 和 s[k+1:j+1])截断的两个间隔可以构成有效的括号； 判定子序列 很简单的解法：利用双指针直接求解： bool isSubsequence(string s, string t) { int i = 0, j = 0; while (i 如果有一系列字符串s1,s2,s....和t做匹配的时候怎么做呢？ 可以按照现在的方法加入for 循环，但是如果使用二分法，可O(N)减低到O(M)(N),但是实际上我们也不追求这个， 统计t中每一个字符出现的位置，创建这样一个数组 然后遍历再每个字母的数组中进行二分搜索就行了。 1.2. 《剑指offer》 在这一部分里面实际上书上和具体实现是存在一定版本上的差异的，我们可以从书上得到解题的那种基本思路，但是实际上并不一定适用于现在的情况，所以我们其实可以基于现在的实现来结合书上的笔记进行整理。在这个文档中主要集中于现今情况下的解题分析； 1.2.1. 基本知识点 重要知识点 书上记录一些比较重要的知识点，后续多次复习或者是重新整理； [ ] 树的3重遍历（*2 循环+递归）= 6种实现； [ ] 红黑树 SIZEOF Sizeof(Empty Class) = 1 ; 加入构造和析构函数 =1；将析构函数标记为virtual， = 4（32位的机器） =8（64位的机器） （会生成虚函数表，并为该类型的所有实例中添加一个指向虚函数表的指针。） 也就是说指针在32位上是4字节 在64位上是8字节。 Singleton 分析书上的C#代码中的代码和我们笔记中的C++情况的异同，然后根据C#中提出的哪些要求来进一步改进C++中的这个单例的实现方式。 // final version of C++ singleton // 不会产生复制构造函数，只会声明一次实例的生成，同时也简化了调用的步骤 public: Random(const Random&) = delete; static Random& Get() { static Random instance; return instance; } static float Float() {return Get().IFloat();} private: float IFloat() {return m_RandomGenerator;} Random(){} float m_RandomGenerator = o.5f 1.2.2. 数据结构 重建二叉树 还是一个基于n-1假设完成的递归思想：递归记得要有初始状态，前序/后序 && 中序，实际上前序和后序的变换没有什么差别，只是顺序颠倒了一下，这里需要注意的就是怎么样推导出这样的一个重建过程是比较重要的。 1.2.3. 算法和数据操作 对重要的算法理念进行整理和分析，便于后续对这些方法进行复习的时候有个脉络。 递归和循环的思想 查找和排序的思想（这里需要对排序的算法进行进一步的整理） 回溯法 动态规划与贪婪算法 位运算 递归和循环：斐波那契数列 相关的分析还有一些重要的点： 迭代由于大量的重复项，所以回导致算法的时间和计算复杂度急剧上升（这个问题应该在各类递归过程中分析，是否会导致递归爆炸栈溢出的问题） 第二个问题在于，int等各个类型的取值范围，需要注意最大能取到哪个数字，从而进行规划 取余的操作应该在哪一步执行，在计算加和的中间就需要执行取余，实际上就是一个移位操作把，把溢出值移位移掉。（是否能够使用移位的手法来进行处理。） 青蛙跳台阶要记得初始情况是不同的记得分析一下就可以了。 class Solution { public: int fib(int n) { // type 1 stupid answer : 大量重复计算问题的发生 int prea = 0; int preb = 1; if (n 0 ? n : 0; for (int i = 0; i 查找和排序：旋转数组的最小数字（11） 解题思路：遍历到第一个比前数小的数，就直接break，有一个想法就是对第一个数进行特殊的判断，但是对于这种大部分都是旋转的情况，这个判断带来的收益远小于负面影响，所以还是不做额外判断。 100% 99.81% class Solution { public: int minArray(vector& numbers) { if(numbers[0] 回溯法：矩阵中的路径（12） 这一题我的解题思路实际上还是基于递归的解法，虽然需要四个方向，但是通常情况下，由于false就会退出，实际上复杂度并不会增加太多，但是这个解法的效率还是太低了。具体的解决思路如下。但是这题好像代表的一种新的算法，后续要看一下。 越界的两种情况的判断顺序 '\\0'的情况 34 42 class Solution { public: bool exist(vector>& board, string word) { // 实现的问题：判断的就是相应的邻格中有没有相应的元素 // 怎么判断是否重复，切换成一个无表示的值？NULL行不行？ // 总结一下：第一步find，第二部search（4 blocks），第三步替换成NULL，第四步移动格子。 // find start (但是这样有个问题就是起始节点不是单一的，有可能有重复的起始节点。) int s_index = 0, s_jndex = 0; bool res = false; for(int i=0;i>& board,string word, int idex, int jdex, int kdex){ // 前面的方法遍历所有可能的起始点，但是这样的话，修改重复数组就完蛋了（用迭代的方法来实现临时的赋值） // 迭代搜索 // 这个顺序要在下面的越界判断之前。 if(kdex == word.size()) return true; if(idex=board.size() || jdex=board[0].size()) return false; bool res = false; if (board[idex][jdex] == word[kdex]){ // board[idex][jdex] = '\\0'; // 四种情况这里要体现一下，但是如果用递归的方式写入四个节点的话，好像复杂度有点问题 bool temp = helpsearch(board, word, idex+1, jdex, kdex+1) || helpsearch(board, word, idex-1, jdex, kdex+1)\\ || helpsearch(board, word, idex, jdex-1, kdex+1) || helpsearch(board, word, idex, jdex+1, kdex+1); res = true && temp; board[idex][jdex] = word[kdex]; } return res; } }; 网站里有个一样的思路，时间和空间复杂度差不多，但是可以看一下是怎么简化代码的。 class Solution { public: bool exist(vector>& board, string word) { rows = board.size(); cols = board[0].size(); for(int i = 0; i >& board, string word, int i, int j, int k) { if(i >= rows || i = cols || j 《回溯法》这个方法的思想比较重要，这边上图，实际上效果时间：87 61是目前最好的但是我觉得还是不太行。 深度每增加一层，即说明匹配到了一个新的字符。使用迭代器指向word.begin()，深度每增加一层，令迭代器向前移动一个位置。如果该层找不到可行的路径，回溯到父节点，深度减小，迭代器也要后退一个位置，如果在搜索的过程中迭代器指向了word.end()，说明找到了可行路径，返回true，搜索结束。如果搜索了整个解空间树也没有找到可行路径，说明没有可行路径，返回false，搜索结束。 class Solution { public: bool findpath(const vector>& board, const string& word, vector& visited, int row, int column, string::iterator& itr) { if (itr == word.end()) //到达字符末尾，匹配成功 return true; if (row = board.size() || column = board[0].size()) //如果坐标不在矩阵范围内 return false; //返回false bool haspath = false; //标记子节点是否含有下一个字符 int columns = board[0].size(); //矩阵的列数 //cout >& board, string& word) { if (!word.size()) //所给的word为空 return true; //获得矩阵的行列数 unsigned int rows = board.size(), columns = board[0].size(); for (int i = 0; i visited(rows * columns, false); //标记节点是否被访问 if (findpath(board, word, visited, i, j, itr)) //从该节点出发寻找到了路径 return true; } } return false; //所有可能的路径都找了也没到找，返回false } }; 回溯法：机器人的行走路线（13） 问题：这题首先特别重要的一定要理解正确题目的意思，我首先就把题目搞错了，K指的并不是行走的步数，实际上只是一个相应的约束条件，理论上是制定了k但是机器人可以走无数步这样的情况。 DFS、BFS 师兄说的循环数组我得想一下是为什么是，或者看下书。 class Solution { private: int subsum(int x) { // NOTE:这个写法要注意对for循环有一个好的理解，知道到底是象征什么意思。还有第一项空置的含义 int res = 0; for (; x; x /= 10) { res += x % 10; } return res; } public: int movingCount(int m, int n, int k) { // 整理思路重新出发 //NOTE:定义vector的初始化长度，防止vector队内存的动态再分配来影响运行时间 // (length, value) if (!k) return 1; vector> visited(m, vector(n, 0)); // 起始点初始化 int ans = 1; visited[0][0] = 1; // 循环递推,中间需要嵌入是否满足条件的判断。 for (int i = 0; i k) continue; // 排除不可达的情况，由于我们是前向遍历，我们只需要考虑他的前一个或者上一个能到达，才行。 // 这个理论可以好好的分析一下。这样的话，实际上还是有切断的处理的把。（前一个和上一个都就直接换行之类的？） if (i - 1 >= 0 && visited[i - 1][j]) { visited[i][j] = true; ans += 1; } else if (j - 1 >= 0 && visited[i][j - 1]) { visited[i][j] = true; ans += 1; } else continue; } } return ans; } }; 动态规划与贪婪算法：剪绳子v1.v2(14/15) 动态规划的基本解法：将问题归化成更小的存在最优情况的子问题。 在解答问题的时候我们会发现一些重要的点： 初始的启动量怎么界定？（因为不是每一步都要迭代到底的，所以会出现一些特殊的情况） 将问题归化到最底层，我们会发现有一些长度如果切分了，相应对乘积的贡献会小于不切分本身的贡献，这种值就是初始的启动量，他们不切分是最好的，在这道题中的体现就是0,1,2,3,4(特殊分界点，切分与否是一致的，) 初始的特殊情况直接输出 时间空间复杂度分析：从算法中很容易知道是n^2/2 我的算法实现（存在问题已解答）： int cuttingRope(int n) { // 这里是特殊的一些情况， if(n maxarray(n+1, 0); maxarray[0] = 0; maxarray[1] = 1; maxarray[2] = 2; maxarray[3] = 3; // bottom-up的循环 for (int i = 4; i maxarray[i]){ maxarray[i] = tmp; } } } return maxarray[n]; } 使用贪婪的解法来解决这个问题 从上面的分析我们知道，我们最好是将每一段都切分到2、3，4的情况，这样的话，能节省计算量 但是我们发现这样的算法的时间效率并没有变高，这是因为这个多重判断的时间复杂度的问题吗，不应该啊。 int cuttingRope(int n) { if(n 4) { n = n - 3; //尽可能地多剪长度为3的绳子 res = res * 3; } return res * n; } 改进判断过多的情况： int cuttingRope(int n) { if(n 位运算：二进制中的1的个数 在这里需要掌握所有二进制运算的基本概念，与或非以及移位操作，还有异或操作； 知道要掌握啥就可以了具体的接替思想的话后续看书就可以，实现代码如下。算法的时间效率貌似和网友还有一定的差距后续再慢慢看吧，我觉得这样已经差不多了，主要在于思想上的。 int hammingWeight(uint32_t n){ // 三种写法，实际上1-2是一类，所以写两种就好了。 // uint32_t i = 1; // int count = 0; // while(i){ // if (n&i) // count++; // i = i 1.2.4. 高质量的代码 代码的完整性：数值的整数次方 这一题有一些特别需要注意的点： 当指数小于1的情况，（在这种情况下底数为0的情况） 使用指数的方法（2分）实现快速的指数计算 -INTMIN的问题，因为0占了一位，所以int下界无法直接去abs，需要进行一个类型转换再进行取负 class Solution { public: double myPow(double x, int n) { // FIXME：当指数小于等于0的时候 // FIXME：当上述情况的时候底数为0的情况 if(!x) return 0; else if(!n || x==1.0) return 1; else if (n>2); res *= res; if (1&n) res = res * x; return res; } }; 拓展一个网友的写法，虽然效果没有我的好，但是还是权当作一个参考 class Solution { public: double myPow(double x, int n) { if(x == 1 || n == 0) return 1; double ans = 1; long num = n; if(n >= 1; } return ans; } }; 1.2.5. 解决面试题的思路 offer29 顺时针读取列表 这题主要看怎么分析问题的吧，我再这题里面的height 和width写反了，有心情的时候改一下，具体的思路如图所示： class Solution { public: vector spiralOrder(vector>& matrix) { vector res; if(matrix.empty()) return res; // 初始参数设置 bool ishori = false; // 先执行水平的 height才是水平 int offset_x=0,offset_y=0,xi=0,yi=-1; int width=matrix.size(), height=matrix[0].size(); // 水平和垂直写反了， while(width-offset_x && height-offset_y){ int step = 0; if(ishori){ while(step 排序搜索统计算法（时间效率那一节） offer 48滑动窗口算法解决，contain和index都需求的问题。 看看网友的解答，已经整理到代码文件中了。 1.3. 《LeetCode》 刷题锻炼手感和了解题目设置和题库； 1.3.1. 经典类型题（后续归纳） 最长回文子串（了解一下马拉车）（5） 关键点都在代码块里 cankao class Solution { public: string longestPalindrome(string s) { // 这题实际上也能用动态规划来解答，但是这题的动态规划效果一般 // TODO：我们也可以用马拉车算法解决学习一下 // 我们python中写的是什么玩意，我有点没看懂，后面再看看是怎么做的,我觉得可以不用看，思路太傻逼了。但是可以分析测试样例。 //使用简单的递归思想用双指针的方式来解决奇偶不同的情况 string res; for(int i=0;iS1.size()?res:S1; res = res.size()>S2.size()?res:S2; } return res; } string findPalindrome(const string& s, int l, int r) { while(l>=0 && r 打印从1到n的最大的n位数（17） 这一题实际上的亮点在于考虑大数问题，显然n位数，这个就是一个很容易溢出的条件，n稍微大点就超出了int的边界范围。 首先不考虑的话（找工作的时候千万不要这么写） class Solution { public: vector printNumbers(int n) { // 书上的方法我已经完全掌握了，但是问题在于这一题要返回整数列表，这就不太对进把， if (n (0); int nums = 1; for (int i = 0; i res(nums-1, 0); for (int i = 0; i 大数问题后面复习的时候来进行联手 实际上有两种解法，基本就是看书去实现一下，这里后面再来写就行了。 LEETCODE 用字符串模拟数组（+，-，输出问题）其中的print我觉得用stoi可能就可以轻松解决了，可能不需要重新编写 全排列问题 表示数值的字符串（20） 这一题简直恶心人，除了书上提到的那些东西，还有一些其他的恶心人的情况。比如 \" 0 \" \"3.\" \"3. \" \"46.e3\" \".e1\" 最终的实现代码： class Solution { public: bool isNumber(string s) { // 实际上要考虑的特殊value只有 + - E e . // 经过 + - E的以后，还是能接受一样的类型，但是没办法进行同样的符号判断了，就是不能重复出现 // FIXME：可以使用递进的方法，也就是将前面的数字一个个吞掉，这样就不用考虑到重复判断的问题了，只需要进行函数的调用 // 但是经过.后，就什么符号都不能出现了，只能是纯整数。 if(s.empty()) return false; int index = 0; bool cures; // 前面是' '的情况 while(index='0' && s[index]begin; } bool isInteger(string& s, int& index){ // 考虑包含 + - 符号 if (s[index] == '+' || s[index] == '-') index++; return isUnsignedInteger(s,index); } }; 拓展：char和int的转化关系 实际上我们可以通过 i+'0'将int之类的类型转换为char的数字，因为这实际上是对应的ASCII的加减，然后会执行隐式的类型变换，将其变换为char类型的值把。 总结：链表删除的形式 通常情况下内容覆盖这种操作应该是不被允许的把，但是it depends,比如18题就是这样去实现的 到底是把原指针delete掉来作为删除的凭据，还是直接指向nullptr，在原本的一开始的题目中，好像都是指向了nullptr，但是又有遇到一些是需要delete的。后续总结一下 1.4. 顺带GIT知识扩充（后续迁移） 1.4.1. 基本的workflow 1.4.2. 使用的一些KeyPoint 用rebased 代替merge： 其实就是merge对分支的处理比较不友好，可能需要我们进行手动的删除，而rebased就是直接进行重构，具体的操作如下： 首先，找到这两条分支的最近公共祖先LCA，然后从master节点开始，重演LCA到dev几个commit的修改，如果这些修改和LCA到master的commit有冲突，就会提示你手动解决冲突，最后的结果就是把dev的分支完全接到master上面。 © AikenHong all right reserved，powered by Gitbook该文件修订时间： 2021-09-30 19:57:38 "}}