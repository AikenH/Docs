# 基础理论分析

@AikenHong 2021

为了更好的撰写论文，需要对整理的理论上有一个更好的认知和总结，主要会划分为一下的几个方面

- 现实困境（背景故事）与需求分析
- 问题的理论定位（科学问题，难点）
- 创新点分析（贡献分析）

近期需要围绕这几个点做更为深入的思考和总结，

## 研究背景

这一部分结合Open World Learning 和其他的论文，整理出我们的故事背景。

为了解决开放世界环境下的模型持久运营的问题，解决现实环境中，数据分布不均匀，以及环境持续变化，新类层出不穷的问题，具体的问题可以划分为以下的几个方面：


| Scenes            | Problem     | Detail                                    |
| ----------------- | ----------- | ----------------------------------------- |
| Data Distribution | Long-Tailed | Unbalance, which bring bias for All       |
|                   |             | Majority annexes minority                 |
|                   | Few-Shot    | Tailed/New Classes has only few data      |
|                   |             | empirical risk minimization is unreliable |
|                   |             | Difficult to model, on Few                |
| Model Upgrade     | Incremental | Learning w catastrophic forgetting        |
|                   |             | Hard to weigh                                          |

## 科学问题定位和相关研究 🗺️

**以解决的问题为主**：以下的Key Word 作为我们主要需要对比的相关研究内容：

1. 深度聚类学习
2. 类别增量学习

最终进行实验对比的时候也是通过这两个方向作为主要的baseline进行比对，而前面的就直接和Backbone的结果进行比对即可。

**以对应的技术方法为主**：则有这些相关的研究方向，（排除上述的方向）

1. 自监督学习
2. 弱监督学习（小样本，半监督）
3. 长尾分布学习（Two-Stage，Causal Analysis，Rebalance）

## Long Tailed Learning ⚖️

基于对==两阶段解耦==的现阶段主流算法进行分析，我们知道有一个良好的能表征特征的分类器，能够帮助我们将问题归化到分类器的维度，对分类器的学习进行有效的调整的话，就能得到一个无偏的表示和预测结果。

基于上述事实，为了解决长尾分布问题中的偏差问题，得到一个相对无偏的最终模型，就需要在后续模型调整的部分对我们的分类器进行无偏的学习，希望得到一个泛化性强的特征提取器，且分类器是一个相对无偏的表示。

为了达到无偏学习的目的，我们考虑从**数据**，**优化**（迭代），以及修正的角度分别来进行优化。

### Backgroud and Relatedwork

[[Papers/LT-Collection]] 后续基于这个进行整理；

### Motivation 🧠

Here comes my motivation to design this network.
这里是我们实现对于长尾的优化的动机和思路来源

**SSL** make backbone general，then we using **SCL fine-tuning** the model which make it **specific in our task** to get better result. 

自监督学习能够得到一个更为通用的表征，在后续针对特定任务进行微调的过程中，我们使用SCL损失在前几个Epoch使得Backbone对于这个task更加specific，这样有利于我们在这种下游任务上得到更好的表现。

而使用SCL的原因是为了保持我们的Backbone的映射关系和预训练保持一致，是这种更为可分的判别性特征，和预训练任务保持一致，从而类似WarmUP让SSL更加的适应我们的下游任务。

But when we using the info from labels we will **intro bias** which will also be caused by the rebalance part（upsample）, but if we using dowmsample method, the the balance will not introduce much, but decrese the info we got, so we decide to drop this. and **using imbalance data** to tuning the model.

但是当我们使用标签信息的时候，我们在训练的过程中就会由于数据的不平衡引入对应的偏差。

Even the rebalance-strategy can’t be perfect, Augs from one image bring less info then a new sample. 

而这种偏差是很难通过上采样消除的，上采样过程中，使用基于一张图像进行数据增强得到新的样本，这种样本带来的info是远不如一个新的图像能给予我们的，如果网络对该图像已经学习，第二次遇见的时候的收益就会大大下降（容易过拟合）；

而下采样虽然能降低偏差，但是是从主动抛弃数据的方式的，而实际上这同样导致经验风险最小化器不可靠的问题。

So we'll make a loss is combined by alignment and SCL in diff factor which controlled by epoch.

所以我决定抛弃rebalance策略，而在Loss的角度基于Diff Lr和对应的SCL降低特定任务和标签带来的Bias，并最终通过校准策略来纠正Task-Specific任务中带来的数据偏向性。

而这里对应的方法我们选择了两种：**Causal TDE** 和 **DisAlign**

In additional we can change loss according to ARC-FACE

此外，在对特征空间的学习中，我们可以考虑像Arc-Face等的损失来对我们的InfoNCE损失进行校正。

### Strucure Design🏗️

PlaceHolder

### Experiments Setting ⚙️

在这里由于我们面向的场景更多的可能偏向的是数据不均衡的的情况，因为我们把长尾的尾部丢给了增量学习去做计算，所以我们考虑使用step-0.5去做我们毕业论文中不均衡的基准设定，这和传统的长尾分布的研究是存在区别的，但是我认为是更贴近实际情况的。

这是对问题进行分解和归化，但是如果我们希望和论文中的一些方法进行比较的话，我们就需要使用通用的采样数据指标去进行比较。

#### Params Setting

![Imb-Factor](https://gitee.com/Aiken97/markdown-image/raw/master/3070imgs/datasampler.png)


#### Benchmark

这里对对应的论文进行整合，得到了一个比较的基准和场景，我们可以以这个为指标进行对比和后续的实验分析，目前只在Cifar100上收集，后续需要对更多数据集进行测试。

基于BackBone对这些方法的实验结果($Top1 Acc$)进行汇总，作为我们后续研究的参照：在进行实验的时候，我们需要首先调整好BenchMark，基于Benchmark做的改进才能和对应的方法进行对比。

**整理原则**：

1. 对应的论文则由该论文本身为主，后续和LT的仓库进行对比分析；
2. 最主要需要对比的应该是ce情况下的指标，这是我们最重要的，当这个指标对齐后，我们就可以和这些方法同台竞技了。

| Dataset      |  ->   | LT-Cifar-100 |  <-   |  ->   | LT-CIfar10 |  <-   | Backbone |
| ------------ |:-----:|:------------:|:-----:|:-----:|:----------:|:-----:| -------- |
| Factor(Exp)  |  100  |      50      |  10   |  100  |     50     |  10   | ResNet32 |
| RESULT       |   -   |      -       |   -   |   -   |     -      |   -   |          |
| CE           | 38.32 |    43.85     | 55.71 | 70.36 |   74.81    | 86.39 | -        |
| Focal Loss   | 38.4  |     44.3     | 56.8  | 70.4  |    76.7    | 86.7  | -        |
| MixUp        | 39.5  |     45.0     | 58.0  | 73.1  |    77.8    | 87.1  | -        |
| CB Loss      | 39.6  |     45.2     | 58.0  | 74.6  |    79.3    | 87.1  | -        |
| BAGS-After   | 47.83 |    51.69     |   -   | 73.59 |   79.03    |   -   | -        |
| SSL-Uniform  | 40.40 |    45.04     | 57.07 | 73.50 |   78.20    | 87.72 |          |
| SSL-Balanced | 43.06 |    47.09     | 58.06 | 76.53 |    80.4    | 87.72 |          |
| LDAM         | 42.0  |     46.6     | 58.7  | 77.0  |    81.0    | 88.2  | -        |
| BBN          | 42.56 |    47.07     | 59.12 | 79.82 |   82.18    | 88.32 | -        |
| Causal       | 44.1  |     50.3     | 59.6  | 80.6  |    83.6    | 88.5  | -        |
| Hybrid-sc    | 46.72 |    51.87     | 63.05 | 81.4  |   85.36    | 91.12 |          |
| Hybrid-psc   | 44.97 |    48.93     | 62.37 | 78.82 |   83.86    | 90.06 |          |

#### Our Results

| Dataset              | ->  | Cf100 | <-  |               |  EXTRA   |
| -------------------- |:---:|:-----:|:---:|:-------------:|:--------:|
| ResNet50-Full-Data   |     |       |     |               |  76.7%   |
| SSL-Full-Data        |     |       |     |               |  78.52%  |
| ⏬                   |  -  |   -   |  -  |       -       |    -     |
| Factor(Exp)          | 100 |  50   | 10  |   Step 0.5    |          |
| RESULT               |  -  |   -   |  -  |       -       |    -     |
| Resnet50             |     |       |     |    65.93%     | Cifar00  |
| SCL  + Mixup         |     |       |     |               |  BS:512  |
| Causal/DisAlign +Mix |     |       |     |    72.65%     | w/o not0 |
| ssl_full             |     |       |     |   \*68.04%    | IMB-data |
| ssl w all            |     |       |     | 75.67%/75.17% | ResNet50 |
| ssl-imb              |     |       |     |   \*73.82%    |          |
| ssl-imb + all        |     |       |     |     76.7%     |          |

实验结果初步分析，我们发现使用imb的Dataset训练出来的Backbone的表现显著的优于基于全数据训练出来的模型结果，这里可能是我们的模型导入问题，我们需要对其进行==验证==：

- [ ] 重新训练SSL Full-Data & SSL Imb-Data，查看结果是否存在差异
- [ ] 在imb情况下重新训练SCL和Causal等方法

**该部分模型最终稳定后**，我们确定不对其进行任何修改后，可以系统化的进行重新测试，用以验证最终的效果。

目前可以确定的是，我们的方法能在SSL（rethink the value of label）之上进行进一步的提升，方法确实行之有效，而且在（未均衡，或者全均衡）的预训练Backbone后，能够完美的解决这种轻微的不均衡问题，最多20倍左右差异。

但是这也有可能是我们的SCL方法在解决过拟合方面带来的增益。

==毕业论文需要补充实验==

只要我们完成下面这个任务，我们可以首先进入下一个阶段，并对当前阶段的实验归档。
- [x] 对MixUp和SCL、Causal的方法补充消融实验，验证最好的模型是我们Use it All的时候，

==论文发表需要补充实验内容==

针对于上述的Benchmark的整理，我们如果希望能够进行论文发表等工作，我们需要补充以下的一些实验内容，或者说对模型进行对应的改正。

- [x] 模型->Resnet32 并按照CE调整得到一个好的Benchmark (For Cifar)
- [ ] 基于100/50/10的Factor重新完成我们的整组对比实验
- [ ] 在LT-Cifar10 和 LT-Image(Resnet50)上进行实验
- [x] 可以考虑DisAlign整合进来的效果

## Incremental Learning 🖇️

This Part we will talk about the Incremental Learning. In This Part, I'll also contain the method to deal with Few-Shot data.

Sounds Like FIL, but sometimes the Semi-Supervised IL may be more Suitable?

### Hypothesis

实际上近期的大量研究，都是基于Two-Stage的架构去实现的，也就是将Backbone和Classifier进行解耦。而这样对于结构的解耦，对于不均衡问题，或者是类别的增长问题，都起到了极大的正面促进作用。

在对问题解耦以后，我们可以将这两个研究课题，归化到更加细节的各种问题上来做模型的优化和处理，将研究的问题归化到一个更为细粒度的层次：

- 如何学习到一个更加Generalization的模型？
- 排除Wrong Negative Sampler的影响？
- 纠正optimizer在数据量的偏差带来的CLF的偏向性（偏差矫正）？
- 细化特征空间中的分界面，实现对新类别的调整？

而对这些子课题的研究，往往能给最终的效果带来实质上的增益，这些策略往往都能达到其设想的效果。

这一点给了我们启发，分类的问题本质上可以划分为这样的两个阶段：

$$
Pred = CLF(H(x))
$$
结合最近的自监督学习的研究，我们需要的这些信息，或者类别，在图像的世界中，往往是存在着的，图像相对于文本来说，是一种更加Raw的信息存储形式，在一定程度上，我们得到的偏差是更小的，而我们需要的信息也往往以无标注的形式存储了起来。

基于这种假设，我们去完成这样一个Long-Tailed & Incremental的Open World Learning Task的时候，自监督学习对于这些Raw，未经处理的图像的利用能力，就是一个天然的泛化优势。

而相对的，而这样一个General的Pre-Task可以通过Specific的数据来适应各种下游的任务，从而实现在多样的下游任务中保证一个良好的表现。

为了实现这一点，我们还需要在下游调整的时候，保证任务在特征映射空间的同质性需求，防止我们的Backbone往相反的方向学习，从而失去先天的数据优势。

理想的情况下，我们还希望他能校正SSL过程中的fake-negative偏差，综上所述，SCL作为我们后续模型的前置Warm-Up，调整Backbone同时启动Classifier，这样的想法just comeup。

> 为什么不直接使用SCL，这是因为SSL对于Raw Data的利用才是它最大的优势，在更大的数据更广泛的类别下训练能带来SCL无法带来的优势。
> 是否能根据先验知识的引入，带来SSL一个更加符合Knowledge认知的Feature Mapping? Like：序列化类别之间的距离关系。

1.  A Backbone was Trained by Unsupervised Data may  Provide Better Generalization， Which Means we Can just Simplify the Problem to the Training of the Classifier Module.

我假设一个经由更多无监督数据训练的 Backbone 可以提供 Task-Special 无法企及的更高的泛化性能，基于这样的一个假设就可以将问题简化到分类器层面。

2.  Using SCL to Abjust the Backbone for the new class situation, to alleviate the bias the origin process caused. In other hand, we can make the backbone more task specific.

类似于 LT 中，我们在直接使用 Backbone 的时候，可能在特定的下游数据上的效果是欠佳的，或者由于我们在中继的上一个节点有过调整，我们在进行 Incremental 的时候，可以考虑主要使用新类的 SCL 对 Backbone 进行进一步的调整。

### Deal With Semi/FSL

在这里主要参考一下的几篇论文种的思路，来对我们的增量框架中半监督的部分进行优化：

EnAET ➡️ MixMatch ➡️ FixMatch ➡️ MixUp

在进行详细的回顾之前，我们可以提取出一下的几个主要指导思想：

- 一致性预测：希望模型对于不同扰动的图像有较高的识别鲁棒性 
- 置信度预测：希望模型给出的置信度更高，对伪标签部分的输出熵进行计算
- 基于AET的架构来学习一个嵌入空间中的表征

还有一些简要的Trick：

- EMA策略更新教师模型，
- MixUp做数据增强, Sharpen(0->OneHot)

这一部分作为我们在FIL任务中针对于FS这一部分问题的额外处理的Hints, 结合我们的SSL(transfer)+SCL 来克服少量数据导致的模型问题

我们通过对任务的简化, 结合数据增强等等的策略, 来实现这种复杂情景下的增量研究.

### Structure Design



## TB organize

### LabelGe

定义LabelGe的聚类中心的时候，我们可以考虑两种情况，一种是

- 仅仅使用新类数据进行聚类，将新类别的数据使用伪标签进行训练
- 使用全类的数据进行训练，分开Hard和Pseudo Label的权重
- 使用Mixup的训练策略来处理新类的数据，具体的标签的设置需要好好琢磨


#### Align Cluster Labels

关于标签对齐的问题，我们不能频繁的和真实的标签进行对齐，这样可能会导致我们的**模型训练的方向产生频繁的抖动**，这也是为什么awb用一个EMA模型来进行平衡的原因。


==对齐方法==：
基于KM & 匈牙利算法进行标签匹配这一部分我们主要采用两种思路去构建Cost:

1. 使用旧聚类中心对我们更新后的Feature进行分类, 基于每个新类别中旧类别的数据占比作为最终的Cost依据.
2. 基于旧分类器和新分类器的聚类中心之间的距离去构建Cost矩阵, 因为两次迭代过程中的距离我们假设是较为相近的, 所以这种标签匹配机制应该是一个更为合理的点.

==对齐的时机==

- 我们应该在train之前和GT对齐，假设两次聚类之间的距离变化较小，基于这种假定来执行最后的test。
- 又或者说在最后test之前对齐，若是如此，我们只是给模型传入一个dict，将预测的最终结果映射到真实标签。到Test过程中，比如传入一个dict，当我们的输出再哪个范围内的时候，我们对齐进行转换。

经过实验后发现，新类的准确率为零，是标签对齐出现了问题，导致最终的预测标签失配。


#### Projector reuse

确定我们的假设来说我们的故事, 目前的假设主要有以下的两种存在冲突的假设.
除了这些原因之外, 我们还应该考虑到:聚类问题和分类问题之间到底是一个冲突的任务还是我们应该把他当成两个相辅相成的任务.

1. 分类任务和聚类任务的区分, 是否会造成AWB所说的这种永动机的情况(也就是相互反向拉扯), 在这种情况下我们最好是分配projector的一部分在Model中,通过EMA来更新我们的Projector.
2. 又或者我们可以采用EnAET的一致性预测原则, 通过聚类或者输出的距离结合Contrasive Loss进行我们的训练

我们可以基于这两种假设来对我们的Projector进行再利用.

