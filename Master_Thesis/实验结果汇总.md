# DevLog ⌨️
@AikenHong 2021

Record of the Experiment Result and Analysis.

通过这个文件来记录我们的实验内容和对齐进行分析总结，将之前的UniFramework文件中的部分迁移到这里来，每部分分开整理，避免杂乱。


## Part0 Configs of Params 📚

这一部分汇总包括长尾的曲线和新类的设置等等，收集和分析对应的数据情况，便于论文的攥写。

### The New Class Setting

分析类别抽取部分的设置：
- 类别抽取情况: 
- 新类设置的情况: 80+20

### Dataset
```chart
type: bar
labels: [cifar10, cifar100, m-ImageNet, t-ImageNet, ImageNet]
series:
  - title: Class Num
    data: [10, 100, 100, 200, 2000]
  - title: Each Num \* 10
    data: [500,50,50,50,100]

tension: 0.2
width: 80%
labelColors: false
fill: false
beginAtZero: false
```

| Name         | Class | EachNum   | Resolution   | Useage    |
| ------------ | ----- | --------- | ------------ | --------- |
| Cifar10      | 10    | 5000+1000 | 32\*32       | Cls LT    |
| Cifar100     | 100   | 500+100   | 32\*32       | Cls NC LT | 
| TinyImageNet | 200   | 500+50+50 | 64\*64       | Cls NC LT |
| MiniImageNet | 100   | 500+100   | 86\*86       | Cls NC LT |
| ImageNet-1k  | 1000  | 700-1300  | avg:469\*387 | PreTrain  |

[Tiny-ImageNet-200](https://github.com/rmccorm4/Tiny-Imagenet-200) | Tiny-ImageNet-[Plus](https://www.cnblogs.com/liuyangcode/p/14689893.html)


## Part1 Backbone 🔥

模型的Benchmark性能，记录训练中遇到的问题和对应的实验结果，基于Benchmark的性能来对后续的实验做指导和分析的依据。

在这里的数据集`cifar10`，我们先不在上面进行实验，首先以`cifar100`为准，10的类别太少，单类数据的数目又太多，这上面实验的指标很难作数，我们后续可能会迁移到`Tiny/Mini ImageNet`，做进一步的验证。

- [x] 考虑LT的采样方式不同无法进行对比，后续还是需要对齐。
- [x] 现阶段看看我们的策略在现有的方法之下能不能再有所提升，如果有的话就已经可以了。

### Results

首先总结和分析在各种基准情景下的实验结果，所有的结果都来自与测试的Metric，训练的结果如果需要后续可以补上，（比如为了说明一些特定的问题

以下有一些需要说明的指标：

confi（等待测试输出）: 阈值设置为0.5（暂定）的时候的召回率/精确率

Resenet-Small： 指的是对于分辨率过于低的图片，修改入口的卷积层`7,3,2`为`3,1,1`的ResNet，避免丢失太多的图像细节，在该设置上会对准确率有极大的提升。

**Table1. Benchmark（Without Change the Model）**
In This Part we will save the best result for all the situation(after abjust the params)

<center>CIFAR100 Sampler: Step 0.5  With Resnet18 As Backbone</center>

| Backbone       | BS  | Scenario    | acc@1         |
| -------------- | --- | ----------- | ------------- |
| Resnet18-Small | 512 | -           | 75.13%        |
|                |     | LT0.5       | 62.87%        |
|                |     | NC20        | 74.49%        |
|                |     | both        | 63.67%        |
| (simclr_full)  | 512 | same/difflr | 75.98%/75.20% |
|                |     | LT0.5       | 64.03%/61.48% |
|                |     | both        | 66.39%/63.42% |
|                |     | NC20        | 75.27%/74.62% |
| (simclr_imb)   |     |             |               |
| (with Causal)  |     |             |               |
| (with Two)     |     |             |               |

<center>CIFAR100 Sampler:  Step 0.5  With Resnet50 As Backbone BS512</center>

| Backbone          | reCheck | Scenario | acc@1         |
| ----------------- | ------- | -------- | ------------- |
| Resnet50-Small    | lost    | -        | 76.7%         |
|                   | lost    | LT0.5    | 65.93%        |
|                   | lost    | NC20     | 77.79%        |
|                   | lost    | Both     | 68.49%        |
| + MixUp           |         | -        | -             |
| + SCL             |         |          |               |
| + causal/DisAlign |         |          |               |
| SSL_Full          | lost    |          | 78.52%        |
|                   | lost    | LT0.5    | 68.04%        |
|                   | lost    | Both     | 69.60%        |
| + all             |         | LT0.5    | 75.67%/75.17% |
| SSL-imb           |         | LT0.5    | 73.82         |
| SSL-imb + all     |         | LT0.5    | 76.70%        |


<center>CIFAR100 Sampler:  Step 0.5  With EfficientB0 As Backbone</center>

| Backbone     | BS  | Scenario   | acc@1   |
| ------------ | --- | ---------- | ------- |
| Efficient-B0 |     | -          | .73/.90 |
|              |     | LT0.5      |         |
|              |     | NC20       |         |
|              |     | NC20 LT0.5 |         |


训练的时间也是一个比较重要的指标

- 考虑一下下次使用simclr的时候要不要执行dropout等等的操作
- 考虑一下不考虑超类的问题，看看最终进行新类划分的时候会有什么情况

使用自监督预训练过后，模型的收敛速度快的令人发指, 同时如果我们使用diff_lr 0_lr的情况下，算法的效果在大多数情况下都没有共同调整的情况好，这是一个值得我们深思的地方，fix可以理解，但是为什么difflr的情况下算法的效果没有原本的好，是不是我们应该调整别的区分学习率的策略。
### Analysis
基准模型在训练过程中都能达到接近100%的acc@1，都在训练集上有一定的过拟合现象，问题过于简单，模型足够完全拟合训练集，故而我们采取了

- Dropout 0.5
- Data Augmentation（这一部分也是一个比较重要的地方，所以我们最好参数化每一个Transformer，用config管理起来会更好）
- Not Zero Loss0.12（防止模型过早收敛）

但是最终的表现还是存在着这样的过过拟合表现，可以使用SimCLR等等通过扩充的数据集，增大训练规模，或者MIM策略训练一个泛用性更强的通用特征表示器。

也可以适当的丰富数据增强的策略，使得问题的难度进一步的提升。

最核心的策略当然是扩充数据，但是在扩充数据的时候要考虑数据规模和模型的capability的问题。

### Cifar10/100 上的模型调整

#### ResNet

在对cifar10-100的图像进行分类的时候需要修改初始的入口层，因为cifar数据集中的图像太小，如果一开始使用7*7的卷积层，在精度上会损失很多特征信息。

-   可以将7*7 2的卷积改成3*3 1,然后去掉maxpooling层
-   亦可以将图像resize到224*224

前者在cifar10中最终测试可得接近93%的准确率，在cifar100中最终测试可以取得稳定77%的准确率

此外，对图像进行`randomcrop`的过程中，由于原图本来就只有`32*32`所以我们希望crop到32的时候，我们最好是先进行padding，不然该增强是一个无效的增强。

如果我们使用Resnet50

####  Efficient Net

和对ResNet进行调整的时候一样，训练集太过简单，所以过快的收敛，影响了模型的泛化能力，这里考虑可能是dropout没有设置好，或者是任务过于简单，我们可以对其设置一些图像的增强等等的操作来对对训练过程进行调整，可以将一部分需要较多io的任务存放在本地，然后在线进行一些random transformer。

在这里不需要对模型进行修改，只需要调整学习的参数即可。

### LT情景下体现出来的问题

从上面的实验结果中可以发现，在长尾的数据情况下，模型的过拟合问题相当的严重，在训练和测试过程中的准确率存在巨大的鸿沟，我们应该优先解决这个问题，作为我们的第一个创新点，最好是结合pretrain，调整出一个较好的结果。


## Part2 Pre Training ✈️

主要使用`self-supervised learning`和`MIM`两种策略来进行预训练，通过预训练得到一个通用的特征表示模型，基于预训练的模型用于后续的下游任务。

将后续的长尾，小样本，和新类的问题都归化到分类器层中，简化问题的维度。

故事可以这么讲：对于我们的标记数据来说，往往是不足的，但是对于无监督的数据来说，数据量缺往往能够balance出大量的数据，所以我们可以用一个FULL或者Extra数据进行pretrain，ssl试试先。

### SimCLR

训练过程中，发现基于Resnet18的SimCLR结合了MOCO，Loss始终不下降，Acc5趋近于1 Acc1趋近于0，且在部分batch 突变到1。最后发现是memory bank出现了一些问题，这个问题需要后续去定位。

便于在大型数据集（高分辨率图片）上启用MoCo的训练机制。

自监督学习任务的难度和**Batch_Size是强相关**的：
当BatchSize增大，每次迭代中需要区分的对象就明显增加，在这种情况下要达到一样的准确率的难度会比较大，也就会出现验证中的准确率和Batch_size明显下降的情况。
如果要保持一定的准确率可能需要额外的参数调整

**自监督过程中的验证任务**

| dataset   | Model    | BatchSize/epo        | Acc@1/@5      | Loss |
| --------- | -------- | -------------------- | ------------- | ---- |
| Cifar100  | ResNet18 | 2048/500             | 58%/70%       | 2.51 |
|           |          | 1024/500             | 68%/79%       | 1.77 |
|           |          | 512trainBASE(samelr) | 73%           |      |
|           |          | trainBASE(difflr)    |               |      |
|           |          | trainBASE(fixlr)     |               |      |
|           | ResNet50 | 2048/500             |               |      |
|           |          | 1024/500             | 81.86%/89.33% | 1.21 | 
|           |          | trainBASE(samelr)    |               |      |
|           |          | trainBASE(difflr)    |               |      |
|           |          | trainBASE(fixlr)     |               |      |


在自监督的训练任务中，我们需要考虑使用更大的模型为我们带来的收益，在自监督的过程中，使用大模型的效果会更好，在下游任务中对模型进行蒸馏可能可以达到更好的效果。

在我们的任务中，我们可以适当的增大模型，但是如果要在其中嵌入蒸馏的策略的话，可能并不是一个最佳的选择。

simclr v2中，好像将projector的第一层进行了保存，考虑论文中的消融实验，如果有必要的话，我们可以对原有模型进行改造，使其适应该种结构。参考官方文档，在后续加入一个fc层即可

## Part3 Incremental with Semi-Supervised

在这一部分对小样本和伪标签的增量方式进行探讨. 后续做
- 对比实验的时候，我们要把我们的Basic-Dataset和Incremental-Dataset保存下来，然后传递给WYZ，让其为我们跑一些结果出来
- 直接和LWF比较，这个直接在我们的代码中体现即可
- 和SCL的方法也能做基本的对比，也是在我们的框架中可以实现
- 其他的就交给wyz进行实验了

### Semi with Incremental

目前在训练过程中，完全没有学习到新的类别，在这里我们需要分析一下出现的问题，私以为可能导致的原因有以下几点：

- 数据集的问题
- 损失函数设计不合理的问题
- LabelGE的对齐出现了问题（我认为这是最可能的原因，可以统计最终的每个类别的预测的真实标签的数目，进行纠错）

> 实验检查后发现，最终的聚类结果存在比较好的聚类中心，但是没有和实际的类别对应起来，这可能是导致最终结果存在偏差的原因，因为在训练过程中，我们证明了我们最终能够和聚类的结果十分贴近，考虑使用该策略进行匹配。

由此可以对对齐的时机进行修正，我们在训练结束的最后再将样本和真实标签进行匹配，这种情况下，我们能够得到一个更好的对齐.

具体的实现上，考虑到处最后的mapping_dict，将其传入test的过程，通过这个mapping dict进行最终预测结果的转换。

- [x] (还没验证正确性)为了实现上的便捷有效，我们传入reverse dict，将数据读取出来的label转换成我们的label，再去计算对应的准确率和损失。
- [x] 代码验证正确，但是要重新训练一下看看结果是否符合预期
- [x] 我们最好将mapping dict转换成tensor形式，避免降低效率

### Check This Out

==蒸馏过程中训练集的准确率和测试集严重失配== 可能导致的原因有以下：

- [ ] 最终的标签映射没有对应上，或者说是计算准确率的时候没有对应上
- [x] 训练集和测试集的构成存在很大的差异和问题
- [x] 因为中途我们使用的是伪标签作为监督，所以实际上这部分我们没办法使用验证集，而如果我们每次都和真实的标签对齐的话，可能会导致学习的过程更加曲折or not？

==我们是否应该考虑更换我们的任务== 和带标签的增量学习进行对比：

暂时决定为不换
1. 这样我们的问题可能会更加明确，但是可能会匹配不上我们的框架
2. 考虑使用特征聚类的结果

==添加验证任务是否更利于监督==

验证任务的评价指标较难界定，如果我们还是基于聚类设置伪标签的话，很难从acc1的角度去评价这一点，但是还是可以考察泛化能

==SCL任务是否适用于Pseudo的场景==

- [ ] 当前阶段我们测试算法是否正确的时候，使用ssl；认为如果要使用SCL，我们需要在标签信息足够稳定的情况下使用，但是如果标签信息已经稳定，我们就不需要这个了，所以SSL会比SCL更适合于这个任务。

- [ ] 这一点上，结合第一个点的重新实验来做，除非我们一开始的Backbone对新类已经有一个很好的效果了。

==Dis旧知识的遗忘过于严重==

这一点上对于这个过于严重的负优化问题，我们可以尝试这样的方式去修改KD Loss，模拟分布进行约束，或者增加相应的旧类的权重和数据

```python
# but tha +/ - operation need the datatype is np or tensor we canot do this in list 
num_old, num_new = 80, 100
temp_pred = old_pred - 1/num_old

old_pred = [1/100] * 100 
old_pred[:num_old] += temp_pred
```


## Part4 Long-Tailed Learning

主要的思想我们已经再理论分析中实现了，这里可以记录一些实验中的方式组合之间的问题已经如何进行组合的问题，以及我们会遇到的一些问题：

**MixUP**
如果我们使用mixup的话，我们记得再训练的时候取消计算acc，且区分训练和测试阶段的流程，测试的时候可以直接使用ce进行损失的计算，不需要计算对应的监督对比损失了。

同时mixup和ce可以很简单的混合使用，但是我们需要考虑基于==causal analysis==或者==disalign==修正
过后的损失是否还适用于这种场景

**Combine Loss**
我们使用combiner来讲损失的权重通过epoch计算出来的$\alpha$进行结合，再交由`lr/optimizer`对我们模型的backbone和clf进行分开的调整

**Sampler**
使用这两种策略进行组合的话，我们不需要对数据集进行rebalance（we can do this if we want）

**Intergrate with distill**

基于上述的这些具体设置，我们对我们的模型框架进行修正

### 实验中遇到的问题

Causal Model的不适配问题，需要对train和eval进行不同的处理

## Part5 Ablation

设计和整理我们的后续实验内容, 便于我们做出对应的修改和实验.

==后续研究大方向==

| Area     | Detail                                       | fi  |
| -------- | -------------------------------------------- | --- |
| Pre      | pretrain on big model(50 or bigger)          | x   |
|          | move Projector                               | x   |
|          | need distill if the model size is dif        | TBD |
|          | pretrain and test on other dataset           |     |
| LT       | Two-Stage and Causal Analysis                | x   |
| Confi    | Recall and Precious with other methor        | TBD |
| Ablation | Baseline(without SS, projector, Loss Change) |     |
| or       | SS                                           |     |
| Idea     | Modify for projector(with classifier)        | x   |
|          | Supervised Contrasive Loss                   | x   | 
| Incre    | Mixup and DataAugmentation                   |     |

==具体的研究研究内容==

这一部分后续的要和backbone之类的表进行整合
