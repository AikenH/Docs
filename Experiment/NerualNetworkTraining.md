# 训练策略调整

@Aiken 2020，

主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。

## 基于问题分析

针对在训练过程中会出现的一些基本问题进行超参数或者训练步骤的调整，以及可能导致问题出现的原因。

### 学习率的设置

#### 学习率的基本概念

$\omega^{n} \leftarrow \omega^{n}-\eta \frac{\partial L}{\partial \omega^{n}}$ 其中的权重就是学习率lr，

|          | 学习率大           | 学习率小              |
| -------- | ------------------ | --------------------- |
| 学习速度 | 快                 | 慢                    |
| 使用情景 | 刚开始训练时       | 一定的次数过后        |
| 副作用   | 1. Loss爆炸 2.振荡 | 1.过拟合 2.收敛速度慢 |

**学习率的下降机制**

[详细理解pytorch的六种学习率pytorch](https://blog.csdn.net/weixin_42662358/article/details/93732852)

|        | 轮数减缓        | 指数减缓                         | 分数减缓                                     |
| ------ | --------------- | -------------------------------- | -------------------------------------------- |
| 英文名 | step decay      | exponential decay                | 1/t1/t decay                                 |
| 方法   | 每N轮学习率减半 | 学习率按训练轮数增长指数插值递减 | lrt=lr0/(1+kt)，k 控制减缓幅度，t 为训练轮数 |

#### 学习率的基本设置

在训练过程中，一般根据训练轮数设置动态变化的学习率。

- 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。
- 一定轮数过后：逐渐减缓。
- 接近训练结束：学习速率的衰减应该在100倍以上。

**Note：**
如果是 **迁移学习** ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4≤10−4) 在新数据上进行 **微调** 。

#### 学习率调整策略

在训练过程中可视化Loss下降曲线是相当重要的，那么针对Loss出现异常的情况我们应该怎么样去调整使得Loss逐步趋于正常呢？

![image-20201120105459815](https://gitee.com/Aiken97/markdown-image/raw/master/img/20210911210315.png)

**曲线 初始时 上扬 [红线]：**（直接起飞**梯度爆炸**）
Solution：初始 **学习率过大** 导致 振荡，应减小学习率，并 从头开始训练 。
**曲线 初始时 强势下降 没多久 归于水平 [紫线]：**
Solution：**后期** 学习率过大 导致 无法拟合，应减小学习率，并 重新训练 后几轮 。
**曲线 全程缓慢 [黄线]：**
Solution：初始 **学习率过小** 导致 收敛慢，应增大学习率，并 从头 开始训练 。



### 限制网络的输出范围

实际上，这一部分的应用就属于激活函数的数学理念问题了，我们倘若需要将网络的**输出限制在一定的范围**内，除了**自己编写相关的数据处理**手段之外，**激活函数**实际上有一部分原因就是为了这点设置的。

1. 神经网络基于对非线性运算的需要，引入了激活函数，强化了网络的学习能力；
2. 同时神经网络**对于输出**有所要求（很多时候是以一种概率表达的方式输出的）所以就会需要softmax（0，1同时sum==1）之类的函数，**可以将分类器的原始输出映射为概率。** Sigmoid tanh之类的将输出限制在（0，1），但是并没有对加和有要求，这里可以做一个区分https://www.cnblogs.com/jins-note/p/12528412.html区分sigmoid（多分类）和Softmax（单分类）
3. Softmax和tanh可能会出现梯度消失的问题，ReLU将输出限制在（0，1）
   [一部分激活函数的特点](https://zhuanlan.zhihu.com/p/73214810)

所以很显然，我们可以通过对于相应的激活函数的应用，来限制我们的网络输出范围。